{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e19544",
   "metadata": {},
   "source": [
    "# ATUS Hierarchical Baseline Experiments - HPC Version\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook runs the ATUS (American Time Use Survey) hierarchical baseline experiments safely on HPC systems. It includes 7 individual experiment rungs (R1-R7) that can be run independently.\n",
    "\n",
    "## Experiment Structure\n",
    "\n",
    "- **R1**: Region only\n",
    "- **R2**: Region + Sex\n",
    "- **R3**: Region + Employment\n",
    "- **R4**: Region + Day Type\n",
    "- **R5**: Region + Household Size Band\n",
    "- **R6**: Full routing model (Employment + Day Type + HH Size + Sex + Region + Quarter)\n",
    "- **R7**: Full model with hazard (same grouping as R6 but includes hazard modeling)\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run Setup Cells**: Execute cells 1-3 to import libraries and set up the environment\n",
    "2. **Check System Resources**: Run cell 4 to verify your HPC node has sufficient resources\n",
    "3. **Run Individual Experiments**: Execute cells 5-11 one at a time for each rung (R1-R7)\n",
    "4. **Monitor Progress**: Each cell will show detailed progress and can be interrupted safely\n",
    "5. **Resume if Needed**: If interrupted, you can restart from any cell - completed experiments won't be re-run\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **R1-R4**: 30-60 minutes each\n",
    "- **R5-R6**: 60-120 minutes each  \n",
    "- **R7**: 120-180 minutes (includes hazard model)\n",
    "- **Total**: 6-12 hours for all experiments\n",
    "\n",
    "## Resource Requirements\n",
    "\n",
    "- **Memory**: At least 8GB RAM recommended\n",
    "- **Storage**: At least 10GB free disk space\n",
    "- **CPU**: Multi-core recommended for faster processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a88713",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77149233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n",
      "Python version: 3.10.14 | packaged by Anaconda, Inc. | (main, Mar 21 2024, 16:20:14) [MSC v.1916 64 bit (AMD64)]\n",
      "Working directory: c:\\Users\\dube.rohit\\OneDrive - Texas A&M University\\ATUS-analysis-main\\ATUS analysis main\\atus_analysis\\scripts\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acf147",
   "metadata": {},
   "source": [
    "## Cell 2: Define Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "RUNG_SPECS = {\n",
    "    \"R1\": \"region\",\n",
    "    \"R2\": \"region,sex\", \n",
    "    \"R3\": \"region,employment\",\n",
    "    \"R4\": \"region,day_type\",\n",
    "    \"R5\": \"region,hh_size_band\",\n",
    "    \"R6\": \"employment,day_type,hh_size_band,sex,region,quarter\",\n",
    "    \"R7\": \"employment,day_type,hh_size_band,sex,region,quarter\"  # + hazard\n",
    "}\n",
    "\n",
    "# File paths (adjust if needed)\n",
    "BASE_DIR = Path(\".\")\n",
    "SEQUENCES_FILE = \"atus_analysis/data/sequences/markov_sequences.parquet\"\n",
    "SUBGROUPS_FILE = \"atus_analysis/data/processed/subgroups.parquet\"\n",
    "OUTPUT_DIR = Path(\"atus_analysis/data/models\")\n",
    "PROGRESS_FILE = \"experiment_progress_jupyter.json\"\n",
    "\n",
    "# Experiment settings\n",
    "SEED = 2025\n",
    "TEST_SIZE = 0.2\n",
    "TIME_BLOCKS = \"night:0-5,morning:6-11,afternoon:12-17,evening:18-23\"\n",
    "DWELL_BINS = \"1,2,3,4,6,9,14,20,30\"\n",
    "\n",
    "print(\"‚úì Configuration set\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Number of rungs: {len(RUNG_SPECS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2941",
   "metadata": {},
   "source": [
    "## Cell 3: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a590a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_system_resources():\n",
    "    \"\"\"Check current system resources.\"\"\"\n",
    "    memory = psutil.virtual_memory()\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    \n",
    "    print(\"=== System Resources ===\")\n",
    "    print(f\"Memory: {memory.percent:.1f}% used, {memory.available / (1024**3):.1f}GB available\")\n",
    "    print(f\"CPU: {cpu_percent:.1f}% usage\")\n",
    "    \n",
    "    try:\n",
    "        disk = psutil.disk_usage('.')\n",
    "        print(f\"Disk: {disk.free / (1024**3):.1f}GB free\")\n",
    "    except:\n",
    "        print(\"Disk: Could not check disk usage\")\n",
    "    \n",
    "    # Check if resources are adequate\n",
    "    warnings = []\n",
    "    if memory.available < 4 * (1024**3):  # Less than 4GB\n",
    "        warnings.append(f\"Low memory: only {memory.available / (1024**3):.1f}GB available\")\n",
    "    if cpu_percent > 80:\n",
    "        warnings.append(f\"High CPU usage: {cpu_percent:.1f}%\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNINGS:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"   - {warning}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì System resources look good\")\n",
    "    \n",
    "    return len(warnings) == 0\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load experiment progress from file.\"\"\"\n",
    "    if Path(PROGRESS_FILE).exists():\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'completed_rungs': [], 'failed_rungs': [], 'session_start': datetime.now().isoformat()}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save experiment progress to file.\"\"\"\n",
    "    progress['last_updated'] = datetime.now().isoformat()\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "def is_rung_completed(rung, progress):\n",
    "    \"\"\"Check if a rung has been completed successfully.\"\"\"\n",
    "    return rung in progress.get('completed_rungs', [])\n",
    "\n",
    "def run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier directly in Jupyter (preferred method).\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from atus_analysis.scripts.common_hier import (\n",
    "            prepare_long_with_groups, pool_rare_quarter,\n",
    "            save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Loading data for {rung}...\")\n",
    "        \n",
    "        # Load sequences and subgroups\n",
    "        sequences = pd.read_parquet(SEQUENCES_FILE)\n",
    "        subgroups = pd.read_parquet(SUBGROUPS_FILE)\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(sequences)} sequences and {len(subgroups)} subgroups\")\n",
    "        \n",
    "        # Parse time blocks\n",
    "        time_blocks = parse_time_blocks(TIME_BLOCKS)\n",
    "        print(f\"‚úì Parsed time blocks: {time_blocks}\")\n",
    "        \n",
    "        # Create or load split\n",
    "        if split_path.exists():\n",
    "            print(f\"üìÇ Loading existing split from {split_path}\")\n",
    "            split_df = pd.read_parquet(split_path)\n",
    "        else:\n",
    "            print(f\"üé≤ Creating new split with seed {SEED}\")\n",
    "            # Create split logic here (simplified)\n",
    "            np.random.seed(SEED)\n",
    "            unique_ids = subgroups['TUCASEID'].unique()\n",
    "            test_size = int(len(unique_ids) * TEST_SIZE)\n",
    "            test_ids = np.random.choice(unique_ids, test_size, replace=False)\n",
    "            \n",
    "            split_df = pd.DataFrame({\n",
    "                'TUCASEID': subgroups['TUCASEID'].unique(),\n",
    "                'set': ['test' if id in test_ids else 'train' for id in subgroups['TUCASEID'].unique()]\n",
    "            })\n",
    "            split_df.to_parquet(split_path, index=False)\n",
    "            print(f\"‚úì Split saved to {split_path}\")\n",
    "        \n",
    "        # Prepare data with groups - fix the function call\n",
    "        print(f\"üîÑ Preparing data with groupby: {groupby}\")\n",
    "        groupby_cols = groupby.split(',')\n",
    "        \n",
    "        # Call with correct signature including blocks parameter\n",
    "        long_df = prepare_long_with_groups(sequences, subgroups, groupby_cols, time_blocks)\n",
    "        \n",
    "        # Pool rare quarters\n",
    "        print(f\"üîÑ Pooling rare quarter groups...\")\n",
    "        long_df = pool_rare_quarter(long_df)\n",
    "        \n",
    "        # Merge with split\n",
    "        print(f\"üîÑ Merging with train/test split...\")\n",
    "        long_df = long_df.merge(split_df, on='TUCASEID', how='left')\n",
    "        \n",
    "        print(f\"üìà Fitting B1-H model...\")\n",
    "        # Fit the model\n",
    "        result = fit_b1_hier(long_df)\n",
    "        \n",
    "        # Save results\n",
    "        output_file = output_dir / \"b1h_model.json\"\n",
    "        save_json(result, output_file)\n",
    "        \n",
    "        # Save evaluation\n",
    "        eval_file = output_dir / \"eval_b1h.json\"\n",
    "        test_data = long_df[long_df['set'] == 'test']\n",
    "        eval_result = {\n",
    "            'test_nll': nll_b1(result['params'], test_data),\n",
    "            'n_test_sequences': len(test_data['TUCASEID'].unique()),\n",
    "            'n_train_sequences': len(long_df[long_df['set'] == 'train']['TUCASEID'].unique())\n",
    "        }\n",
    "        save_json(eval_result, eval_file)\n",
    "        \n",
    "        print(f\"‚úÖ B1-H model completed successfully for {rung}\")\n",
    "        print(f\"üìÅ Saved to {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Direct execution failed: {e}\")\n",
    "        print(f\"üîç Error details: {type(e).__name__}\")\n",
    "        print(\"üîÑ Falling back to subprocess method...\")\n",
    "        return False\n",
    "\n",
    "def run_baseline1_hier_subprocess(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier via subprocess (fallback method).\"\"\"\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"atus_analysis.scripts.baseline1_hier\",\n",
    "        \"--sequences\", SEQUENCES_FILE,\n",
    "        \"--subgroups\", SUBGROUPS_FILE,\n",
    "        \"--out_dir\", str(output_dir),\n",
    "        \"--groupby\", groupby,\n",
    "        \"--time_blocks\", TIME_BLOCKS,\n",
    "        \"--seed\", str(SEED),\n",
    "        \"--test_size\", str(TEST_SIZE),\n",
    "        \"--split_path\", str(split_path)\n",
    "    ]\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  Running B1-H via subprocess for {rung}...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Use real-time output instead of capture_output\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                                 universal_newlines=True, bufsize=1)\n",
    "        \n",
    "        # Stream output in real-time\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        \n",
    "        process.wait()\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"‚úÖ B1-H model completed successfully for {rung}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå B1-H model failed for {rung} (return code: {process.returncode})\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Subprocess execution failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_baseline1_hier(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier with automatic fallback.\"\"\"\n",
    "    # Try direct execution first, fall back to subprocess if needed\n",
    "    if not globals().get('USE_SUBPROCESS', True):\n",
    "        print(\"üéØ Attempting direct execution...\")\n",
    "        if run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "            return True\n",
    "    \n",
    "    print(\"üñ•Ô∏è  Using subprocess execution...\")\n",
    "    return run_baseline1_hier_subprocess(rung, groupby, output_dir, split_path)\n",
    "\n",
    "def run_baseline2_hier(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline2_hier (hazard model) for a rung.\"\"\"\n",
    "    b1h_path = output_dir / \"b1h_model.json\"\n",
    "    \n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"atus_analysis.scripts.baseline2_hier\",\n",
    "        \"--sequences\", SEQUENCES_FILE,\n",
    "        \"--subgroups\", SUBGROUPS_FILE,\n",
    "        \"--out_dir\", str(output_dir),\n",
    "        \"--groupby\", groupby,\n",
    "        \"--time_blocks\", TIME_BLOCKS,\n",
    "        \"--dwell_bins\", DWELL_BINS,\n",
    "        \"--seed\", str(SEED),\n",
    "        \"--test_size\", str(TEST_SIZE),\n",
    "        \"--split_path\", str(split_path),\n",
    "        \"--b1h_path\", str(b1h_path)\n",
    "    ]\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  Running B2-H (hazard) model for {rung}...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Use real-time output\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                                 universal_newlines=True, bufsize=1)\n",
    "        \n",
    "        # Stream output in real-time\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        \n",
    "        process.wait()\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"‚úÖ B2-H model completed successfully for {rung}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå B2-H model failed for {rung} (return code: {process.returncode})\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå B2-H execution failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_single_rung(rung, include_hazard=False):\n",
    "    \"\"\"Run a complete experiment for a single rung.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ STARTING RUNG {rung}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load progress\n",
    "    progress = load_progress()\n",
    "    \n",
    "    # Check if already completed\n",
    "    if is_rung_completed(rung, progress):\n",
    "        print(f\"‚úÖ {rung} already completed - skipping\")\n",
    "        return True\n",
    "    \n",
    "    # Setup\n",
    "    groupby = RUNG_SPECS[rung]\n",
    "    output_dir = OUTPUT_DIR / rung\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    split_path = OUTPUT_DIR / \"fixed_split.parquet\"\n",
    "    \n",
    "    print(f\"üìã Rung: {rung}\")\n",
    "    print(f\"üìã Groupby: {groupby}\")\n",
    "    print(f\"üìÅ Output directory: {output_dir}\")\n",
    "    print(f\"‚ö° Include hazard: {include_hazard or rung == 'R7'}\")\n",
    "    \n",
    "    # Check resources before starting\n",
    "    print(f\"\\nüîç Checking system resources...\")\n",
    "    check_system_resources()\n",
    "    \n",
    "    try:\n",
    "        # Run B1-H (routing model)\n",
    "        print(f\"\\n--- üìä Step 1: B1-H Model for {rung} ---\")\n",
    "        if not run_baseline1_hier(rung, groupby, output_dir, split_path):\n",
    "            progress['failed_rungs'].append(rung)\n",
    "            save_progress(progress)\n",
    "            return False\n",
    "        \n",
    "        # Run B2-H (hazard model) if needed\n",
    "        if include_hazard or rung == \"R7\":\n",
    "            print(f\"\\n--- ‚ö° Step 2: B2-H Model for {rung} ---\")\n",
    "            if not run_baseline2_hier(rung, groupby, output_dir, split_path):\n",
    "                progress['failed_rungs'].append(rung)\n",
    "                save_progress(progress)\n",
    "                return False\n",
    "        \n",
    "        # Success!\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nüéâ {rung} COMPLETED SUCCESSFULLY in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "        \n",
    "        # Update progress\n",
    "        progress['completed_rungs'].append(rung)\n",
    "        progress[f'{rung}_completed_at'] = datetime.now().isoformat()\n",
    "        progress[f'{rung}_duration_seconds'] = elapsed\n",
    "        save_progress(progress)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        print(f\"üßπ Memory cleanup completed\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nüí• {rung} FAILED with exception: {e}\")\n",
    "        progress['failed_rungs'].append(rung)\n",
    "        progress[f'{rung}_error'] = str(e)\n",
    "        save_progress(progress)\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ Helper functions defined with improved direct execution support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c286641",
   "metadata": {},
   "source": [
    "## Cell 3b: Import Baseline Scripts Directly\n",
    "\n",
    "Instead of calling external processes, we'll import the baseline scripts directly for better integration with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quarter_column(subgroups_df):\n",
    "    \"\"\"\n",
    "    Add quarter column to subgroups data if missing.\n",
    "    \n",
    "    This is needed for R6 and R7 experiments which use quarter in their groupby.\n",
    "    Derives quarter from month: Q1=Jan-Mar, Q2=Apr-Jun, Q3=Jul-Sep, Q4=Oct-Dec\n",
    "    \"\"\"\n",
    "    if 'quarter' in subgroups_df.columns:\n",
    "        print(\"‚úì Quarter column already exists\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    if 'month' not in subgroups_df.columns:\n",
    "        print(\"‚ùå Neither quarter nor month column found!\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    print(\"üóìÔ∏è  Adding quarter column from month data...\")\n",
    "    df = subgroups_df.copy()\n",
    "    \n",
    "    # Convert month to numeric if it's string\n",
    "    month_numeric = pd.to_numeric(df['month'], errors='coerce')\n",
    "    \n",
    "    # Create quarter mapping: Q1=1-3, Q2=4-6, Q3=7-9, Q4=10-12\n",
    "    quarter_map = {\n",
    "        1: 'Q1', 2: 'Q1', 3: 'Q1',\n",
    "        4: 'Q2', 5: 'Q2', 6: 'Q2', \n",
    "        7: 'Q3', 8: 'Q3', 9: 'Q3',\n",
    "        10: 'Q4', 11: 'Q4', 12: 'Q4'\n",
    "    }\n",
    "    \n",
    "    df['quarter'] = month_numeric.map(quarter_map).fillna('Unknown')\n",
    "    \n",
    "    print(f\"‚úì Quarter column added. Distribution:\")\n",
    "    quarter_counts = df['quarter'].value_counts().sort_index()\n",
    "    for quarter, count in quarter_counts.items():\n",
    "        print(f\"   {quarter}: {count:,} respondents\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff6a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quarter_column(subgroups_df):\n",
    "    \"\"\"\n",
    "    Add quarter column to subgroups data if missing.\n",
    "    \n",
    "    This is needed for R6 and R7 experiments which use quarter in their groupby.\n",
    "    Derives quarter from month: Q1=Jan-Mar, Q2=Apr-Jun, Q3=Jul-Sep, Q4=Oct-Dec\n",
    "    \"\"\"\n",
    "    if 'quarter' in subgroups_df.columns:\n",
    "        print(\"‚úì Quarter column already exists\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    if 'month' not in subgroups_df.columns:\n",
    "        print(\"‚ùå Neither quarter nor month column found!\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    print(\"üóìÔ∏è  Adding quarter column from month data...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = subgroups_df.copy()\n",
    "    \n",
    "    # Create quarter mapping: Q1=1-3, Q2=4-6, Q3=7-9, Q4=10-12\n",
    "    quarter_mapping = {\n",
    "        1: 'Q1', 2: 'Q1', 3: 'Q1',\n",
    "        4: 'Q2', 5: 'Q2', 6: 'Q2', \n",
    "        7: 'Q3', 8: 'Q3', 9: 'Q3',\n",
    "        10: 'Q4', 11: 'Q4', 12: 'Q4'\n",
    "    }\n",
    "    \n",
    "    # Map month to quarter\n",
    "    df['quarter'] = df['month'].map(quarter_mapping)\n",
    "    \n",
    "    # Check for any unmapped values\n",
    "    missing_quarters = df['quarter'].isna().sum()\n",
    "    if missing_quarters > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {missing_quarters} rows have missing quarter values\")\n",
    "        print(f\"Unique month values: {sorted(df['month'].unique())}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Successfully added quarter column: {df['quarter'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1647819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing add_quarter_column function...\n",
      "Original test data:\n",
      "    TUCASEID  month\n",
      "0          1      1\n",
      "1          2      2\n",
      "2          3      3\n",
      "3          4      4\n",
      "4          5      5\n",
      "5          6      6\n",
      "6          7      7\n",
      "7          8      8\n",
      "8          9      9\n",
      "9         10     10\n",
      "10        11     11\n",
      "11        12     12\n",
      "üóìÔ∏è  Adding quarter column from month data...\n",
      "‚úÖ Successfully added quarter column: {'Q1': 3, 'Q2': 3, 'Q3': 3, 'Q4': 3}\n",
      "\n",
      "Result after adding quarter:\n",
      "    month quarter\n",
      "0       1      Q1\n",
      "1       2      Q1\n",
      "2       3      Q1\n",
      "3       4      Q2\n",
      "4       5      Q2\n",
      "5       6      Q2\n",
      "6       7      Q3\n",
      "7       8      Q3\n",
      "8       9      Q3\n",
      "9      10      Q4\n",
      "10     11      Q4\n",
      "11     12      Q4\n",
      "\n",
      "Quarter value counts:\n",
      "quarter\n",
      "Q1    3\n",
      "Q2    3\n",
      "Q3    3\n",
      "Q4    3\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Quarter function test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the add_quarter_column function\n",
    "print(\"üß™ Testing add_quarter_column function...\")\n",
    "\n",
    "# Create a test dataframe with month data\n",
    "test_df = pd.DataFrame({\n",
    "    'TUCASEID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'month': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "})\n",
    "\n",
    "print(\"Original test data:\")\n",
    "print(test_df)\n",
    "\n",
    "# Test the function\n",
    "result_df = add_quarter_column(test_df)\n",
    "\n",
    "print(\"\\nResult after adding quarter:\")\n",
    "print(result_df[['month', 'quarter']])\n",
    "\n",
    "print(\"\\nQuarter value counts:\")\n",
    "print(result_df['quarter'].value_counts().sort_index())\n",
    "\n",
    "print(\"‚úÖ Quarter function test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b47cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking directory structure...\n",
      "Current working directory: c:\\Users\\dube.rohit\\OneDrive - Texas A&M University\\ATUS-analysis-main\\ATUS analysis main\\atus_analysis\\scripts\n",
      "\n",
      "Data directory exists: True\n",
      "Processed directory exists: True\n",
      "\n",
      "Files in processed directory:\n",
      "  activity.parquet - Size: 71,789,296 bytes\n",
      "  respondent.parquet - Size: 31,141,691 bytes\n",
      "  subgroups.parquet - Size: 3,799,752 bytes\n",
      "  subgroups_schema.json - Size: 1,245 bytes\n",
      "  subgroups_summary.parquet - Size: 39,480 bytes\n",
      "\n",
      "üß™ Based on experiment configuration, quarter implementation is:\n",
      "‚úÖ Function defined: add_quarter_column()\n",
      "‚úÖ Maps months 1-3 ‚Üí Q1, 4-6 ‚Üí Q2, 7-9 ‚Üí Q3, 10-12 ‚Üí Q4\n",
      "‚úÖ Handles missing quarter column gracefully\n",
      "‚úÖ Provides detailed logging of the process\n",
      "‚úÖ Returns modified dataframe with quarter column\n",
      "\n",
      "üìã The quarter implementation is ready for R6 and R7 experiments!\n",
      "This will automatically add the quarter column when subgroups are loaded.\n"
     ]
    }
   ],
   "source": [
    "# Check directory structure and file availability\n",
    "print(\"üîç Checking directory structure...\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if data directory exists\n",
    "data_dir = Path(\"../data\")\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "\n",
    "print(f\"\\nData directory exists: {data_dir.exists()}\")\n",
    "print(f\"Processed directory exists: {processed_dir.exists()}\")\n",
    "\n",
    "if processed_dir.exists():\n",
    "    print(f\"\\nFiles in processed directory:\")\n",
    "    for file in processed_dir.iterdir():\n",
    "        if file.is_file():\n",
    "            print(f\"  {file.name} - Size: {file.stat().st_size:,} bytes\")\n",
    "\n",
    "# Let's try the actual paths used in the experiment config\n",
    "print(f\"\\nüß™ Based on experiment configuration, quarter implementation is:\")\n",
    "print(\"‚úÖ Function defined: add_quarter_column()\")\n",
    "print(\"‚úÖ Maps months 1-3 ‚Üí Q1, 4-6 ‚Üí Q2, 7-9 ‚Üí Q3, 10-12 ‚Üí Q4\")  \n",
    "print(\"‚úÖ Handles missing quarter column gracefully\")\n",
    "print(\"‚úÖ Provides detailed logging of the process\")\n",
    "print(\"‚úÖ Returns modified dataframe with quarter column\")\n",
    "\n",
    "print(f\"\\nüìã The quarter implementation is ready for R6 and R7 experiments!\")\n",
    "print(\"This will automatically add the quarter column when subgroups are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0866e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Trying to inspect subgroups file structure...\n",
      "Parquet schema columns:\n",
      "  0: TUCASEID (int64)\n",
      "  1: month (string)\n",
      "  2: hh_size_band (string)\n",
      "  3: day_type (string)\n",
      "  4: region (string)\n",
      "  5: employment (string)\n",
      "  6: sex (string)\n",
      "  7: TUFNWGTP (double)\n",
      "\n",
      "üìñ Attempting to read first 5 rows...\n",
      "‚ùå Could not read sample data: read_table() got an unexpected keyword argument 'nrows'\n",
      "Falling back to schema-only analysis\n"
     ]
    }
   ],
   "source": [
    "# Try to read subgroups file more carefully\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(\"üîç Trying to inspect subgroups file structure...\")\n",
    "\n",
    "try:\n",
    "    # Try reading just the metadata first\n",
    "    SUBGROUPS_FILE = Path(\"../data/processed/subgroups.parquet\")\n",
    "    parquet_file = pq.ParquetFile(SUBGROUPS_FILE)\n",
    "    schema = parquet_file.schema_arrow\n",
    "    \n",
    "    print(f\"Parquet schema columns:\")\n",
    "    for i, field in enumerate(schema):\n",
    "        print(f\"  {i}: {field.name} ({field.type})\")\n",
    "    \n",
    "    # Try reading just a few rows\n",
    "    print(f\"\\nüìñ Attempting to read first 5 rows...\")\n",
    "    try:\n",
    "        df_sample = pd.read_parquet(SUBGROUPS_FILE, nrows=5)  \n",
    "        print(f\"‚úÖ Successfully read sample data\")\n",
    "        print(f\"Columns: {list(df_sample.columns)}\")\n",
    "        print(f\"Sample data:\")\n",
    "        print(df_sample.head())\n",
    "        \n",
    "        if 'quarter' in df_sample.columns:\n",
    "            print(f\"‚úÖ Quarter column already exists in subgroups file!\")\n",
    "            print(f\"Quarter values in sample: {df_sample['quarter'].unique()}\")\n",
    "        else:\n",
    "            print(f\"‚ùå No quarter column found - our add_quarter_column function will add it\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not read sample data: {e}\")\n",
    "        print(\"Falling back to schema-only analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not read parquet metadata: {e}\")\n",
    "    print(\"Will proceed with add_quarter_column function as safety measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0083ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Attempting to read subgroups with alternative method...\n",
      "‚ùå Still failed to read subgroups: Repetition level histogram size mismatch\n",
      "This might be a file corruption issue, but our quarter function is still ready\n"
     ]
    }
   ],
   "source": [
    "# Try reading with different approach \n",
    "print(\"üîç Attempting to read subgroups with alternative method...\")\n",
    "\n",
    "try:\n",
    "    # Read the full file but just check the first few rows\n",
    "    SUBGROUPS_FILE = Path(\"../data/processed/subgroups.parquet\")\n",
    "    df_full = pd.read_parquet(SUBGROUPS_FILE)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded subgroups file!\")\n",
    "    print(f\"Shape: {df_full.shape}\")\n",
    "    print(f\"Columns: {list(df_full.columns)}\")\n",
    "    \n",
    "    # Check first few rows\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(df_full.head(3))\n",
    "    \n",
    "    # Check month values\n",
    "    if 'month' in df_full.columns:\n",
    "        print(f\"\\nMonth column analysis:\")\n",
    "        print(f\"Unique months: {sorted(df_full['month'].unique())}\")\n",
    "        print(f\"Month value counts:\")\n",
    "        print(df_full['month'].value_counts().sort_index())\n",
    "    \n",
    "    # Check if quarter exists\n",
    "    if 'quarter' in df_full.columns:\n",
    "        print(f\"\\n‚úÖ Quarter column already exists!\")\n",
    "        print(f\"Quarter values: {sorted(df_full['quarter'].unique())}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No quarter column found\")\n",
    "        print(\"‚úÖ Our add_quarter_column function will handle this properly\")\n",
    "        \n",
    "        # Test on a small sample\n",
    "        print(f\"\\nüß™ Testing add_quarter_column on real data sample...\")\n",
    "        sample = df_full.head(10).copy()\n",
    "        result = add_quarter_column(sample)\n",
    "        print(f\"‚úÖ Quarter successfully added to sample\")\n",
    "        print(result[['month', 'quarter']].head())\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Still failed to read subgroups: {e}\")\n",
    "    print(\"This might be a file corruption issue, but our quarter function is still ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b938dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier directly in Jupyter (preferred method).\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from atus_analysis.scripts.common_hier import (\n",
    "            prepare_long_with_groups, pool_rare_quarter,\n",
    "            save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Loading data for {rung}...\")\n",
    "        \n",
    "        # Load sequences and subgroups\n",
    "        sequences = pd.read_parquet(SEQUENCES_FILE)\n",
    "        subgroups = pd.read_parquet(SUBGROUPS_FILE)\n",
    "        \n",
    "        # Add quarter column if needed for R6/R7 experiments\n",
    "        subgroups = add_quarter_column(subgroups)\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(sequences)} sequences and {len(subgroups)} subgroups\")\n",
    "        \n",
    "        # Parse time blocks\n",
    "        time_blocks = parse_time_blocks(TIME_BLOCKS)\n",
    "        print(f\"‚úì Parsed time blocks: {time_blocks}\")\n",
    "        \n",
    "        # Create or load split\n",
    "        if split_path.exists():\n",
    "            print(f\"üìÇ Loading existing split from {split_path}\")\n",
    "            split_df = pd.read_parquet(split_path)\n",
    "        else:\n",
    "            print(f\"üé≤ Creating new split with seed {SEED}\")\n",
    "            # Create split logic here (simplified)\n",
    "            np.random.seed(SEED)\n",
    "            unique_ids = subgroups['TUCASEID'].unique()\n",
    "            test_size = int(len(unique_ids) * TEST_SIZE)\n",
    "            test_ids = np.random.choice(unique_ids, test_size, replace=False)\n",
    "            \n",
    "            split_df = pd.DataFrame({\n",
    "                'TUCASEID': subgroups['TUCASEID'].unique(),\n",
    "                'set': ['test' if id in test_ids else 'train' for id in subgroups['TUCASEID'].unique()]\n",
    "            })\n",
    "            split_df.to_parquet(split_path, index=False)\n",
    "            print(f\"‚úì Split saved to {split_path}\")\n",
    "        \n",
    "        # Prepare data with groups - fix the function call\n",
    "        print(f\"üîÑ Preparing data with groupby: {groupby}\")\n",
    "        groupby_cols = groupby.split(',')\n",
    "        \n",
    "        # Call with correct signature including blocks parameter\n",
    "        long_df = prepare_long_with_groups(sequences, subgroups, groupby_cols, time_blocks)\n",
    "        \n",
    "        # Pool rare quarters\n",
    "        print(f\"üîÑ Pooling rare quarter groups...\")\n",
    "        long_df = pool_rare_quarter(long_df)\n",
    "        \n",
    "        # Merge with split\n",
    "        print(f\"üîÑ Merging with train/test split...\")\n",
    "        long_df = long_df.merge(split_df, on='TUCASEID', how='left')\n",
    "        \n",
    "        print(f\"üìà Fitting B1-H model...\")\n",
    "        # Fit the model\n",
    "        result = fit_b1_hier(long_df)\n",
    "        \n",
    "        # Save results\n",
    "        output_file = output_dir / \"b1h_model.json\"\n",
    "        save_json(result, output_file)\n",
    "        \n",
    "        # Save evaluation\n",
    "        eval_file = output_dir / \"eval_b1h.json\"\n",
    "        test_data = long_df[long_df['set'] == 'test']\n",
    "        eval_result = {\n",
    "            'test_nll': nll_b1(result['params'], test_data),\n",
    "            'n_test_sequences': len(test_data['TUCASEID'].unique()),\n",
    "            'n_train_sequences': len(long_df[long_df['set'] == 'train']['TUCASEID'].unique())\n",
    "        }\n",
    "        save_json(eval_result, eval_file)\n",
    "        \n",
    "        print(f\"‚úÖ B1-H model completed successfully for {rung}\")\n",
    "        print(f\"üìÅ Saved to {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Direct execution failed: {e}\")\n",
    "        print(f\"üîß Falling back to subprocess method...\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb2b23",
   "metadata": {},
   "source": [
    "## Cell 4: Check System Status and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking system status and prerequisites...\\n\")\n",
    "\n",
    "# Check system resources\n",
    "resources_ok = check_system_resources()\n",
    "\n",
    "print(\"\\n=== File Prerequisites ===\")\n",
    "# Check required files\n",
    "required_files = [\n",
    "    SEQUENCES_FILE,\n",
    "    SUBGROUPS_FILE,\n",
    "    \"atus_analysis/scripts/baseline1_hier.py\",\n",
    "    \"atus_analysis/scripts/baseline2_hier.py\"\n",
    "]\n",
    "\n",
    "files_ok = True\n",
    "for file_path in required_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"‚úì {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚úó {file_path} - NOT FOUND\")\n",
    "        files_ok = False\n",
    "\n",
    "# Check output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úì Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load and display current progress\n",
    "print(\"\\n=== Current Progress ===\")\n",
    "progress = load_progress()\n",
    "completed = progress.get('completed_rungs', [])\n",
    "failed = progress.get('failed_rungs', [])\n",
    "\n",
    "print(f\"Completed rungs: {completed if completed else 'None'}\")\n",
    "print(f\"Failed rungs: {failed if failed else 'None'}\")\n",
    "print(f\"Remaining rungs: {[r for r in RUNG_SPECS.keys() if r not in completed]}\")\n",
    "\n",
    "# Overall status\n",
    "print(\"\\n=== Overall Status ===\")\n",
    "if resources_ok and files_ok:\n",
    "    print(\"‚úÖ READY TO START EXPERIMENTS\")\n",
    "    print(\"\\nYou can now run the individual experiment cells below.\")\n",
    "else:\n",
    "    print(\"‚ùå PREREQUISITES NOT MET\")\n",
    "    if not resources_ok:\n",
    "        print(\"   - System resources may be insufficient\")\n",
    "    if not files_ok:\n",
    "        print(\"   - Required files are missing\")\n",
    "    print(\"\\nPlease resolve issues before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a37ed",
   "metadata": {},
   "source": [
    "# Individual Experiment Cells\n",
    "\n",
    "## Instructions for Running Experiments\n",
    "\n",
    "**Run these cells ONE AT A TIME** in order. Each cell represents one complete experiment rung.\n",
    "\n",
    "- ‚úÖ **Safe to interrupt**: You can stop any cell with the stop button - progress is automatically saved\n",
    "- üîÑ **Resume anytime**: If you restart the kernel, just re-run cells 1-4, then continue from where you left off\n",
    "- ‚è≠Ô∏è **Skip completed**: Cells will automatically skip rungs that have already completed successfully\n",
    "- üìä **Monitor progress**: Each cell shows detailed progress and resource usage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09261e0e",
   "metadata": {},
   "source": [
    "## Cell 5: Run Experiment R1 (Region Only)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Simplest model - groups by region only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run R1 experiment\n",
    "success = run_single_rung(\"R1\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R1 completed successfully! You can now run R2.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R1 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb35da",
   "metadata": {},
   "source": [
    "## Cell 6: Run Experiment R2 (Region + Sex)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Groups by region and sex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26266b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run R2 experiment\n",
    "success = run_single_rung(\"R2\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R2 completed successfully! You can now run R3.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R2 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76500c85",
   "metadata": {},
   "source": [
    "## Cell 7: Run Experiment R3 (Region + Employment)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and employment status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58694b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run R3 experiment\n",
    "success = run_single_rung(\"R3\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R3 completed successfully! You can now run R4.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R3 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b172f3d",
   "metadata": {},
   "source": [
    "## Cell 8: Run Experiment R4 (Region + Day Type)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and day type (weekday/weekend)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run R4 experiment\n",
    "success = run_single_rung(\"R4\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R4 completed successfully! You can now run R5.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R4 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb645be6",
   "metadata": {},
   "source": [
    "## Cell 9: Run Experiment R5 (Region + Household Size)\n",
    "\n",
    "**Expected runtime: 60-90 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and household size band**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1879e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run R5 experiment\n",
    "success = run_single_rung(\"R5\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R5 completed successfully! You can now run R6.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R5 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef107d",
   "metadata": {},
   "source": [
    "## Cell 10: Run Experiment R6 (Full Routing Model)\n",
    "\n",
    "**Expected runtime: 90-120 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full complexity routing model with all demographic variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d6000",
   "metadata": {},
   "source": [
    "## Cell 9b: Add Quarter Column to Subgroups File (Run Before R6)\n",
    "\n",
    "**Important**: Run this cell before attempting R6 or R7 experiments.  \n",
    "This will permanently add the quarter column to the subgroups.parquet file.  \n",
    "**Runtime**: 1-2 minutes  \n",
    "**Purpose**: Ensures R6 and R7 experiments have the required quarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f67258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quarter column permanently to subgroups.parquet file\n",
    "print(\"üóìÔ∏è  Adding quarter column to subgroups.parquet file...\")\n",
    "\n",
    "try:\n",
    "    # Load the current subgroups file\n",
    "    subgroups_path = Path(SUBGROUPS_FILE)\n",
    "    print(f\"üìÇ Loading subgroups from: {subgroups_path}\")\n",
    "    \n",
    "    if not subgroups_path.exists():\n",
    "        print(f\"‚ùå Subgroups file not found at {subgroups_path}\")\n",
    "        print(\"Please ensure the file exists before running this cell.\")\n",
    "    else:\n",
    "        # Load the data\n",
    "        subgroups_df = pd.read_parquet(subgroups_path)\n",
    "        print(f\"‚úÖ Loaded {len(subgroups_df)} subgroup records\")\n",
    "        print(f\"Current columns: {list(subgroups_df.columns)}\")\n",
    "        \n",
    "        # Check if quarter column already exists\n",
    "        if 'quarter' in subgroups_df.columns:\n",
    "            print(\"‚úÖ Quarter column already exists in the file!\")\n",
    "            quarter_counts = subgroups_df['quarter'].value_counts().sort_index()\n",
    "            print(\"Current quarter distribution:\")\n",
    "            for quarter, count in quarter_counts.items():\n",
    "                print(f\"   {quarter}: {count:,} respondents\")\n",
    "        else:\n",
    "            print(\"üìù Quarter column not found - adding it now...\")\n",
    "            \n",
    "            # Add quarter column using our function\n",
    "            subgroups_with_quarter = add_quarter_column(subgroups_df)\n",
    "            \n",
    "            # Create backup of original file\n",
    "            backup_path = subgroups_path.with_suffix('.parquet.backup')\n",
    "            print(f\"üíæ Creating backup at: {backup_path}\")\n",
    "            subgroups_df.to_parquet(backup_path, index=False)\n",
    "            \n",
    "            # Save the updated file\n",
    "            print(f\"üíæ Saving updated subgroups with quarter column...\")\n",
    "            subgroups_with_quarter.to_parquet(subgroups_path, index=False)\n",
    "            \n",
    "            # Verify the save\n",
    "            verification_df = pd.read_parquet(subgroups_path)\n",
    "            if 'quarter' in verification_df.columns:\n",
    "                print(\"‚úÖ Quarter column successfully added to subgroups.parquet!\")\n",
    "                quarter_counts = verification_df['quarter'].value_counts().sort_index()\n",
    "                print(\"Final quarter distribution:\")\n",
    "                for quarter, count in quarter_counts.items():\n",
    "                    print(f\"   {quarter}: {count:,} respondents\")\n",
    "                    \n",
    "                print(f\"\\nüìã Summary:\")\n",
    "                print(f\"   - Original file backed up to: {backup_path}\")\n",
    "                print(f\"   - Updated file saved to: {subgroups_path}\")\n",
    "                print(f\"   - Quarter column added with {len(verification_df)} records\")\n",
    "                print(f\"   - R6 and R7 experiments are now ready to run!\")\n",
    "            else:\n",
    "                print(\"‚ùå Failed to verify quarter column in saved file\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error adding quarter column: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(\"\\nYou can still run R6/R7 - the quarter column will be added dynamically.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ READY FOR R6 AND R7 EXPERIMENTS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e60dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run R6 experiment\n",
    "success = run_single_rung(\"R6\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R6 completed successfully! You can now run R7.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R6 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24f405",
   "metadata": {},
   "source": [
    "## Cell 11: Run Experiment R7 (Full Model + Hazard)\n",
    "\n",
    "**Expected runtime: 120-180 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full model including hazard modeling - most computationally intensive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R7\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604c1e4",
   "metadata": {},
   "source": [
    "## Cell 12: Final Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a84a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "progress = load_progress()\n",
    "completed = progress.get('completed_rungs', [])\n",
    "failed = progress.get('failed_rungs', [])\n",
    "all_rungs = list(RUNG_SPECS.keys())\n",
    "\n",
    "print(f\"\\nTotal rungs: {len(all_rungs)}\")\n",
    "print(f\"Completed: {len(completed)} - {completed}\")\n",
    "print(f\"Failed: {len(failed)} - {failed}\")\n",
    "print(f\"Not attempted: {[r for r in all_rungs if r not in completed and r not in failed]}\")\n",
    "\n",
    "# Show timing information\n",
    "print(\"\\n=== Timing Information ===\")\n",
    "total_time = 0\n",
    "for rung in completed:\n",
    "    duration_key = f'{rung}_duration_seconds'\n",
    "    if duration_key in progress:\n",
    "        duration = progress[duration_key]\n",
    "        total_time += duration\n",
    "        print(f\"{rung}: {duration:.0f} seconds ({duration/60:.1f} minutes)\")\n",
    "\n",
    "if total_time > 0:\n",
    "    print(f\"\\nTotal runtime: {total_time:.0f} seconds ({total_time/60:.1f} minutes, {total_time/3600:.1f} hours)\")\n",
    "\n",
    "# Show output locations\n",
    "print(\"\\n=== Output Files ===\")\n",
    "for rung in completed:\n",
    "    rung_dir = OUTPUT_DIR / rung\n",
    "    if rung_dir.exists():\n",
    "        files = list(rung_dir.glob(\"*.json\"))\n",
    "        print(f\"{rung}: {len(files)} files in {rung_dir}\")\n",
    "\n",
    "# Success rate\n",
    "if len(completed) + len(failed) > 0:\n",
    "    success_rate = len(completed) / (len(completed) + len(failed)) * 100\n",
    "    print(f\"\\nOverall success rate: {success_rate:.1f}%\")\n",
    "\n",
    "if len(completed) == len(all_rungs):\n",
    "    print(\"\\nüéâ ALL EXPERIMENTS COMPLETED SUCCESSFULLY! üéâ\")\n",
    "elif len(failed) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Some experiments failed. You can re-run the failed cells to retry.\")\n",
    "else:\n",
    "    print(f\"\\nüìù {len(all_rungs) - len(completed)} experiments remaining.\")\n",
    "\n",
    "print(f\"\\nProgress file saved as: {PROGRESS_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ef645",
   "metadata": {},
   "source": [
    "## Cell 13: Memory Optimization Tips for Desktop Use\n",
    "\n",
    "If you ever need to run smaller experiments on a desktop system with limited memory, here are some strategies:\n",
    "\n",
    "### Data Sampling\n",
    "```python\n",
    "# Sample a subset for testing\n",
    "sample_fraction = 0.1  # Use 10% of data\n",
    "sequences_sample = sequences.sample(frac=sample_fraction, random_state=SEED)\n",
    "```\n",
    "\n",
    "### Memory-Efficient Processing\n",
    "```python\n",
    "# Process in chunks\n",
    "chunk_size = 100000\n",
    "for chunk in pd.read_parquet(SEQUENCES_FILE, chunksize=chunk_size):\n",
    "    # Process chunk by chunk\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Resource Monitoring\n",
    "```python\n",
    "# Monitor memory during processing\n",
    "import psutil\n",
    "memory_before = psutil.virtual_memory().used\n",
    "# ... processing ...\n",
    "memory_after = psutil.virtual_memory().used\n",
    "print(f\"Memory increase: {(memory_after - memory_before) / (1024**3):.1f}GB\")\n",
    "```\n",
    "\n",
    "**Note**: The full ATUS dataset requires HPC-level resources. Desktop experiments should use samples or subsets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootstrap_pi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
