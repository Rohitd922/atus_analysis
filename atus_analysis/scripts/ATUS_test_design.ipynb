{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c39e102-9f11-4295-a1fc-85ed4a92d4d8",
   "metadata": {},
   "source": [
    "# ATUS Hierarchical Baseline Experiments - HPC Version\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook runs the ATUS (American Time Use Survey) hierarchical baseline experiments safely on HPC systems. It includes 7 individual experiment rungs (R1-R7) that can be run independently.\n",
    "\n",
    "## Experiment Structure\n",
    "\n",
    "- **R1**: Region only\n",
    "- **R2**: Region + Sex\n",
    "- **R3**: Region + Employment\n",
    "- **R4**: Region + Day Type\n",
    "- **R5**: Region + Household Size Band\n",
    "- **R6**: Full routing model (Employment + Day Type + HH Size + Sex + Region + Quarter)\n",
    "- **R7**: Full model with hazard (same grouping as R6 but includes hazard modeling)\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run Setup Cells**: Execute cells 1-3 to import libraries and set up the environment\n",
    "2. **Check System Resources**: Run cell 4 to verify your HPC node has sufficient resources\n",
    "3. **Run Individual Experiments**: Execute cells 5-11 one at a time for each rung (R1-R7)\n",
    "4. **Monitor Progress**: Each cell will show detailed progress and can be interrupted safely\n",
    "5. **Resume if Needed**: If interrupted, you can restart from any cell - completed experiments won't be re-run\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **R1-R4**: 30-60 minutes each\n",
    "- **R5-R6**: 60-120 minutes each  \n",
    "- **R7**: 120-180 minutes (includes hazard model)\n",
    "- **Total**: 6-12 hours for all experiments\n",
    "\n",
    "## Resource Requirements\n",
    "\n",
    "- **Memory**: At least 16GB RAM recommended\n",
    "- **Storage**: At least 20GB free disk space\n",
    "- **CPU**: Multi-core recommended for faster processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d0268-3ef4-4574-9ef4-0f6500a2acc7",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261231a-9036-492e-a95e-d05b7ee91ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n",
      "Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\n",
      "Working directory: /ztank/scratch/user/u.rd143338/atus_analysis-main\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths - this notebook should be in scripts/ folder  \n",
    "scripts_dir = Path.cwd()\n",
    "project_root = scripts_dir.parent  \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from scripts.run_single_experiments import run_single_experiment\n",
    "from scripts.baseline1_hier import run_baseline1_hier \n",
    "from scripts.baseline2_hier import run_baseline2_hier\n",
    "\n",
    "print(\"Helper functions defined with improved direct execution support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3e35c-40cf-455e-9e58-ca42f7e25934",
   "metadata": {},
   "source": [
    "## Define Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99f1e2-2f0f-4f6f-a6be-882dac63c843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration set\n",
      "Output directory: atus_analysis/data/models\n",
      "Number of rungs: 7\n"
     ]
    }
   ],
   "source": [
    "def run_experiment_safely(name, func):\n",
    "    \"\"\"\n",
    "    Safely run an experiment with error handling and memory management.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n--- Running {name} ---\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run the experiment\n",
    "        func()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"{name} completed in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in {name}: {str(e)}\")\n",
    "        print(\"Details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Check for common error patterns\n",
    "        error_str = str(e).lower()\n",
    "        if 'memory' in error_str or 'out of memory' in error_str:\n",
    "            print(\"WARNINGS:\")\n",
    "            print(\"- Memory error detected. Try reducing batch size or closing other applications.\")\n",
    "        elif 'cuda' in error_str:\n",
    "            print(\"WARNINGS:\")\n",
    "            print(\"- CUDA error detected. GPU memory might be full.\")\n",
    "        elif 'file not found' in error_str or 'no such file' in error_str:\n",
    "            print(\"WARNINGS:\")  \n",
    "            print(\"- File not found. Check that all prerequisite steps completed successfully.\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "print(\"Helper functions defined with improved direct execution support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6ffe7-45a3-42db-886c-393f0702e40a",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829560a-7fc9-450d-a38d-848cded9b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined with improved direct execution support\n"
     ]
    }
   ],
   "source": [
    "def run_single_experiment_wrapper(rung, dataset_path, args, compute_expected=True):\n",
    "    \"\"\"\n",
    "    Enhanced wrapper that handles direct execution of run_single_experiment with better error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert rung to string for run_single_experiment\n",
    "        rung_str = f\"R{rung}\"\n",
    "        \n",
    "        print(f\"Loading data for {rung_str}...\")\n",
    "        \n",
    "        # Load data  \n",
    "        df = pd.read_parquet(dataset_path)\n",
    "        \n",
    "        # Check if 'quarter' exists, if not use 'month' and derive quarter\n",
    "        if 'quarter' not in df.columns:\n",
    "            if 'month' in df.columns:\n",
    "                df['quarter'] = df['month'].apply(lambda x: (x-1)//3 + 1)\n",
    "                print(f\"Derived quarter from month column\")\n",
    "            else:\n",
    "                raise ValueError(\"Neither quarter nor month column found!\")\n",
    "        \n",
    "        # Generate or load splits\n",
    "        from pathlib import Path\n",
    "        models_dir = Path(args.output_dir) / \"models\" / rung_str\n",
    "        split_path = models_dir / \"fixed_split.parquet\"\n",
    "        \n",
    "        if split_path.exists():\n",
    "            print(f\"Loading existing split from {split_path}\")\n",
    "            split_df = pd.read_parquet(split_path)\n",
    "            # Extract train/test splits  \n",
    "            train_ids = split_df[split_df['split'] == 'train']['caseid'].tolist()\n",
    "            test_ids = split_df[split_df['split'] == 'test']['caseid'].tolist()\n",
    "        else:\n",
    "            print(f\"Generating new train/test split for {rung_str}\")\n",
    "            # Generate splits using original logic\n",
    "            unique_cases = df['caseid'].unique()\n",
    "            n_train = int(0.8 * len(unique_cases))\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            train_ids = np.random.choice(unique_cases, size=n_train, replace=False).tolist()\n",
    "            test_ids = [cid for cid in unique_cases if cid not in train_ids]\n",
    "            \n",
    "            # Save split\n",
    "            split_data = []\n",
    "            for cid in train_ids:\n",
    "                split_data.append({'caseid': cid, 'split': 'train'})\n",
    "            for cid in test_ids:\n",
    "                split_data.append({'caseid': cid, 'split': 'test'})\n",
    "            \n",
    "            split_df = pd.DataFrame(split_data)\n",
    "            models_dir.mkdir(parents=True, exist_ok=True)\n",
    "            split_df.to_parquet(split_path)\n",
    "            print(f\"Saved split to {split_path}\")\n",
    "        \n",
    "        # Now run the experiment with direct execution\n",
    "        print(f\"--- Step 1: B1-H Model for {rung_str} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Call B1-H directly\n",
    "            run_baseline1_hier(\n",
    "                data=df,\n",
    "                train_ids=train_ids,\n",
    "                test_ids=test_ids,\n",
    "                rung=rung_str,\n",
    "                output_dir=args.output_dir,\n",
    "                compute_expected=compute_expected,\n",
    "                verbose=True\n",
    "            )\n",
    "            print(f\"B1-H model completed successfully for {rung_str}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Direct execution failed: {e}\")\n",
    "            \n",
    "            # Fallback to subprocess if direct execution fails\n",
    "            print(f\"Falling back to subprocess execution for {rung_str}\")\n",
    "            cmd = [\n",
    "                sys.executable, \"-m\", \"scripts.baseline1_hier\",\n",
    "                \"--rung\", rung_str,\n",
    "                \"--output_dir\", args.output_dir,\n",
    "                \"--dataset_path\", dataset_path\n",
    "            ]\n",
    "            if compute_expected:\n",
    "                cmd.append(\"--compute_expected\")\n",
    "                \n",
    "            process = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True)\n",
    "            if process.returncode == 0:\n",
    "                print(f\"B1-H model completed successfully for {rung_str}\")\n",
    "            else:\n",
    "                print(f\"B1-H model failed for {rung_str} (return code: {process.returncode})\")\n",
    "                print(\"STDOUT:\", process.stdout[-1000:])  # Last 1000 chars\n",
    "                print(\"STDERR:\", process.stderr[-1000:])\n",
    "                raise Exception(f\"B1-H subprocess failed with return code {process.returncode}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Subprocess execution failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_baseline2_hier_wrapper(rung, dataset_path, args, compute_expected=True):\n",
    "    \"\"\"\n",
    "    Wrapper for B2-H baseline (subprocess only - no direct execution due to complexity)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rung_str = f\"R{rung}\"\n",
    "        print(f\"--- Step 2: B2-H Model for {rung_str} ---\")\n",
    "        \n",
    "        cmd = [\n",
    "            sys.executable, \"-m\", \"scripts.baseline2_hier\", \n",
    "            \"--rung\", rung_str,\n",
    "            \"--output_dir\", args.output_dir,\n",
    "            \"--dataset_path\", dataset_path\n",
    "        ]\n",
    "        if compute_expected:\n",
    "            cmd.append(\"--compute_expected\")\n",
    "            \n",
    "        process = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True)\n",
    "        if process.returncode == 0:\n",
    "            print(f\"B2-H model completed successfully for {rung_str}\")\n",
    "        else:\n",
    "            print(f\"B2-H model failed for {rung_str} (return code: {process.returncode})\")\n",
    "            print(\"STDOUT:\", process.stdout[-1000:])  # Last 1000 chars  \n",
    "            print(\"STDERR:\", process.stderr[-1000:])\n",
    "            raise Exception(f\"B2-H subprocess failed with return code {process.returncode}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"B2-H execution failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_full_experiment_wrapper(rung, dataset_path, args, compute_expected=True):\n",
    "    \"\"\"\n",
    "    Run both B1-H and B2-H for a given rung with enhanced monitoring\n",
    "    \"\"\"\n",
    "    rung_str = f\"R{rung}\"\n",
    "    print(f\"STARTING RUNG {rung_str}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if already completed\n",
    "    models_dir = Path(args.output_dir) / \"models\" / rung_str\n",
    "    expected_files = [\"b1h_results.csv\", \"b2h_results.csv\"]\n",
    "    \n",
    "    if all((models_dir / f).exists() for f in expected_files):\n",
    "        print(f\"{rung_str} already completed - skipping\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Check memory and dataset size\n",
    "        import psutil\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"Available memory: {memory.available / (1024**3):.1f} GB\")\n",
    "        \n",
    "        df = pd.read_parquet(dataset_path)\n",
    "        print(f\"Dataset size: {len(df):,} rows, {df.memory_usage(deep=True).sum() / (1024**3):.2f} GB\")\n",
    "        \n",
    "        if memory.available < 8 * (1024**3):  # Less than 8GB available\n",
    "            print(\"WARNING: Low memory detected. Consider closing other applications.\")\n",
    "        \n",
    "        # Step 1: B1-H  \n",
    "        print(f\"--- Step 1: B1-H Model for {rung_str} ---\")\n",
    "        run_single_experiment_wrapper(rung, dataset_path, args, compute_expected)\n",
    "        \n",
    "        # Clear some memory between steps\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        # Step 2: B2-H (optional - comment out if having memory issues)\n",
    "        # run_baseline2_hier_wrapper(rung, dataset_path, args, compute_expected)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"{rung_str} COMPLETED SUCCESSFULLY in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"ERROR in {rung_str} after {elapsed:.1f} seconds: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4e59c-9891-4b1e-a7f5-53b2be7128cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check completion status from previous runs\n",
    "completed = progress.get('completed_rungs', [])\n",
    "failed = progress.get('failed_rungs', [])\n",
    "\n",
    "print(f\"Completed rungs: {completed if completed else 'None'}\")\n",
    "print(f\"Failed rungs: {failed if failed else 'None'}\")\n",
    "print(f\"Remaining rungs: {[r for r in RUNG_SPECS.keys() if r not in completed]}\")\n",
    "\n",
    "# Overall status\n",
    "print(\"\\n=== Overall Status ===\")\n",
    "if resources_ok and files_ok:\n",
    "    print(\"READY TO START EXPERIMENTS\")\n",
    "    print(\"\\nYou can now run the individual experiment cells below.\")\n",
    "else:\n",
    "    print(\"PREREQUISITES NOT MET\")\n",
    "    if not resources_ok:\n",
    "        print(\"   - System resources may be insufficient\")\n",
    "    if not files_ok:\n",
    "        print(\"   - Required files are missing\")\n",
    "    print(\"\\nPlease resolve issues before continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22aec82f-3e0b-46a7-9c58-8dd0616ae023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier directly in Jupyter (preferred method).\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from atus_analysis.scripts.common_hier import (\n",
    "            prepare_long_with_groups, pool_rare_quarter,\n",
    "            save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ“Š Loading data for {rung}...\")\n",
    "        \n",
    "        # Load sequences and subgroups\n",
    "        sequences = pd.read_parquet(SEQUENCES_FILE)\n",
    "        subgroups = pd.read_parquet(SUBGROUPS_FILE)\n",
    "        \n",
    "        # Add quarter column if needed for R6/R7 experiments\n",
    "        subgroups = add_quarter_column(subgroups)\n",
    "        \n",
    "        print(f\"âœ“ Loaded {len(sequences)} sequences and {len(subgroups)} subgroups\")\n",
    "        \n",
    "        # Parse time blocks\n",
    "        time_blocks = parse_time_blocks(TIME_BLOCKS)\n",
    "        print(f\"âœ“ Parsed time blocks: {time_blocks}\")\n",
    "        \n",
    "        # Create or load split\n",
    "        if split_path.exists():\n",
    "            print(f\"ðŸ“‚ Loading existing split from {split_path}\")\n",
    "            split_df = pd.read_parquet(split_path)\n",
    "        else:\n",
    "            print(f\"ðŸŽ² Creating new split with seed {SEED}\")\n",
    "            # Create split logic here (simplified)\n",
    "            np.random.seed(SEED)\n",
    "            unique_ids = subgroups['TUCASEID'].unique()\n",
    "            test_size = int(len(unique_ids) * TEST_SIZE)\n",
    "            test_ids = np.random.choice(unique_ids, test_size, replace=False)\n",
    "            \n",
    "            split_df = pd.DataFrame({\n",
    "                'TUCASEID': subgroups['TUCASEID'].unique(),\n",
    "                'set': ['test' if id in test_ids else 'train' for id in subgroups['TUCASEID'].unique()]\n",
    "            })\n",
    "            split_df.to_parquet(split_path, index=False)\n",
    "            print(f\"âœ“ Split saved to {split_path}\")\n",
    "        \n",
    "        # Prepare data with groups - fix the function call\n",
    "        print(f\"ðŸ”„ Preparing data with groupby: {groupby}\")\n",
    "        groupby_cols = groupby.split(',')\n",
    "        \n",
    "        # Call with correct signature including blocks parameter\n",
    "        long_df = prepare_long_with_groups(sequences, subgroups, groupby_cols, time_blocks)\n",
    "        \n",
    "        # Pool rare quarters\n",
    "        print(f\"ðŸ”„ Pooling rare quarter groups...\")\n",
    "        long_df = pool_rare_quarter(long_df)\n",
    "        \n",
    "        # Merge with split\n",
    "        print(f\"ðŸ”„ Merging with train/test split...\")\n",
    "        long_df = long_df.merge(split_df, on='TUCASEID', how='left')\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Fitting B1-H model...\")\n",
    "        # Fit the model\n",
    "        result = fit_b1_hier(long_df)\n",
    "        \n",
    "        # Save results\n",
    "        output_file = output_dir / \"b1h_model.json\"\n",
    "        save_json(result, output_file)\n",
    "        \n",
    "        # Save evaluation\n",
    "        eval_file = output_dir / \"eval_b1h.json\"\n",
    "        test_data = long_df[long_df['set'] == 'test']\n",
    "        eval_result = {\n",
    "            'test_nll': nll_b1(result['params'], test_data),\n",
    "            'n_test_sequences': len(test_data['TUCASEID'].unique()),\n",
    "            'n_train_sequences': len(long_df[long_df['set'] == 'train']['TUCASEID'].unique())\n",
    "        }\n",
    "        save_json(eval_result, eval_file)\n",
    "        \n",
    "        print(f\"âœ… B1-H model completed successfully for {rung}\")\n",
    "        print(f\"ðŸ“ Saved to {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Direct execution failed: {e}\")\n",
    "        print(f\"ðŸ”§ Falling back to subprocess method...\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc4ed-ab5a-4e42-86d1-84e4d2042ddd",
   "metadata": {},
   "source": [
    "## Cell 3b: Import Baseline Scripts Directly\n",
    "\n",
    "Instead of calling external processes, we'll import the baseline scripts directly for better integration with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8619ea-d557-4291-aed5-364acdb7504a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Successfully imported baseline common functions\n"
     ]
    }
   ],
   "source": [
    "# Import the baseline scripts directly instead of using subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path so we can import the modules\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    # Import the baseline functions directly\n",
    "    from atus_analysis.scripts.common_hier import (\n",
    "        prepare_long_with_groups, pool_rare_quarter, \n",
    "        save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "    )\n",
    "    print(\"Successfully imported baseline common functions\")\n",
    "    \n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Could not import baseline functions directly: {e}\")\n",
    "    print(\"Will fall back to subprocess calls\")\n",
    "    USE_SUBPROCESS = True\n",
    "else:\n",
    "    USE_SUBPROCESS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8672f3-8031-47bd-91c2-3728eea65c17",
   "metadata": {},
   "source": [
    "## Check System Status and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44daad3a-db50-4af4-8f20-a4cda6b1ce23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking system status and prerequisites...\n",
      "\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941552.2GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "=== File Prerequisites ===\n",
      "âœ“ atus_analysis/data/sequences/markov_sequences.parquet\n",
      "âœ“ atus_analysis/data/processed/subgroups.parquet\n",
      "âœ“ atus_analysis/scripts/baseline1_hier.py\n",
      "âœ“ atus_analysis/scripts/baseline2_hier.py\n",
      "âœ“ Output directory: atus_analysis/data/models\n",
      "\n",
      "=== Current Progress ===\n",
      "Completed rungs: ['R1', 'R2', 'R3', 'R4', 'R5']\n",
      "Failed rungs: ['R1', 'R6', 'R6', 'R6']\n",
      "Remaining rungs: ['R6', 'R7']\n",
      "\n",
      "=== Overall Status ===\n",
      "âœ… READY TO START EXPERIMENTS\n",
      "\n",
      "You can now run the individual experiment cells below.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking system status and prerequisites...\\n\")\n",
    "\n",
    "# Check system resources\n",
    "resources_ok = check_system_resources()\n",
    "\n",
    "print(\"\\n=== File Prerequisites ===\")\n",
    "# Check required files\n",
    "required_files = [\n",
    "    SEQUENCES_FILE,\n",
    "    SUBGROUPS_FILE,\n",
    "    \"atus_analysis/scripts/baseline1_hier.py\",\n",
    "    \"atus_analysis/scripts/baseline2_hier.py\"\n",
    "]\n",
    "\n",
    "files_ok = True\n",
    "for file_path in required_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"âœ“ {file_path}\")\n",
    "    else:\n",
    "        print(f\"âœ— {file_path} - NOT FOUND\")\n",
    "        files_ok = False\n",
    "\n",
    "# Check output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"âœ“ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load and display current progress\n",
    "print(\"\\n=== Current Progress ===\")\n",
    "progress = load_progress()\n",
    "completed = progress.get('completed_rungs', [])\n",
    "failed = progress.get('failed_rungs', [])\n",
    "\n",
    "print(f\"Completed rungs: {completed if completed else 'None'}\")\n",
    "print(f\"Failed rungs: {failed if failed else 'None'}\")\n",
    "print(f\"Remaining rungs: {[r for r in RUNG_SPECS.keys() if r not in completed]}\")\n",
    "\n",
    "# Overall status\n",
    "print(\"\\n=== Overall Status ===\")\n",
    "if resources_ok and files_ok:\n",
    "    print(\"âœ… READY TO START EXPERIMENTS\")\n",
    "    print(\"\\nYou can now run the individual experiment cells below.\")\n",
    "else:\n",
    "    print(\"âŒ PREREQUISITES NOT MET\")\n",
    "    if not resources_ok:\n",
    "        print(\"   - System resources may be insufficient\")\n",
    "    if not files_ok:\n",
    "        print(\"   - Required files are missing\")\n",
    "    print(\"\\nPlease resolve issues before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad4c12-a087-47e3-8766-ed1032e94f34",
   "metadata": {},
   "source": [
    "# Individual Experiment Cells\n",
    "\n",
    "## Instructions for Running Experiments\n",
    "\n",
    "**Run these cells ONE AT A TIME** in order. Each cell represents one complete experiment rung.\n",
    "\n",
    "- **Safe to interrupt**: You can stop any cell with the stop button - progress is automatically saved\n",
    "- **Resume anytime**: If you restart the kernel, just re-run cells 1-4, then continue from where you left off\n",
    "- **Skip completed**: Cells will automatically skip rungs that have already completed successfully\n",
    "- **Monitor progress**: Each cell shows detailed progress and resource usage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38d17c-292a-4281-ba26-3dcf7217b238",
   "metadata": {},
   "source": [
    "## Cell 5: Run Experiment R1 (Region Only)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Simplest model - groups by region only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91988ab-5442-4aa5-9ed6-cfb0b9d9f939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING RUNG R1\n",
      "============================================================\n",
      "ðŸ“‹ Rung: R1\n",
      "ðŸ“‹ Groupby: region\n",
      "ðŸ“ Output directory: atus_analysis/data/models/R1\n",
      "âš¡ Include hazard: False\n",
      "\n",
      "ðŸ” Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.1% used, 746.5GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.8GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "--- ðŸ“Š Step 1: B1-H Model for R1 ---\n",
      "ðŸŽ¯ Attempting direct execution...\n",
      "ðŸ“Š Loading data for R1...\n",
      "âœ“ Loaded 36404352 sequences and 252808 subgroups\n",
      "ðŸ“‚ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "ðŸ”„ Preparing data with groupby: region\n",
      "âŒ Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "ðŸ”„ Falling back to subprocess method...\n",
      "ðŸ–¥ï¸  Using subprocess execution...\n",
      "ðŸ–¥ï¸  Running B1-H via subprocess for R1...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R1 --groupby region --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Output directory: atus_analysis/data/models/R1\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Groupby dimensions: region\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 00:21:20,645 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 00:21:20,646 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Grouping by: ['region']\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 00:35:23,966 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 4 groups\n",
      "2025-08-10 00:35:24,208 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 00:35:31,212 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 00:35:31,353 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 00:35:31,353 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 00:35:56,456 - __main__ - INFO - âœ“ Model fitting completed\n",
      "2025-08-10 00:35:56,461 - __main__ - INFO - Model saved to: atus_analysis/data/models/R1/b1h_model.json\n",
      "2025-08-10 00:35:56,461 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 00:39:26,261 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R1/eval_b1h.json\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - âœ“ B1-H training completed in 1088.88 seconds\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - âœ“ B1-H written to: atus_analysis/data/models/R1\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - ============================================================\n",
      "âœ“ B1-H written to: atus_analysis/data/models/R1\n",
      "âœ… B1-H model completed successfully for R1\n",
      "\n",
      "ðŸŽ‰ R1 COMPLETED SUCCESSFULLY in 1094.3 seconds (18.2 minutes)\n",
      "ðŸ§¹ Memory cleanup completed\n",
      "\n",
      " R1 completed successfully! You can now run R2.\n"
     ]
    }
   ],
   "source": [
    "# Run R1 experiment\n",
    "success = run_single_rung(\"R1\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nR1 completed successfully! You can now run R2.\")\n",
    "else:\n",
    "    print(\"\\nR1 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8749bab-46d0-46b8-9925-3cb8c73c34a9",
   "metadata": {},
   "source": [
    "## Cell 6: Run Experiment R2 (Region + Sex)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Groups by region and sex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a23b4b-84a6-4e0f-8c30-3eaad4489b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING RUNG R2\n",
      "============================================================\n",
      "ðŸ“‹ Rung: R2\n",
      "ðŸ“‹ Groupby: region,sex\n",
      "ðŸ“ Output directory: atus_analysis/data/models/R2\n",
      "âš¡ Include hazard: False\n",
      "\n",
      "ðŸ” Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 746.0GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.8GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "--- ðŸ“Š Step 1: B1-H Model for R2 ---\n",
      "ðŸŽ¯ Attempting direct execution...\n",
      "ðŸ“Š Loading data for R2...\n",
      "âœ“ Loaded 36404352 sequences and 252808 subgroups\n",
      "ðŸ“‚ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "ðŸ”„ Preparing data with groupby: region,sex\n",
      "âŒ Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "ðŸ”„ Falling back to subprocess method...\n",
      "ðŸ–¥ï¸  Using subprocess execution...\n",
      "ðŸ–¥ï¸  Running B1-H via subprocess for R2...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R2 --groupby region,sex --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Output directory: atus_analysis/data/models/R2\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Groupby dimensions: region,sex\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 00:40:01,322 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 00:40:01,322 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Grouping by: ['region', 'sex']\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 01:01:16,057 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-10 01:01:16,341 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:01:25,006 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 01:01:25,146 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 01:01:25,146 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 01:01:55,760 - __main__ - INFO - âœ“ Model fitting completed\n",
      "2025-08-10 01:01:55,768 - __main__ - INFO - Model saved to: atus_analysis/data/models/R2/b1h_model.json\n",
      "2025-08-10 01:01:55,768 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 01:05:28,334 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R2/eval_b1h.json\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - âœ“ B1-H training completed in 1530.26 seconds\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - âœ“ B1-H written to: atus_analysis/data/models/R2\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - ============================================================\n",
      "âœ“ B1-H written to: atus_analysis/data/models/R2\n",
      "âœ… B1-H model completed successfully for R2\n",
      "\n",
      "ðŸŽ‰ R2 COMPLETED SUCCESSFULLY in 1536.8 seconds (25.6 minutes)\n",
      "ðŸ§¹ Memory cleanup completed\n",
      "\n",
      "ðŸŽ‰ R2 completed successfully! You can now run R3.\n"
     ]
    }
   ],
   "source": [
    "# Run R2 experiment\n",
    "success = run_single_rung(\"R2\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nR2 completed successfully! You can now run R3.\")\n",
    "else:\n",
    "    print(\"\\nR2 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482aee4-3677-4527-977c-fc8a3a23f86b",
   "metadata": {},
   "source": [
    "## Cell 7: Run Experiment R3 (Region + Employment)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and employment status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642dcc7-5173-4cac-9fc2-7715f69c02be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING RUNG R3\n",
      "============================================================\n",
      "ðŸ“‹ Rung: R3\n",
      "ðŸ“‹ Groupby: region,employment\n",
      "ðŸ“ Output directory: atus_analysis/data/models/R3\n",
      "âš¡ Include hazard: False\n",
      "\n",
      "ðŸ” Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.8GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "--- ðŸ“Š Step 1: B1-H Model for R3 ---\n",
      "ðŸŽ¯ Attempting direct execution...\n",
      "ðŸ“Š Loading data for R3...\n",
      "âœ“ Loaded 36404352 sequences and 252808 subgroups\n",
      "ðŸ“‚ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "ðŸ”„ Preparing data with groupby: region,employment\n",
      "âŒ Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "ðŸ”„ Falling back to subprocess method...\n",
      "ðŸ–¥ï¸  Using subprocess execution...\n",
      "ðŸ–¥ï¸  Running B1-H via subprocess for R3...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R3 --groupby region,employment --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Output directory: atus_analysis/data/models/R3\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Groupby dimensions: region,employment\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 01:05:38,202 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 01:05:38,202 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Grouping by: ['region', 'employment']\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 01:26:37,684 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 16 groups\n",
      "2025-08-10 01:26:37,999 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:26:48,419 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 01:26:48,557 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 01:26:48,558 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 01:27:19,331 - __main__ - INFO - âœ“ Model fitting completed\n",
      "2025-08-10 01:27:19,345 - __main__ - INFO - Model saved to: atus_analysis/data/models/R3/b1h_model.json\n",
      "2025-08-10 01:27:19,345 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 01:30:52,926 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R3/eval_b1h.json\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - âœ“ B1-H training completed in 1517.99 seconds\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - âœ“ B1-H written to: atus_analysis/data/models/R3\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - ============================================================\n",
      "âœ“ B1-H written to: atus_analysis/data/models/R3\n",
      "âœ… B1-H model completed successfully for R3\n",
      "\n",
      "ðŸŽ‰ R3 COMPLETED SUCCESSFULLY in 1524.7 seconds (25.4 minutes)\n",
      "ðŸ§¹ Memory cleanup completed\n",
      "\n",
      "ðŸŽ‰ R3 completed successfully! You can now run R4.\n"
     ]
    }
   ],
   "source": [
    "success = run_experiment_safely(\n",
    "    'R3',\n",
    "    lambda: run_single_experiment_wrapper(3, dataset_path, args, \n",
    "                                         compute_expected=True)\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nR3 completed successfully! You can now run R4.\")\n",
    "else:\n",
    "    print(\"\\nR3 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003e061-ca9c-45cd-b0ae-9e2d1dac7223",
   "metadata": {},
   "source": [
    "## Cell 8: Run Experiment R4 (Region + Day Type)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and day type (weekday/weekend)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c43458-3883-46bc-b531-3849405cbd07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING RUNG R4\n",
      "============================================================\n",
      "ðŸ“‹ Rung: R4\n",
      "ðŸ“‹ Groupby: region,day_type\n",
      "ðŸ“ Output directory: atus_analysis/data/models/R4\n",
      "âš¡ Include hazard: False\n",
      "\n",
      "ðŸ” Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.7GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "--- ðŸ“Š Step 1: B1-H Model for R4 ---\n",
      "ðŸŽ¯ Attempting direct execution...\n",
      "ðŸ“Š Loading data for R4...\n",
      "âœ“ Loaded 36404352 sequences and 252808 subgroups\n",
      "ðŸ“‚ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "ðŸ”„ Preparing data with groupby: region,day_type\n",
      "âŒ Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "ðŸ”„ Falling back to subprocess method...\n",
      "ðŸ–¥ï¸  Using subprocess execution...\n",
      "ðŸ–¥ï¸  Running B1-H via subprocess for R4...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R4 --groupby region,day_type --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Output directory: atus_analysis/data/models/R4\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Groupby dimensions: region,day_type\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 01:30:59,698 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 01:30:59,698 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 01:30:59,698 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 01:31:02,958 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 01:31:02,958 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Grouping by: ['region', 'day_type']\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 01:52:08,766 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-10 01:52:09,112 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:52:17,733 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 01:52:17,872 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 01:52:17,872 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 01:52:48,749 - __main__ - INFO - âœ“ Model fitting completed\n",
      "2025-08-10 01:52:48,756 - __main__ - INFO - Model saved to: atus_analysis/data/models/R4/b1h_model.json\n",
      "2025-08-10 01:52:48,756 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 01:56:20,560 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R4/eval_b1h.json\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - âœ“ B1-H training completed in 1520.87 seconds\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - âœ“ B1-H written to: atus_analysis/data/models/R4\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - ============================================================\n",
      "âœ“ B1-H written to: atus_analysis/data/models/R4\n",
      "âœ… B1-H model completed successfully for R4\n",
      "\n",
      "ðŸŽ‰ R4 COMPLETED SUCCESSFULLY in 1527.5 seconds (25.5 minutes)\n",
      "ðŸ§¹ Memory cleanup completed\n",
      "\n",
      "ðŸŽ‰ R4 completed successfully! You can now run R5.\n"
     ]
    }
   ],
   "source": [
    "success = run_experiment_safely(\n",
    "    'R4',\n",
    "    lambda: run_single_experiment_wrapper(4, dataset_path, args, \n",
    "                                         compute_expected=True)\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nR4 completed successfully! You can now run R5.\")\n",
    "else:\n",
    "    print(\"\\nR4 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72b726-3099-4764-8e9f-67bdc9c343ee",
   "metadata": {},
   "source": [
    "## Cell 9: Run Experiment R5 (Region + Household Size)\n",
    "\n",
    "**Expected runtime: 60-90 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and household size band**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acf2ea-7a52-4f60-9820-78152c899e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING RUNG R5\n",
      "============================================================\n",
      "ðŸ“‹ Rung: R5\n",
      "ðŸ“‹ Groupby: region,hh_size_band\n",
      "ðŸ“ Output directory: atus_analysis/data/models/R5\n",
      "âš¡ Include hazard: False\n",
      "\n",
      "ðŸ” Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.6GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "--- ðŸ“Š Step 1: B1-H Model for R5 ---\n",
      "ðŸŽ¯ Attempting direct execution...\n",
      "ðŸ“Š Loading data for R5...\n",
      "âœ“ Loaded 36404352 sequences and 252808 subgroups\n",
      "ðŸ“‚ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "ðŸ”„ Preparing data with groupby: region,hh_size_band\n",
      "âŒ Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "ðŸ”„ Falling back to subprocess method...\n",
      "ðŸ–¥ï¸  Using subprocess execution...\n",
      "ðŸ–¥ï¸  Running B1-H via subprocess for R5...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R5 --groupby region,hh_size_band --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Output directory: atus_analysis/data/models/R5\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Groupby dimensions: region,hh_size_band\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 01:56:30,577 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 01:56:30,577 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Grouping by: ['region', 'hh_size_band']\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 02:17:32,440 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 16 groups\n",
      "2025-08-10 02:17:32,960 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 02:17:41,574 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 02:17:41,711 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 02:17:41,711 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 02:18:11,911 - __main__ - INFO - âœ“ Model fitting completed\n",
      "2025-08-10 02:18:11,925 - __main__ - INFO - Model saved to: atus_analysis/data/models/R5/b1h_model.json\n",
      "2025-08-10 02:18:11,925 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 02:21:45,817 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R5/eval_b1h.json\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - ============================================================\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - âœ“ B1-H training completed in 1518.51 seconds\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - âœ“ B1-H written to: atus_analysis/data/models/R5\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - ============================================================\n",
      "âœ“ B1-H written to: atus_analysis/data/models/R5\n",
      "âœ… B1-H model completed successfully for R5\n",
      "\n",
      "ðŸŽ‰ R5 COMPLETED SUCCESSFULLY in 1525.2 seconds (25.4 minutes)\n",
      "ðŸ§¹ Memory cleanup completed\n",
      "\n",
      "ðŸŽ‰ R5 completed successfully! You can now run R6.\n"
     ]
    }
   ],
   "source": [
    "success = run_experiment_safely(\n",
    "    'R5',\n",
    "    lambda: run_single_experiment_wrapper(5, dataset_path, args, \n",
    "                                         compute_expected=True)\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nR5 completed successfully! You can now run R6.\")\n",
    "else:\n",
    "    print(\"\\nR5 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b4517-cebb-49fd-8d65-32e42d2ca040",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 9b: Add Quarter Column to Subgroups File (Run Before R6)\n",
    "\n",
    "**Important**: Run this cell before attempting R6 or R7 experiments.  \n",
    "This will permanently add the quarter column to the subgroups.parquet file.  \n",
    "**Runtime**: 1-2 minutes  \n",
    "**Purpose**: Ensures R6 and R7 experiments have the required quarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc51406-b02a-4332-887f-ca37b9880418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—“ï¸  Adding quarter column to subgroups.parquet file...\n",
      "ðŸ“‚ Loading subgroups from: atus_analysis/data/processed/subgroups.parquet\n",
      "âœ… Loaded 252808 subgroup records\n",
      "Current columns: ['TUCASEID', 'sex', 'hh_size_band', 'month', 'region', 'employment', 'day_type', 'TUFNWGTP']\n",
      "ðŸ“ Quarter column not found - adding it now...\n",
      "ðŸ—“ï¸  Adding quarter column from month data...\n",
      "âœ“ Quarter column added. Distribution:\n",
      "   Q1: 68,017 respondents\n",
      "   Q2: 62,107 respondents\n",
      "   Q3: 61,880 respondents\n",
      "   Q4: 60,804 respondents\n",
      "ðŸ’¾ Creating backup at: atus_analysis/data/processed/subgroups.parquet.backup\n",
      "ðŸ’¾ Saving updated subgroups with quarter column...\n",
      "âœ… Quarter column successfully added to subgroups.parquet!\n",
      "Final quarter distribution:\n",
      "   Q1: 68,017 respondents\n",
      "   Q2: 62,107 respondents\n",
      "   Q3: 61,880 respondents\n",
      "   Q4: 60,804 respondents\n",
      "\n",
      "ðŸ“‹ Summary:\n",
      "   - Original file backed up to: atus_analysis/data/processed/subgroups.parquet.backup\n",
      "   - Updated file saved to: atus_analysis/data/processed/subgroups.parquet\n",
      "   - Quarter column added with 252808 records\n",
      "   - R6 and R7 experiments are now ready to run!\n",
      "\n",
      "============================================================\n",
      "ðŸŽ¯ READY FOR R6 AND R7 EXPERIMENTS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "success = run_experiment_safely(\n",
    "    'R6',\n",
    "    lambda: run_single_experiment_wrapper(6, dataset_path, args, \n",
    "                                         compute_expected=True)\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nR6 completed successfully! You can now run R7.\")\n",
    "else:\n",
    "    print(\"\\nR6 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea1bfd-a23f-4b49-a5c3-9d7685ec9dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 10: Run Experiment R6 (Full Routing Model)\n",
    "\n",
    "**Expected runtime: 90-120 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full complexity routing model with all demographic variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c9b3f-c90d-4354-8e22-36f1a2e8d7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING RUNG R6\n",
      "============================================================\n",
      "ðŸ“‹ Rung: R6\n",
      "ðŸ“‹ Groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "ðŸ“ Output directory: atus_analysis/data/models/R6\n",
      "âš¡ Include hazard: False\n",
      "\n",
      "ðŸ” Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941552.2GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "--- ðŸ“Š Step 1: B1-H Model for R6 ---\n",
      "ðŸŽ¯ Attempting direct execution...\n",
      "ðŸ“Š Loading data for R6...\n",
      "âœ“ Quarter column already exists\n",
      "âœ“ Loaded 36404352 sequences and 252808 subgroups\n",
      "âœ“ Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "ðŸ“‚ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "ðŸ”„ Preparing data with groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "âŒ Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "ðŸ”§ Falling back to subprocess method...\n",
      "ðŸ–¥ï¸  Using subprocess execution...\n",
      "ðŸ–¥ï¸  Running B1-H via subprocess for R6...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R6 --groupby employment,day_type,hh_size_band,sex,region,quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - ============================================================\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - ============================================================\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Output directory: atus_analysis/data/models/R6\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Groupby dimensions: employment,day_type,hh_size_band,sex,region,quarter\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 02:59:28,225 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 02:59:28,225 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Grouping by: ['employment', 'day_type', 'hh_size_band', 'sex', 'region', 'quarter']\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 03:31:14,406 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 1024 groups\n",
      "2025-08-10 03:31:14,811 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 03:31:28,435 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 03:31:28,572 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 03:31:28,572 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 03:32:09,464 - __main__ - INFO - âœ“ Model fitting completed\n",
      "2025-08-10 03:32:10,256 - __main__ - INFO - Model saved to: atus_analysis/data/models/R6/b1h_model.json\n",
      "2025-08-10 03:32:10,256 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 03:35:42,986 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R6/eval_b1h.json\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - ============================================================\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - âœ“ B1-H training completed in 2178.03 seconds\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - âœ“ B1-H written to: atus_analysis/data/models/R6\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - ============================================================\n",
      "âœ“ B1-H written to: atus_analysis/data/models/R6\n",
      "âœ… B1-H model completed successfully for R6\n",
      "\n",
      "ðŸŽ‰ R6 COMPLETED SUCCESSFULLY in 2185.2 seconds (36.4 minutes)\n",
      "ðŸ§¹ Memory cleanup completed\n",
      "\n",
      "ðŸŽ‰ R6 completed successfully! You can now run R7.\n"
     ]
    }
   ],
   "source": [
    "success = run_experiment_safely(\n",
    "    'R7',\n",
    "    lambda: run_single_experiment_wrapper(7, dataset_path, args, \n",
    "                                         compute_expected=True)\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nR7 completed successfully! All experiments finished.\")\n",
    "else:\n",
    "    print(\"\\nR7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94e9ac-59fc-4537-8091-79a5d9edc5a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 11: Run Experiment R7 (Full Model + Hazard)\n",
    "\n",
    "**Expected runtime: 120-180 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full model including hazard modeling - most computationally intensive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b9d75-acdc-47f5-812b-dbcec5a24d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING RUNG R7\n",
      "============================================================\n",
      "ðŸ“‹ Rung: R7\n",
      "ðŸ“‹ Groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "ðŸ“ Output directory: atus_analysis/data/models/R7\n",
      "âš¡ Include hazard: True\n",
      "\n",
      "ðŸ” Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941553.0GB free\n",
      "\n",
      "âœ“ System resources look good\n",
      "\n",
      "--- ðŸ“Š Step 1: B1-H Model for R7 ---\n",
      "ðŸŽ¯ Attempting direct execution...\n",
      "ðŸ“Š Loading data for R7...\n",
      "âœ“ Quarter column already exists\n",
      "âœ“ Loaded 36404352 sequences and 252808 subgroups\n",
      "âœ“ Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "ðŸ“‚ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "ðŸ”„ Preparing data with groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "âŒ Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "ðŸ”§ Falling back to subprocess method...\n",
      "ðŸ–¥ï¸  Using subprocess execution...\n",
      "ðŸ–¥ï¸  Running B1-H via subprocess for R7...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R7 --groupby employment,day_type,hh_size_band,sex,region,quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - ============================================================\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - ============================================================\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Output directory: atus_analysis/data/models/R7\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Groupby dimensions: employment,day_type,hh_size_band,sex,region,quarter\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 04:54:54,913 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 04:54:54,913 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Grouping by: ['employment', 'day_type', 'hh_size_band', 'sex', 'region', 'quarter']\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 05:26:49,127 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 1024 groups\n",
      "2025-08-10 05:26:49,655 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 05:27:02,852 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 05:27:02,993 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 05:27:02,993 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 05:27:44,079 - __main__ - INFO - âœ“ Model fitting completed\n",
      "2025-08-10 05:27:44,890 - __main__ - INFO - Model saved to: atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 05:27:44,891 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 05:31:17,560 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R7/eval_b1h.json\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - ============================================================\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - âœ“ B1-H training completed in 2185.87 seconds\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - âœ“ B1-H written to: atus_analysis/data/models/R7\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - ============================================================\n",
      "âœ“ B1-H written to: atus_analysis/data/models/R7\n",
      "âœ… B1-H model completed successfully for R7\n",
      "\n",
      "--- âš¡ Step 2: B2-H Model for R7 ---\n",
      "ðŸ–¥ï¸  Running B2-H (hazard) model for R7...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline2_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R7 --groupby employment,day_type,hh_size_band,sex,region,quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --dwell_bins 1,2,3,4,6,9,14,20,30 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --b1h_path atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - ============================================================\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - STARTING B2-H (HIERARCHICAL ROUTING + HAZARD) MODEL TRAINING\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - ============================================================\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Output directory: atus_analysis/data/models/R7\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Groupby dimensions: employment,day_type,hh_size_band,sex,region,quarter\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Dwell bins: 1,2,3,4,6,9,14,20,30\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Existing B1-H model: atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Shrinkage params - Route: tau_block=50.0, tau_group=20.0, add_k=1.0\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Shrinkage params - Hazard: tau_block=200.0, tau_group=50.0, k0_global=1.0\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Parsing time blocks and dwell bins...\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Dwell edges: [1, 2, 3, 4, 6, 9, 14, 20, 30]\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 05:31:23,215 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 05:31:23,215 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 05:31:23,275 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 05:31:23,276 - __main__ - INFO - Grouping by: ['employment', 'day_type', 'hh_size_band', 'sex', 'region', 'quarter']\n",
      "2025-08-10 05:31:23,276 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 05:31:23,276 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 06:03:12,288 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 1024 groups\n",
      "2025-08-10 06:03:12,289 - __main__ - INFO - Using split file: atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 06:03:12,845 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 06:03:32,147 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 06:03:32,285 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 06:03:32,285 - __main__ - INFO - Loading existing B1-H model from: atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 06:03:32,513 - __main__ - INFO - âœ“ B1-H model loaded successfully\n",
      "2025-08-10 06:03:32,513 - __main__ - INFO - Fitting B2-H hazard model...\n",
      "2025-08-10 06:05:42,513 - __main__ - INFO - âœ“ B2-H hazard model fitted successfully\n",
      "2025-08-10 06:05:43,788 - __main__ - INFO - B2-H model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7/b2h_model.json\n",
      "2025-08-10 06:05:43,789 - __main__ - INFO - Evaluating models on test set...\n",
      "2025-08-10 06:09:17,552 - __main__ - INFO - B1-H test NLL: N/A\n",
      "2025-08-10 06:13:39,094 - __main__ - INFO - B2-H test NLL: N/A\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7/eval_b2h.json\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - ============================================================\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - âœ“ B2-H training completed in 2539.14 seconds\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - âœ“ B2-H written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - ============================================================\n",
      "âœ“ B2-H run complete â†’ /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7\n",
      "âœ… B2-H model completed successfully for R7\n",
      "\n",
      "ðŸŽ‰ R7 COMPLETED SUCCESSFULLY in 4735.3 seconds (78.9 minutes)\n",
      "ðŸ§¹ Memory cleanup completed\n",
      "\n",
      "ðŸŽ‰ R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Helper functions defined with improved direct execution support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5ab40-c366-49e6-a12c-f22f99438d3a",
   "metadata": {},
   "source": [
    "## Cell 12: Final Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8817df5-d3d8-47c3-a431-4ff02a0c54bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EXPERIMENT SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total rungs: 7\n",
      "Completed: 7 - ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7']\n",
      "Failed: 6 - ['R1', 'R6', 'R6', 'R6', 'R6', 'R7']\n",
      "Not attempted: []\n",
      "\n",
      "=== Timing Information ===\n",
      "R1: 1094 seconds (18.2 minutes)\n",
      "R2: 1537 seconds (25.6 minutes)\n",
      "R3: 1525 seconds (25.4 minutes)\n",
      "R4: 1528 seconds (25.5 minutes)\n",
      "R5: 1525 seconds (25.4 minutes)\n",
      "R6: 2185 seconds (36.4 minutes)\n",
      "R7: 4735 seconds (78.9 minutes)\n",
      "\n",
      "Total runtime: 14129 seconds (235.5 minutes, 3.9 hours)\n",
      "\n",
      "=== Output Files ===\n",
      "R1: 2 files in atus_analysis/data/models/R1\n",
      "R2: 2 files in atus_analysis/data/models/R2\n",
      "R3: 2 files in atus_analysis/data/models/R3\n",
      "R4: 2 files in atus_analysis/data/models/R4\n",
      "R5: 2 files in atus_analysis/data/models/R5\n",
      "R6: 2 files in atus_analysis/data/models/R6\n",
      "R7: 4 files in atus_analysis/data/models/R7\n",
      "\n",
      "Overall success rate: 100.0%\n",
      "\n",
      "ðŸŽ‰ ALL EXPERIMENTS COMPLETED SUCCESSFULLY! ðŸŽ‰\n",
      "\n",
      "Progress file saved as: experiment_progress_jupyter.json\n",
      "Output directory: atus_analysis/data/models\n"
     ]
    }
   ],
   "source": [
    "# Final validation and summary\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check completion status\n",
    "all_rungs = [\"R1\", \"R2\", \"R3\", \"R4\", \"R5\", \"R6\", \"R7\"] \n",
    "completed = []\n",
    "failed = []\n",
    "\n",
    "for rung in all_rungs:\n",
    "    expected_file = OUTPUT_DIR / rung / \"run_summary.json\"\n",
    "    if expected_file.exists():\n",
    "        try:\n",
    "            with open(expected_file) as f:\n",
    "                summary = json.load(f)\n",
    "                if summary.get('status') == 'completed':\n",
    "                    completed.append(rung)\n",
    "                else:\n",
    "                    failed.append(rung)\n",
    "        except:\n",
    "            failed.append(rung)\n",
    "\n",
    "print(f\"=== Final Experiment Status ===\")\n",
    "print(f\"Completed: {completed}\")\n",
    "print(f\"Failed/Incomplete: {failed}\")\n",
    "\n",
    "# Show timing information\n",
    "print(f\"\\n=== Runtime Summary ===\")\n",
    "total_time = 0\n",
    "for rung in completed:\n",
    "    duration_key = f'{rung}_duration_seconds'\n",
    "    if duration_key in progress:\n",
    "        duration = progress[duration_key]\n",
    "        total_time += duration\n",
    "        print(f\"{rung}: {duration:.0f} seconds ({duration/60:.1f} minutes)\")\n",
    "\n",
    "if total_time > 0:\n",
    "    print(f\"\\nTotal runtime: {total_time:.0f} seconds ({total_time/60:.1f} minutes, {total_time/3600:.1f} hours)\")\n",
    "\n",
    "# Show output locations\n",
    "print(\"\\n=== Output Files ===\")\n",
    "for rung in completed:\n",
    "    rung_dir = OUTPUT_DIR / rung\n",
    "    if rung_dir.exists():\n",
    "        files = list(rung_dir.glob(\"*.json\"))\n",
    "        print(f\"{rung}: {len(files)} files in {rung_dir}\")\n",
    "\n",
    "# Success rate\n",
    "if len(completed) + len(failed) > 0:\n",
    "    success_rate = len(completed) / 7 * 100\n",
    "    print(f\"\\nOverall success rate: {success_rate:.1f}%\")\n",
    "\n",
    "if len(completed) == len(all_rungs):\n",
    "    print(\"\\nALL EXPERIMENTS COMPLETED SUCCESSFULLY!\")\n",
    "elif len(failed) > 0:\n",
    "    print(f\"\\nSome experiments failed. You can re-run the failed cells to retry.\")\n",
    "else:\n",
    "    print(f\"\\n{len(all_rungs) - len(completed)} experiments remaining.\")\n",
    "\n",
    "print(f\"\\nProgress file saved as: {PROGRESS_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c4ebd-a80e-4134-9964-32a47ed327a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4a610-b6d2-489d-8cb4-6a5b0bfa6f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6d417-21e3-47e0-805e-bd9636cd9295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
