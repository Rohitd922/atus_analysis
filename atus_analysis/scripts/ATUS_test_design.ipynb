{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c39e102-9f11-4295-a1fc-85ed4a92d4d8",
   "metadata": {},
   "source": [
    "# ATUS Hierarchical Baseline Experiments - HPC Version\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook runs the ATUS (American Time Use Survey) hierarchical baseline experiments safely on HPC systems. It includes 7 individual experiment rungs (R1-R6) that can be run independently.\n",
    "\n",
    "## Experiment Structure\n",
    "\n",
    "- **R1**: Region only\n",
    "- **R2**: Region + Sex\n",
    "- **R3**: Region + Employment\n",
    "- **R4**: Region + Day Type\n",
    "- **R5**: Region + Household Size Band\n",
    "- **R6**: Region + Quarter\n",
    "- **R7**: Region, Sex, Employment, Day Type, HH Size band, Quarter\n",
    "Full routing model (Employment + Day Type + HH Size + Sex + Region + Quarter)\n",
    "\n",
    "- all models run with and without hazard\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run Setup Cells**: Execute cells 1-3 to import libraries and set up the environment\n",
    "2. **Check System Resources**: Run cell 4 to verify your HPC node has sufficient resources\n",
    "3. **Run Individual Experiments**: Execute cells 5-11 one at a time for each rung (R1-R7)\n",
    "4. **Monitor Progress**: Each cell will show detailed progress and can be interrupted safely\n",
    "5. **Resume if Needed**: If interrupted, you can restart from any cell - completed experiments won't be re-run\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **R1-R4**: 30-60 minutes each\n",
    "- **R5-R6**: 60-120 minutes each  \n",
    "- **R7**: 120-180 minutes (includes hazard model)\n",
    "- **Total**: 6-12 hours for all experiments\n",
    "\n",
    "\n",
    "## Resource Requirements\n",
    "\n",
    "- **Memory**: At least 16GB RAM recommended\n",
    "- **Storage**: At least 20GB free disk space\n",
    "- **CPU**: Multi-core recommended for faster processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d0268-3ef4-4574-9ef4-0f6500a2acc7",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4261231a-9036-492e-a95e-d05b7ee91ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\n",
      "Working directory: /ztank/scratch/user/u.rd143338/atus_analysis-main\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import logging\n",
    "import json\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "# psutil is optional; handle gracefully\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "# Set working directory\n",
    "os.chdir('/ztank/scratch/user/u.rd143338/atus_analysis-main')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3e35c-40cf-455e-9e58-ca42f7e25934",
   "metadata": {},
   "source": [
    "## Define Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e99f1e2-2f0f-4f6f-a6be-882dac63c843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set\n",
      "Output directory: atus_analysis/data/models\n",
      "Number of rungs: 22\n",
      "Threads: 170\n"
     ]
    }
   ],
   "source": [
    "# Experiment configuration\n",
    "RUNG_SPECS = {\n",
    "    \"R1\": \"region\",\n",
    "    \"R2\": \"region,sex\", \n",
    "    \"R3\": \"region,employment\",\n",
    "    \"R4\": \"region,day_type\",\n",
    "    \"R5\": \"region,hh_size_band\",\n",
    "    \"R6\": \"region, quarter\",\n",
    "    \"R7\": \"employment,day_type,hh_size_band,sex,region,quarter\",  # + hazard\n",
    "    \n",
    "    \n",
    "    \"R8\":\"sex\",\n",
    "    \"R9\":\"employment\",\n",
    "    \"R10\":\"day_type\",\n",
    "    \"R11\":\"hh_size_band\",\n",
    "    \"R12\":\"quarter\",\n",
    "    \n",
    "    \"R14\": \"employment,sex\", \n",
    "    \"R15\": \"employment,day_type\",\n",
    "    \"R16\": \"employment, hh_size_band\",\n",
    "    \"R17\": \"employment, quarter\",\n",
    "    \n",
    "    \"R18\": \"sex, day_type\",\n",
    "    \"R19\": \"sex, hh_size_band\",\n",
    "    \"R20\": \"sex, quarter\",\n",
    "    \n",
    "    \"R21\": \"day_type, hh_size_band\",\n",
    "    \"R22\": \"day_type, quarter\",\n",
    "    \"R23\": \"quarter, hh_size_band\",\n",
    "}\n",
    "\n",
    "# File paths (adjust if needed)\n",
    "BASE_DIR = Path(\".\")\n",
    "SEQUENCES_FILE = \"atus_analysis/data/sequences/markov_sequences.parquet\"\n",
    "SUBGROUPS_FILE = \"atus_analysis/data/processed/subgroups.parquet\"\n",
    "OUTPUT_DIR = Path(\"atus_analysis/data/models\")\n",
    "PROGRESS_FILE = \"experiment_progress_jupyter.json\"\n",
    "\n",
    "# Experiment settings\n",
    "SEED = 2025\n",
    "TEST_SIZE = 0.2\n",
    "TIME_BLOCKS = \"night:0-5,morning:6-11,afternoon:12-17,evening:18-23\"\n",
    "DWELL_BINS = \"1,2,3,4,6,9,14,20,30\"\n",
    "\n",
    "# 144-slot pooling strength (keep consistent everywhere)\n",
    "KAPPA_SLOT = 50\n",
    "\n",
    "# NEW: how many CPU threads to use (set to your liking)\n",
    "NUM_THREADS = min( (os.cpu_count() or 1), 170 )   # e.g., cap at 40\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Number of rungs: {len(RUNG_SPECS)}\")\n",
    "print(f\"Threads: {NUM_THREADS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6ffe7-45a3-42db-886c-393f0702e40a",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0829560a-7fc9-450d-a38d-848cded9b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined (144-slot matrices enabled; stratified split; NPZ sidecars).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Common-hier APIs (ensure you've replaced common_hier.py with the fixed version)\n",
    "from atus_analysis.scripts.common_hier import (\n",
    "    prepare_long_with_groups, pool_rare_quarter, parse_time_blocks,\n",
    "    fit_b1_hier, nll_b1, save_json,\n",
    ")\n",
    "\n",
    "\n",
    "def _require_globals():\n",
    "    missing = []\n",
    "    for name in [\"SEQUENCES_FILE\",\"SUBGROUPS_FILE\",\"OUTPUT_DIR\",\"TIME_BLOCKS\",\n",
    "                 \"DWELL_BINS\",\"RUNG_SPECS\",\"SEED\",\"TEST_SIZE\",\"KAPPA_SLOT\"]:\n",
    "        if name not in globals():\n",
    "            missing.append(name)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Define these globals before running: {', '.join(missing)}\")\n",
    "\n",
    "def check_system_resources():\n",
    "    \"\"\"Check current system resources (non-fatal).\"\"\"\n",
    "    print(\"=== System Resources ===\")\n",
    "    if psutil is None:\n",
    "        print(\"psutil not available; skipping detailed checks.\")\n",
    "        return True\n",
    "\n",
    "    memory = psutil.virtual_memory()\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "\n",
    "    print(f\"Memory: {memory.percent:.1f}% used, {memory.available / (1024**3):.1f}GB available\")\n",
    "    print(f\"CPU: {cpu_percent:.1f}% usage\")\n",
    "    try:\n",
    "        disk = psutil.disk_usage('.')\n",
    "        print(f\"Disk: {disk.free / (1024**3):.1f}GB free\")\n",
    "    except Exception:\n",
    "        print(\"Disk: Could not check disk usage\")\n",
    "\n",
    "    warnings = []\n",
    "    if memory.available < 4 * (1024**3):\n",
    "        warnings.append(f\"Low memory: only {memory.available / (1024**3):.1f}GB available\")\n",
    "    if cpu_percent > 80:\n",
    "        warnings.append(f\"High CPU usage: {cpu_percent:.1f}%\")\n",
    "\n",
    "    if warnings:\n",
    "        print(\"\\nWARNINGS:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"  - {warning}\")\n",
    "    else:\n",
    "        print(\"\\n✓ System resources look good\")\n",
    "\n",
    "    return True  # non-fatal info\n",
    "\n",
    "def _progress_file() -> Path:\n",
    "    pf = globals().get(\"PROGRESS_FILE\", None)\n",
    "    if pf is None:\n",
    "        # default under OUTPUT_DIR if available, else CWD\n",
    "        if \"OUTPUT_DIR\" in globals():\n",
    "            return Path(OUTPUT_DIR) / \"progress.json\"\n",
    "        return Path.cwd() / \"progress.json\"\n",
    "    return Path(pf)  # <-- ensure Path\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load experiment progress from file.\"\"\"\n",
    "    pf = _progress_file()\n",
    "    if pf.exists():\n",
    "        with open(pf, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'completed_rungs': [], 'failed_rungs': [], 'session_start': datetime.now().isoformat()}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save experiment progress to file.\"\"\"\n",
    "    pf = _progress_file()\n",
    "    progress['last_updated'] = datetime.now().isoformat()\n",
    "    pf.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(pf, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "def is_rung_completed(rung, progress):\n",
    "    \"\"\"Check if a rung has been completed successfully.\"\"\"\n",
    "    return rung in progress.get('completed_rungs', [])\n",
    "\n",
    "def _make_or_load_stratified_split(long_df: pd.DataFrame, split_path: Path, test_size: float, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create or load a respondent-level train/test split STRATIFIED by group_key,\n",
    "    matching the CLI baseline scripts.\n",
    "    \"\"\"\n",
    "    if split_path.exists():\n",
    "        split = pd.read_parquet(split_path)[[\"TUCASEID\",\"set\"]]\n",
    "        if not {\"TUCASEID\",\"set\"}.issubset(split.columns):\n",
    "            raise RuntimeError(f\"Split file {split_path} missing required columns.\")\n",
    "        print(f\"✓ Loaded existing split from {split_path}\")\n",
    "        return split\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    meta = long_df[[\"TUCASEID\",\"group_key\"]].drop_duplicates().copy()\n",
    "    meta[\"rand\"] = rng.rand(len(meta))\n",
    "    meta[\"set\"] = \"train\"\n",
    "\n",
    "    total_test = 0\n",
    "    for _, grp in meta.groupby(\"group_key\"):\n",
    "        n_test = int(round(test_size * len(grp)))\n",
    "        if n_test:\n",
    "            take = grp.sort_values(\"rand\").head(n_test).index\n",
    "            meta.loc[take, \"set\"] = \"test\"\n",
    "            total_test += n_test\n",
    "\n",
    "    out = meta[[\"TUCASEID\",\"set\"]]\n",
    "    split_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_parquet(split_path, index=False)\n",
    "    print(f\"✓ Created stratified split and saved to {split_path} \"\n",
    "          f\"({(out['set']=='test').sum()} test / {len(out)-(out['set']=='test').sum()} train)\")\n",
    "    return out\n",
    "\n",
    "def run_baseline1_hier_direct(rung, groupby, output_dir: Path, split_path: Path):\n",
    "    \"\"\"\n",
    "    Direct (in-notebook) B1-H run that ALWAYS computes 144 per-slot (10-min) matrices.\n",
    "    Writes:\n",
    "      - b1h_model.json\n",
    "      - b1h_slot_mats.npz  (per-slot matrices, arrays g0, g1, ...)\n",
    "      - eval_b1h.json\n",
    "    \"\"\"\n",
    "    _require_globals()\n",
    "    try:\n",
    "        # Optional threads limiter\n",
    "        try:\n",
    "            from threadpoolctl import threadpool_limits\n",
    "        except Exception:\n",
    "            threadpool_limits = None\n",
    "\n",
    "        print(f\"Loading data for {rung} ...\")\n",
    "        sequences = pd.read_parquet(SEQUENCES_FILE)\n",
    "        subgroups = pd.read_parquet(SUBGROUPS_FILE)\n",
    "\n",
    "        # Parse time blocks & group columns\n",
    "        blocks = parse_time_blocks(TIME_BLOCKS)\n",
    "        groupby_cols = [c.strip() for c in groupby.split(\",\") if c.strip()]\n",
    "\n",
    "        # Derive quarter if needed\n",
    "        if \"quarter\" in groupby_cols and \"quarter\" not in subgroups.columns:\n",
    "            try:\n",
    "                from __main__ import add_quarter_column\n",
    "                subgroups = add_quarter_column(subgroups)\n",
    "            except Exception:\n",
    "                if \"month\" in subgroups.columns:\n",
    "                    month_num = pd.to_numeric(subgroups[\"month\"], errors=\"coerce\")\n",
    "                    qmap = {1:\"Q1\",2:\"Q1\",3:\"Q1\",4:\"Q2\",5:\"Q2\",6:\"Q2\",7:\"Q3\",8:\"Q3\",9:\"Q3\",10:\"Q4\",11:\"Q4\",12:\"Q4\"}\n",
    "                    subgroups = subgroups.copy()\n",
    "                    subgroups[\"quarter\"] = month_num.map(qmap).fillna(\"Unknown\")\n",
    "                    print(\"quarter column derived from month (fallback)\")\n",
    "                else:\n",
    "                    print(\"quarter requested but neither 'quarter' nor 'month' present; continuing without it\")\n",
    "\n",
    "        # Pool rare quarter cells if desired (0.0 == no-op)\n",
    "        subgroups = pool_rare_quarter(subgroups, groupby_cols, \"TUFNWGTP\", threshold=0.0)\n",
    "\n",
    "        # Build long table with groups + blocks\n",
    "        long_df = prepare_long_with_groups(\n",
    "            sequences, subgroups, groupby_cols, \"TUFNWGTP\", blocks\n",
    "        )\n",
    "\n",
    "        # Train/test split (STRATIFIED by group_key, shared file)\n",
    "        split_df = _make_or_load_stratified_split(long_df, split_path, TEST_SIZE, SEED)\n",
    "\n",
    "        # Merge split and prepare train/test\n",
    "        df = long_df.merge(split_df, on=\"TUCASEID\", how=\"left\")\n",
    "        if df[\"set\"].isna().any():\n",
    "            missing = df[df[\"set\"].isna()][\"TUCASEID\"].nunique()\n",
    "            raise RuntimeError(f\"{missing} respondents missing train/test assignment in split\")\n",
    "\n",
    "        train_df = df[df[\"set\"] == \"train\"].copy()\n",
    "        test_df  = df[df[\"set\"] == \"test\"].copy()\n",
    "        n_states = int(df[\"state_id\"].max()) + 1\n",
    "\n",
    "        # Threads for BLAS/OpenMP kernels\n",
    "        num_threads = int(globals().get(\"NUM_THREADS\", os.cpu_count() or 1))\n",
    "        print(f\"Using up to {num_threads} threads\")\n",
    "\n",
    "        ctx = (threadpool_limits(limits=num_threads) if threadpool_limits else nullcontext())\n",
    "        # Provide nullcontext() if threadpoolctl is absent\n",
    "        try:\n",
    "            from contextlib import nullcontext\n",
    "        except Exception:\n",
    "            class nullcontext:\n",
    "                def __enter__(self): return None\n",
    "                def __exit__(self, *args): return False\n",
    "\n",
    "        with ctx:\n",
    "            b1_model = fit_b1_hier(\n",
    "                train_df, n_states, \"TUFNWGTP\",\n",
    "                tau_block=50.0, tau_group=20.0, add_k=1.0,\n",
    "                compute_slot_mats=True,                 # always build 144 matrices\n",
    "                kappa_slot=float(globals().get(\"KAPPA_SLOT\", 100.0)),\n",
    "                time_blocks_spec=TIME_BLOCKS\n",
    "            )\n",
    "\n",
    "        # ---- Save model and evaluation (with NPZ sidecar for slot matrices) ----\n",
    "        out_json = output_dir / \"b1h_model.json\"\n",
    "        slot_sidecar = output_dir / \"b1h_slot_mats.npz\"\n",
    "        b1_model_to_save = dict(b1_model)  # shallow copy\n",
    "\n",
    "        if \"slot_matrices\" in b1_model_to_save:\n",
    "            # Convert dict{group_key: [144 × K×K]} → per-group arrays in NPZ\n",
    "            slot_dict = b1_model_to_save.pop(\"slot_matrices\")\n",
    "            groups_order = b1_model_to_save.get(\"meta\", {}).get(\"groups_order\", list(slot_dict.keys()))\n",
    "            arrays = {f\"g{gi}\": np.array(slot_dict[gk], dtype=np.float32) for gi, gk in enumerate(groups_order)}\n",
    "            np.savez_compressed(slot_sidecar, **arrays)  # write compressed sidecar\n",
    "            # reference sidecar in JSON\n",
    "            b1_model_to_save[\"slot_matrices_npz\"] = str(slot_sidecar)\n",
    "            b1_model_to_save.setdefault(\"meta\", {})[\"slot_sidecar\"] = str(slot_sidecar)\n",
    "            b1_model_to_save[\"meta\"][\"groups_order\"] = groups_order\n",
    "\n",
    "        save_json(b1_model_to_save, out_json)\n",
    "        print(f\"Saved model JSON to {out_json}\")\n",
    "        if slot_sidecar.exists():\n",
    "            print(f\"Saved slot matrices sidecar: {slot_sidecar}  ({slot_sidecar.stat().st_size/1024/1024:.1f} MB)\")\n",
    "\n",
    "        # Evaluate on test (use saved dict so sidecar path is available)\n",
    "        eval_result = nll_b1(test_df, b1_model_to_save, n_states, \"TUFNWGTP\")\n",
    "        save_json(eval_result, output_dir / \"eval_b1h.json\")\n",
    "        print(\"B1-H (direct) complete:\", eval_result)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Direct execution failed: {e}\")\n",
    "        print(\"Falling back to subprocess method...\")\n",
    "        return False\n",
    "\n",
    "def run_baseline1_hier_subprocess(rung, groupby, output_dir: Path, split_path: Path):\n",
    "    \"\"\"Run baseline1_hier via subprocess (fallback method).\"\"\"\n",
    "    _require_globals()\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"atus_analysis.scripts.baseline1_hier\",\n",
    "        \"--sequences\", str(SEQUENCES_FILE),\n",
    "        \"--subgroups\", str(SUBGROUPS_FILE),\n",
    "        \"--out_dir\", str(output_dir),\n",
    "        \"--groupby\", groupby,\n",
    "        \"--time_blocks\", TIME_BLOCKS,\n",
    "        \"--seed\", str(SEED),\n",
    "        \"--test_size\", str(TEST_SIZE),\n",
    "        \"--split_path\", str(split_path),\n",
    "        \"--kappa_slot\", str(KAPPA_SLOT),  # ensure 144 slot matrices\n",
    "    ]\n",
    "    print(f\"Running B1-H via subprocess for {rung}...\")\n",
    "    print(\"Command:\", \" \".join(cmd))\n",
    "\n",
    "    # export BLAS/OpenMP thread counts to the child process\n",
    "    env = os.environ.copy()\n",
    "    num_threads = str(globals().get(\"NUM_THREADS\", os.cpu_count() or 1))\n",
    "    env.update({\n",
    "        \"OMP_NUM_THREADS\": num_threads,\n",
    "        \"OPENBLAS_NUM_THREADS\": num_threads,\n",
    "        \"MKL_NUM_THREADS\": num_threads,\n",
    "        \"NUMEXPR_NUM_THREADS\": num_threads,\n",
    "        \"VECLIB_MAXIMUM_THREADS\": num_threads,\n",
    "        \"OMP_WAIT_POLICY\": \"PASSIVE\",\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True, bufsize=1, env=env\n",
    "        )\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        process.wait()\n",
    "        if process.returncode == 0:\n",
    "            print(f\"B1-H model completed successfully for {rung}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"B1-H model failed for {rung} (return code: {process.returncode})\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Subprocess execution failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_baseline1_hier(rung, groupby, output_dir: Path, split_path: Path):\n",
    "    \"\"\"Run baseline1_hier with automatic fallback.\"\"\"\n",
    "    use_sub = bool(globals().get('USE_SUBPROCESS', True))\n",
    "    if not use_sub:\n",
    "        print(\"Attempting direct execution...\")\n",
    "        if run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "            return True\n",
    "    print(\"Using subprocess execution...\")\n",
    "    return run_baseline1_hier_subprocess(rung, groupby, output_dir, split_path)\n",
    "\n",
    "def run_baseline2_hier(rung, groupby, output_dir: Path, split_path: Path):\n",
    "    \"\"\"Run baseline2_hier (hazard model) for a rung.\"\"\"\n",
    "    _require_globals()\n",
    "    b1h_path = output_dir / \"b1h_model.json\"\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"atus_analysis.scripts.baseline2_hier\",\n",
    "        \"--sequences\", str(SEQUENCES_FILE),\n",
    "        \"--subgroups\", str(SUBGROUPS_FILE),\n",
    "        \"--out_dir\", str(output_dir),\n",
    "        \"--groupby\", groupby,\n",
    "        \"--time_blocks\", TIME_BLOCKS,\n",
    "        \"--dwell_bins\", DWELL_BINS,\n",
    "        \"--seed\", str(SEED),\n",
    "        \"--test_size\", str(TEST_SIZE),\n",
    "        \"--split_path\", str(split_path),\n",
    "        \"--b1h_path\", str(b1h_path),\n",
    "        \"--kappa_slot\", str(KAPPA_SLOT),  # ensure routing inside B2 has 144 slots too\n",
    "    ]\n",
    "    print(f\"Running B2-H (hazard) model for {rung}...\")\n",
    "    print(\"Command:\", \" \".join(cmd))\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    num_threads = str(globals().get(\"NUM_THREADS\", os.cpu_count() or 1))\n",
    "    env.update({\n",
    "        \"OMP_NUM_THREADS\": num_threads,\n",
    "        \"OPENBLAS_NUM_THREADS\": num_threads,\n",
    "        \"MKL_NUM_THREADS\": num_threads,\n",
    "        \"NUMEXPR_NUM_THREADS\": num_threads,\n",
    "        \"VECLIB_MAXIMUM_THREADS\": num_threads,\n",
    "        \"OMP_WAIT_POLICY\": \"PASSIVE\",\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True, bufsize=1, env=env\n",
    "        )\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        process.wait()\n",
    "        if process.returncode == 0:\n",
    "            print(f\"B2-H model completed successfully for {rung}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"B2-H model failed for {rung} (return code: {process.returncode})\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"B2-H execution failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_single_rung(rung, include_hazard=False):\n",
    "    \"\"\"Run a complete experiment for a single rung; B2-H auto-enabled for R7.\"\"\"\n",
    "    _require_globals()\n",
    "    from contextlib import suppress\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STARTING RUNG {rung}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    progress = load_progress()\n",
    "    if is_rung_completed(rung, progress):\n",
    "        print(f\"{rung} already completed - skipping\")\n",
    "        return True\n",
    "\n",
    "    groupby = RUNG_SPECS[rung]\n",
    "    output_dir = OUTPUT_DIR / rung\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    split_path = OUTPUT_DIR / \"fixed_split.parquet\"\n",
    "\n",
    "    print(f\" Rung: {rung}\")\n",
    "    print(f\" Groupby: {groupby}\")\n",
    "    print(f\" Output directory: {output_dir}\")\n",
    "    print(f\" Include hazard: {include_hazard or rung == 'R7'}\")\n",
    "\n",
    "    print(\"\\nChecking system resources...\")\n",
    "    with suppress(Exception):\n",
    "        check_system_resources()\n",
    "\n",
    "    try:\n",
    "        # Step 1: B1-H\n",
    "        print(f\"\\n---  Step 1: B1-H Model for {rung} ---\")\n",
    "        if not run_baseline1_hier(rung, groupby, output_dir, split_path):\n",
    "            progress['failed_rungs'].append(rung)\n",
    "            save_progress(progress)\n",
    "            return False\n",
    "\n",
    "        # Step 2: B2-H (if requested or rung is R7)\n",
    "        if include_hazard or rung == \"R7\":\n",
    "            print(f\"\\n---  Step 2: B2-H Model for {rung} ---\")\n",
    "            if not run_baseline2_hier(rung, groupby, output_dir, split_path):\n",
    "                progress['failed_rungs'].append(rung)\n",
    "                save_progress(progress)\n",
    "                return False\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n{rung} COMPLETED SUCCESSFULLY in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "        progress['completed_rungs'].append(rung)\n",
    "        progress[f'{rung}_completed_at'] = datetime.now().isoformat()\n",
    "        progress[f'{rung}_duration_seconds'] = elapsed\n",
    "        save_progress(progress)\n",
    "\n",
    "        with suppress(Exception):\n",
    "            gc.collect()\n",
    "            print(\"Memory cleanup completed\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{rung} FAILED with exception: {e}\")\n",
    "        progress['failed_rungs'].append(rung)\n",
    "        progress[f'{rung}_error'] = str(e)\n",
    "        save_progress(progress)\n",
    "        return False\n",
    "\n",
    "print(\"Helper functions defined (144-slot matrices enabled; stratified split; NPZ sidecars).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d4e59c-9891-4b1e-a7f5-53b2be7128cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_quarter_column(subgroups_df):\n",
    "    \"\"\"\n",
    "    Add quarter column to subgroups data if missing.\n",
    "    \n",
    "    This is needed for R6 and R7 experiments which use quarter in their groupby.\n",
    "    Derives quarter from month: Q1=Jan-Mar, Q2=Apr-Jun, Q3=Jul-Sep, Q4=Oct-Dec\n",
    "    \"\"\"\n",
    "    if 'quarter' in subgroups_df.columns:\n",
    "        print(\"✓ Quarter column already exists\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    if 'month' not in subgroups_df.columns:\n",
    "        print(\" Neither quarter nor month column found!\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    print(\"  Adding quarter column from month data...\")\n",
    "    df = subgroups_df.copy()\n",
    "    \n",
    "    # Convert month to numeric if it's string\n",
    "    month_numeric = pd.to_numeric(df['month'], errors='coerce')\n",
    "    \n",
    "    # Create quarter mapping: Q1=1-3, Q2=4-6, Q3=7-9, Q4=10-12\n",
    "    quarter_map = {\n",
    "        1: 'Q1', 2: 'Q1', 3: 'Q1',\n",
    "        4: 'Q2', 5: 'Q2', 6: 'Q2', \n",
    "        7: 'Q3', 8: 'Q3', 9: 'Q3',\n",
    "        10: 'Q4', 11: 'Q4', 12: 'Q4'\n",
    "    }\n",
    "    \n",
    "    df['quarter'] = month_numeric.map(quarter_map).fillna('Unknown')\n",
    "    \n",
    "    print(f\" Quarter column added. Distribution:\")\n",
    "    quarter_counts = df['quarter'].value_counts().sort_index()\n",
    "    for quarter, count in quarter_counts.items():\n",
    "        print(f\"   {quarter}: {count:,} respondents\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc4ed-ab5a-4e42-86d1-84e4d2042ddd",
   "metadata": {},
   "source": [
    "## Cell 3b: Import Baseline Scripts Directly\n",
    "\n",
    "Instead of calling external processes, we'll import the baseline scripts directly for better integration with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8619ea-d557-4291-aed5-364acdb7504a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully imported baseline common functions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add the project root to Python path so we can import the modules\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    # Import the baseline functions directly\n",
    "    from atus_analysis.scripts.common_hier import (\n",
    "        prepare_long_with_groups, pool_rare_quarter, \n",
    "        save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "    )\n",
    "    print(\" Successfully imported baseline common functions\")\n",
    "    \n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Could not import baseline functions directly: {e}\")\n",
    "    print(\"Will fall back to subprocess calls\")\n",
    "    USE_SUBPROCESS = True\n",
    "else:\n",
    "    USE_SUBPROCESS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8639cc8b-0bb8-4ca4-8e0e-d8341b23180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess mode: True\n",
      "KAPPA_SLOT = 50\n"
     ]
    }
   ],
   "source": [
    "# Always use subprocesses (matches your script CLIs & avoids in-notebook state issues)\n",
    "USE_SUBPROCESS = True\n",
    "print(\"Subprocess mode:\", USE_SUBPROCESS)\n",
    "\n",
    "# sanity check the smoothing strength used for 144 slot matrices\n",
    "print(\"KAPPA_SLOT =\", KAPPA_SLOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8672f3-8031-47bd-91c2-3728eea65c17",
   "metadata": {},
   "source": [
    "## Check System Status and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44daad3a-db50-4af4-8f20-a4cda6b1ce23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking system status and prerequisites...\n",
      "\n",
      "=== System Resources ===\n",
      "Memory: 1.1% used, 746.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1947898.3GB free\n",
      "\n",
      "✓ System resources look good\n",
      "\n",
      "=== File Prerequisites ===\n",
      " atus_analysis/data/sequences/markov_sequences.parquet\n",
      " atus_analysis/data/processed/subgroups.parquet\n",
      " atus_analysis/scripts/baseline1_hier.py\n",
      " atus_analysis/scripts/baseline2_hier.py\n",
      " Output directory: atus_analysis/data/models\n",
      "\n",
      "=== Current Progress ===\n",
      "Completed rungs: ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11', 'R12']\n",
      "Failed rungs: None\n",
      "Remaining rungs: ['R14', 'R15', 'R16', 'R17', 'R18', 'R19', 'R20', 'R21', 'R22', 'R23']\n",
      "\n",
      "=== Overall Status ===\n",
      " READY TO START EXPERIMENTS\n",
      "\n",
      "You can now run the individual experiment cells below.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking system status and prerequisites...\\n\")\n",
    "\n",
    "# Check system resources\n",
    "resources_ok = check_system_resources()\n",
    "\n",
    "print(\"\\n=== File Prerequisites ===\")\n",
    "# Check required files\n",
    "required_files = [\n",
    "    SEQUENCES_FILE,\n",
    "    SUBGROUPS_FILE,\n",
    "    \"atus_analysis/scripts/baseline1_hier.py\",\n",
    "    \"atus_analysis/scripts/baseline2_hier.py\"\n",
    "]\n",
    "\n",
    "files_ok = True\n",
    "for file_path in required_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\" {file_path}\")\n",
    "    else:\n",
    "        print(f\" {file_path} - NOT FOUND\")\n",
    "        files_ok = False\n",
    "\n",
    "# Check output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load and display current progress\n",
    "print(\"\\n=== Current Progress ===\")\n",
    "progress = load_progress()\n",
    "completed = progress.get('completed_rungs', [])\n",
    "failed = progress.get('failed_rungs', [])\n",
    "\n",
    "print(f\"Completed rungs: {completed if completed else 'None'}\")\n",
    "print(f\"Failed rungs: {failed if failed else 'None'}\")\n",
    "print(f\"Remaining rungs: {[r for r in RUNG_SPECS.keys() if r not in completed]}\")\n",
    "\n",
    "# Overall status\n",
    "print(\"\\n=== Overall Status ===\")\n",
    "if resources_ok and files_ok:\n",
    "    print(\" READY TO START EXPERIMENTS\")\n",
    "    print(\"\\nYou can now run the individual experiment cells below.\")\n",
    "else:\n",
    "    print(\"PREREQUISITES NOT MET\")\n",
    "    if not resources_ok:\n",
    "        print(\"   - System resources may be insufficient\")\n",
    "    if not files_ok:\n",
    "        print(\"   - Required files are missing\")\n",
    "    print(\"\\nPlease resolve issues before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad4c12-a087-47e3-8766-ed1032e94f34",
   "metadata": {},
   "source": [
    "# Individual Experiment Cells\n",
    "\n",
    "## Instructions for Running Experiments\n",
    "\n",
    "**Run these cells ONE AT A TIME** in order. Each cell represents one complete experiment rung.\n",
    "\n",
    "- ✅ **Safe to interrupt**: You can stop any cell with the stop button - progress is automatically saved\n",
    "- 🔄 **Resume anytime**: If you restart the kernel, just re-run cells 1-4, then continue from where you left off\n",
    "- ⏭️ **Skip completed**: Cells will automatically skip rungs that have already completed successfully\n",
    "- 📊 **Monitor progress**: Each cell shows detailed progress and resource usage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38d17c-292a-4281-ba26-3dcf7217b238",
   "metadata": {},
   "source": [
    "## Cell 5: Run Experiment R1 (Region Only)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Simplest model - groups by region only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e91988ab-5442-4aa5-9ed6-cfb0b9d9f939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R1\n",
      "============================================================\n",
      "R1 already completed - skipping\n",
      "\n",
      " R1 completed successfully! You can now run R2.\n"
     ]
    }
   ],
   "source": [
    "# Run R1 experiment\n",
    "success = run_single_rung(\"R1\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R1 completed successfully! You can now run R2.\")\n",
    "else:\n",
    "    print(\"\\n R1 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8749bab-46d0-46b8-9925-3cb8c73c34a9",
   "metadata": {},
   "source": [
    "## Cell 6: Run Experiment R2 (Region + Sex)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Groups by region and sex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a23b4b-84a6-4e0f-8c30-3eaad4489b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R2\n",
      "============================================================\n",
      "R2 already completed - skipping\n",
      "\n",
      " R2 completed successfully! You can now run R3.\n"
     ]
    }
   ],
   "source": [
    "# Run R2 experiment\n",
    "success = run_single_rung(\"R2\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R2 completed successfully! You can now run R3.\")\n",
    "else:\n",
    "    print(\"\\n R2 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482aee4-3677-4527-977c-fc8a3a23f86b",
   "metadata": {},
   "source": [
    "## Cell 7: Run Experiment R3 (Region + Employment)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and employment status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d642dcc7-5173-4cac-9fc2-7715f69c02be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R3\n",
      "============================================================\n",
      "R3 already completed - skipping\n",
      "\n",
      " R3 completed successfully! You can now run R4.\n"
     ]
    }
   ],
   "source": [
    "# Run R3 experiment\n",
    "success = run_single_rung(\"R3\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R3 completed successfully! You can now run R4.\")\n",
    "else:\n",
    "    print(\"\\n R3 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003e061-ca9c-45cd-b0ae-9e2d1dac7223",
   "metadata": {},
   "source": [
    "## Cell 8: Run Experiment R4 (Region + Day Type)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and day type (weekday/weekend)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c43458-3883-46bc-b531-3849405cbd07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R4\n",
      "============================================================\n",
      "R4 already completed - skipping\n",
      "\n",
      " R4 completed successfully! You can now run R5.\n"
     ]
    }
   ],
   "source": [
    "# Run R4 experiment\n",
    "success = run_single_rung(\"R4\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R4 completed successfully! You can now run R5.\")\n",
    "else:\n",
    "    print(\"\\n R4 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72b726-3099-4764-8e9f-67bdc9c343ee",
   "metadata": {},
   "source": [
    "## Cell 9: Run Experiment R5 (Region + Household Size)\n",
    "\n",
    "**Expected runtime: 60-90 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and household size band**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1acf2ea-7a52-4f60-9820-78152c899e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R5\n",
      "============================================================\n",
      "R5 already completed - skipping\n",
      "\n",
      " R5 completed successfully! You can now run R6.\n"
     ]
    }
   ],
   "source": [
    "# Run R5 experiment\n",
    "success = run_single_rung(\"R5\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R5 completed successfully! You can now run R6.\")\n",
    "else:\n",
    "    print(\"\\n R5 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424fd31-8f9e-4dc8-b1b7-a40777434331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339b4517-cebb-49fd-8d65-32e42d2ca040",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 9b: Add Quarter Column to Subgroups File (Run Before R6)\n",
    "\n",
    "**Important**: Run this cell before attempting R6 or R7 experiments.  \n",
    "This will permanently add the quarter column to the subgroups.parquet file.  \n",
    "**Runtime**: 1-2 minutes  \n",
    "**Purpose**: Ensures R6 and R7 experiments have the required quarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc51406-b02a-4332-887f-ca37b9880418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Add quarter column permanently to subgroups.parquet file\n",
    "# print(\"  Adding quarter column to subgroups.parquet file...\")\n",
    "\n",
    "# try:\n",
    "#     # Load the current subgroups file\n",
    "#     subgroups_path = Path(SUBGROUPS_FILE)\n",
    "#     print(f\" Loading subgroups from: {subgroups_path}\")\n",
    "    \n",
    "#     if not subgroups_path.exists():\n",
    "#         print(f\" Subgroups file not found at {subgroups_path}\")\n",
    "#         print(\"Please ensure the file exists before running this cell.\")\n",
    "#     else:\n",
    "#         # Load the data\n",
    "#         subgroups_df = pd.read_parquet(subgroups_path)\n",
    "#         print(f\" Loaded {len(subgroups_df)} subgroup records\")\n",
    "#         print(f\"Current columns: {list(subgroups_df.columns)}\")\n",
    "        \n",
    "#         # Check if quarter column already exists\n",
    "#         if 'quarter' in subgroups_df.columns:\n",
    "#             print(\" Quarter column already exists in the file!\")\n",
    "#             quarter_counts = subgroups_df['quarter'].value_counts().sort_index()\n",
    "#             print(\"Current quarter distribution:\")\n",
    "#             for quarter, count in quarter_counts.items():\n",
    "#                 print(f\"   {quarter}: {count:,} respondents\")\n",
    "#         else:\n",
    "#             print(\" Quarter column not found - adding it now...\")\n",
    "            \n",
    "#             # Add quarter column using our function\n",
    "#             subgroups_with_quarter = add_quarter_column(subgroups_df)\n",
    "            \n",
    "#             # Create backup of original file\n",
    "#             backup_path = subgroups_path.with_suffix('.parquet.backup')\n",
    "#             print(f\" Creating backup at: {backup_path}\")\n",
    "#             subgroups_df.to_parquet(backup_path, index=False)\n",
    "            \n",
    "#             # Save the updated file\n",
    "#             print(f\" Saving updated subgroups with quarter column...\")\n",
    "#             subgroups_with_quarter.to_parquet(subgroups_path, index=False)\n",
    "            \n",
    "#             # Verify the save\n",
    "#             verification_df = pd.read_parquet(subgroups_path)\n",
    "#             if 'quarter' in verification_df.columns:\n",
    "#                 print(\" Quarter column successfully added to subgroups.parquet!\")\n",
    "#                 quarter_counts = verification_df['quarter'].value_counts().sort_index()\n",
    "#                 print(\"Final quarter distribution:\")\n",
    "#                 for quarter, count in quarter_counts.items():\n",
    "#                     print(f\"   {quarter}: {count:,} respondents\")\n",
    "                    \n",
    "#                 print(f\"\\n Summary:\")\n",
    "#                 print(f\"   - Original file backed up to: {backup_path}\")\n",
    "#                 print(f\"   - Updated file saved to: {subgroups_path}\")\n",
    "#                 print(f\"   - Quarter column added with {len(verification_df)} records\")\n",
    "#                 print(f\"   - R6 and R7 experiments are now ready to run!\")\n",
    "#             else:\n",
    "#                 print(\" Failed to verify quarter column in saved file\")\n",
    "                \n",
    "# except Exception as e:\n",
    "#     print(f\" Error adding quarter column: {e}\")\n",
    "#     print(f\"Error type: {type(e).__name__}\")\n",
    "#     print(\"\\nYou can still run R6/R7 - the quarter column will be added dynamically.\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\" READY FOR R6 AND R7 EXPERIMENTS\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea1bfd-a23f-4b49-a5c3-9d7685ec9dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 10: Run Experiment R6 \n",
    "\n",
    "**Expected runtime: 90-120 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full complexity routing model with all demographic variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2c9b3f-c90d-4354-8e22-36f1a2e8d7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R6\n",
      "============================================================\n",
      "R6 already completed - skipping\n",
      "\n",
      " R6 completed successfully! You can now run R7.\n"
     ]
    }
   ],
   "source": [
    "# Run R6 experiment\n",
    "success = run_single_rung(\"R6\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R6 completed successfully! You can now run R7.\")\n",
    "else:\n",
    "    print(\"\\n R6 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94e9ac-59fc-4537-8091-79a5d9edc5a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 11: Run Experiment R7 (Full Model + Hazard)\n",
    "\n",
    "**Expected runtime: 120-180 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full model including hazard modeling - most computationally intensive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701b9d75-acdc-47f5-812b-dbcec5a24d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R7\n",
      "============================================================\n",
      "R7 already completed - skipping\n",
      "\n",
      " R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R7\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c166731-19fd-4427-b3db-b8e3fdb0c3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R8\n",
      "============================================================\n",
      "R8 already completed - skipping\n",
      "\n",
      " R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R8\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0010305-d6a4-42f0-8ecf-e610a1bc5e63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R9\n",
      "============================================================\n",
      "R9 already completed - skipping\n",
      "\n",
      " R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R9\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e99256f2-983c-421f-b54d-9344f88bfa64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R10\n",
      "============================================================\n",
      "R10 already completed - skipping\n",
      "\n",
      " R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R10\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e59cda-6d41-4ed2-abb6-ceee367be284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R11\n",
      "============================================================\n",
      "R11 already completed - skipping\n",
      "\n",
      " R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R11\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76358fba-f788-4e79-9502-4af84eb3dc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R12\n",
      "============================================================\n",
      "R12 already completed - skipping\n",
      "\n",
      " R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R12\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c569aa-c4e1-4d7b-8bf4-494fd882df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RUNG R14\n",
      "============================================================\n",
      " Rung: R14\n",
      " Groupby: employment,sex\n",
      " Output directory: atus_analysis/data/models/R14\n",
      " Include hazard: True\n",
      "\n",
      "Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.1% used, 746.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1947898.3GB free\n",
      "\n",
      "✓ System resources look good\n",
      "\n",
      "---  Step 1: B1-H Model for R14 ---\n",
      "Using subprocess execution...\n",
      "Running B1-H via subprocess for R14...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R14 --groupby employment,sex --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 20:02:19,799 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:02:19,799 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-30 20:02:19,799 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:02:19,799 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 20:02:19,799 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 20:02:19,799 - __main__ - INFO - Output directory: atus_analysis/data/models/R14\n",
      "2025-08-30 20:02:19,799 - __main__ - INFO - Groupby dimensions: employment,sex\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Per-slot pooling - kappa_slot: 50.0\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 20:02:19,800 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 20:02:21,233 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 20:02:21,233 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 20:02:21,281 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 20:02:21,281 - __main__ - INFO - Grouping by: ['employment', 'sex']\n",
      "2025-08-30 20:02:21,282 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 20:02:21,282 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-30 20:23:21,279 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-30 20:23:21,533 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 20:23:35,087 - __main__ - INFO - Data split: 29,123,424 train records, 7,280,928 test records\n",
      "2025-08-30 20:23:35,223 - __main__ - INFO - Train respondents: 202,246, Test respondents: 50,562\n",
      "2025-08-30 20:23:36,470 - __main__ - INFO - Fitting B1-H hierarchical model (with 144 per-slot matrices)...\n",
      "2025-08-30 20:24:00,606 - __main__ - INFO - ✓ Model fitting completed\n",
      "2025-08-30 20:24:00,651 - __main__ - INFO - Model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/b1h_model.json\n",
      "2025-08-30 20:24:00,652 - __main__ - INFO - Slot matrices sidecar saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/b1h_slot_mats.npz (size=0.8 MB)\n",
      "2025-08-30 20:24:00,652 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-30 20:26:42,996 - __main__ - INFO - B1-H test NLL (per weight): 0.4534\n",
      "2025-08-30 20:26:42,998 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/eval_b1h.json\n",
      "2025-08-30 20:26:42,998 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:26:42,998 - __main__ - INFO -  B1-H training completed in 1463.20 seconds\n",
      "2025-08-30 20:26:42,998 - __main__ - INFO -  Written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14\n",
      "2025-08-30 20:26:42,998 - __main__ - INFO - ============================================================\n",
      " B1-H run complete → /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14\n",
      "B1-H model completed successfully for R14\n",
      "\n",
      "---  Step 2: B2-H Model for R14 ---\n",
      "Running B2-H (hazard) model for R14...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline2_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R14 --groupby employment,sex --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --dwell_bins 1,2,3,4,6,9,14,20,30 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --b1h_path atus_analysis/data/models/R14/b1h_model.json --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 20:26:45,517 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - STARTING B2-H (HIERARCHICAL ROUTING + HAZARD) MODEL TRAINING\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Output directory: atus_analysis/data/models/R14\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Groupby dimensions: employment,sex\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Dwell bins: 1,2,3,4,6,9,14,20,30\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Existing B1-H model: atus_analysis/data/models/R14/b1h_model.json\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Shrinkage params - Route: tau_block=50.0, tau_group=20.0, add_k=1.0, kappa_slot=50.0\n",
      "2025-08-30 20:26:45,517 - __main__ - INFO - Shrinkage params - Hazard: tau_block=200.0, tau_group=50.0, k0_global=1.0\n",
      "2025-08-30 20:26:45,518 - __main__ - INFO - Parsing time blocks and dwell bins...\n",
      "2025-08-30 20:26:45,518 - __main__ - INFO - Time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 20:26:45,518 - __main__ - INFO - Dwell edges: [1, 2, 3, 4, 6, 9, 14, 20, 30]\n",
      "2025-08-30 20:26:45,518 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 20:26:46,937 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 20:26:46,937 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 20:26:46,985 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 20:26:46,985 - __main__ - INFO - Grouping by: ['employment', 'sex']\n",
      "2025-08-30 20:26:46,985 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 20:26:46,985 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-30 20:47:42,820 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-30 20:47:42,821 - __main__ - INFO - Using split file: atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 20:47:43,113 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 20:47:56,933 - __main__ - INFO - Data split: 29,123,424 train records, 7,280,928 test records\n",
      "2025-08-30 20:47:57,068 - __main__ - INFO - Train respondents: 202,246, Test respondents: 50,562\n",
      "2025-08-30 20:47:58,363 - __main__ - INFO - Loading existing B1-H model from: atus_analysis/data/models/R14/b1h_model.json\n",
      "2025-08-30 20:47:58,406 - __main__ - INFO - B1-H (ensured with per-slot matrices & sidecar) saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/b1h_model.json\n",
      "2025-08-30 20:47:58,406 - __main__ - INFO - Slot matrices sidecar present: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/b1h_slot_mats.npz (size=0.8 MB)\n",
      "2025-08-30 20:47:58,406 - __main__ - INFO - Fitting B2-H hazard model...\n",
      "2025-08-30 20:48:55,600 - __main__ - INFO - B2-H hazard model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/b2h_model.json\n",
      "2025-08-30 20:48:55,600 - __main__ - INFO - Evaluating models on test set...\n",
      "2025-08-30 20:51:35,180 - __main__ - INFO - B1-H test NLL (per weight): 0.4534\n",
      "2025-08-30 20:54:57,365 - __main__ - INFO - B2-H test NLL (per weight): 0.4488\n",
      "2025-08-30 20:54:57,366 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/eval_b2h.json\n",
      "2025-08-30 20:54:57,366 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:54:57,366 - __main__ - INFO -  B2-H training completed in 1691.85 seconds\n",
      "2025-08-30 20:54:57,366 - __main__ - INFO -  B2-H written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14\n",
      "2025-08-30 20:54:57,366 - __main__ - INFO - ============================================================\n",
      " B2-H run complete → /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14\n",
      "B2-H model completed successfully for R14\n",
      "\n",
      "R14 COMPLETED SUCCESSFULLY in 3160.9 seconds (52.7 minutes)\n",
      "Memory cleanup completed\n",
      "\n",
      " R14 completed successfully! All experiments are now complete.\n",
      "\n",
      "============================================================\n",
      "STARTING RUNG R15\n",
      "============================================================\n",
      " Rung: R15\n",
      " Groupby: employment,day_type\n",
      " Output directory: atus_analysis/data/models/R15\n",
      " Include hazard: True\n",
      "\n",
      "Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.1% used, 746.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1947898.3GB free\n",
      "\n",
      "✓ System resources look good\n",
      "\n",
      "---  Step 1: B1-H Model for R15 ---\n",
      "Using subprocess execution...\n",
      "Running B1-H via subprocess for R15...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R15 --groupby employment,day_type --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 20:55:00,748 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - ============================================================\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Output directory: atus_analysis/data/models/R15\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Groupby dimensions: employment,day_type\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Per-slot pooling - kappa_slot: 50.0\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 20:55:00,748 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 20:55:02,137 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 20:55:02,137 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 20:55:02,193 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 20:55:02,193 - __main__ - INFO - Grouping by: ['employment', 'day_type']\n",
      "2025-08-30 20:55:02,193 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 20:55:02,193 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-30 21:16:12,342 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-30 21:16:12,843 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 21:16:26,888 - __main__ - INFO - Data split: 29,123,424 train records, 7,280,928 test records\n",
      "2025-08-30 21:16:27,022 - __main__ - INFO - Train respondents: 202,246, Test respondents: 50,562\n",
      "2025-08-30 21:16:28,386 - __main__ - INFO - Fitting B1-H hierarchical model (with 144 per-slot matrices)...\n",
      "2025-08-30 21:16:52,396 - __main__ - INFO - ✓ Model fitting completed\n",
      "2025-08-30 21:16:52,439 - __main__ - INFO - Model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/b1h_model.json\n",
      "2025-08-30 21:16:52,439 - __main__ - INFO - Slot matrices sidecar saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/b1h_slot_mats.npz (size=0.8 MB)\n",
      "2025-08-30 21:16:52,439 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-30 21:19:33,206 - __main__ - INFO - B1-H test NLL (per weight): 0.4531\n",
      "2025-08-30 21:19:33,207 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/eval_b1h.json\n",
      "2025-08-30 21:19:33,207 - __main__ - INFO - ============================================================\n",
      "2025-08-30 21:19:33,207 - __main__ - INFO -  B1-H training completed in 1472.46 seconds\n",
      "2025-08-30 21:19:33,207 - __main__ - INFO -  Written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15\n",
      "2025-08-30 21:19:33,207 - __main__ - INFO - ============================================================\n",
      " B1-H run complete → /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15\n",
      "B1-H model completed successfully for R15\n",
      "\n",
      "---  Step 2: B2-H Model for R15 ---\n",
      "Running B2-H (hazard) model for R15...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline2_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R15 --groupby employment,day_type --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --dwell_bins 1,2,3,4,6,9,14,20,30 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --b1h_path atus_analysis/data/models/R15/b1h_model.json --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 21:19:35,861 - __main__ - INFO - ============================================================\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - STARTING B2-H (HIERARCHICAL ROUTING + HAZARD) MODEL TRAINING\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - ============================================================\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Output directory: atus_analysis/data/models/R15\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Groupby dimensions: employment,day_type\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Dwell bins: 1,2,3,4,6,9,14,20,30\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Existing B1-H model: atus_analysis/data/models/R15/b1h_model.json\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Shrinkage params - Route: tau_block=50.0, tau_group=20.0, add_k=1.0, kappa_slot=50.0\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Shrinkage params - Hazard: tau_block=200.0, tau_group=50.0, k0_global=1.0\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Parsing time blocks and dwell bins...\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Dwell edges: [1, 2, 3, 4, 6, 9, 14, 20, 30]\n",
      "2025-08-30 21:19:35,862 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 21:19:37,292 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 21:19:37,292 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 21:19:37,340 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 21:19:37,340 - __main__ - INFO - Grouping by: ['employment', 'day_type']\n",
      "2025-08-30 21:19:37,340 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 21:19:37,340 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-30 21:40:57,187 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-30 21:40:57,188 - __main__ - INFO - Using split file: atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 21:40:57,681 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 21:41:11,466 - __main__ - INFO - Data split: 29,123,424 train records, 7,280,928 test records\n",
      "2025-08-30 21:41:11,600 - __main__ - INFO - Train respondents: 202,246, Test respondents: 50,562\n",
      "2025-08-30 21:41:13,011 - __main__ - INFO - Loading existing B1-H model from: atus_analysis/data/models/R15/b1h_model.json\n",
      "2025-08-30 21:41:13,055 - __main__ - INFO - B1-H (ensured with per-slot matrices & sidecar) saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/b1h_model.json\n",
      "2025-08-30 21:41:13,055 - __main__ - INFO - Slot matrices sidecar present: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/b1h_slot_mats.npz (size=0.8 MB)\n",
      "2025-08-30 21:41:13,055 - __main__ - INFO - Fitting B2-H hazard model...\n",
      "2025-08-30 21:42:10,519 - __main__ - INFO - B2-H hazard model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/b2h_model.json\n",
      "2025-08-30 21:42:10,519 - __main__ - INFO - Evaluating models on test set...\n",
      "2025-08-30 21:44:51,789 - __main__ - INFO - B1-H test NLL (per weight): 0.4531\n",
      "2025-08-30 21:48:20,445 - __main__ - INFO - B2-H test NLL (per weight): 0.4487\n",
      "2025-08-30 21:48:20,658 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/eval_b2h.json\n",
      "2025-08-30 21:48:20,659 - __main__ - INFO - ============================================================\n",
      "2025-08-30 21:48:20,659 - __main__ - INFO -  B2-H training completed in 1724.80 seconds\n",
      "2025-08-30 21:48:20,659 - __main__ - INFO -  B2-H written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15\n",
      "2025-08-30 21:48:20,659 - __main__ - INFO - ============================================================\n",
      " B2-H run complete → /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15\n",
      "B2-H model completed successfully for R15\n",
      "\n",
      "R15 COMPLETED SUCCESSFULLY in 3203.5 seconds (53.4 minutes)\n",
      "Memory cleanup completed\n",
      "\n",
      " R15 completed successfully! All experiments are now complete.\n",
      "\n",
      "============================================================\n",
      "STARTING RUNG R16\n",
      "============================================================\n",
      " Rung: R16\n",
      " Groupby: employment, hh_size_band\n",
      " Output directory: atus_analysis/data/models/R16\n",
      " Include hazard: True\n",
      "\n",
      "Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.1% used, 746.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1947897.7GB free\n",
      "\n",
      "✓ System resources look good\n",
      "\n",
      "---  Step 1: B1-H Model for R16 ---\n",
      "Using subprocess execution...\n",
      "Running B1-H via subprocess for R16...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R16 --groupby employment, hh_size_band --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 21:48:24,281 - __main__ - INFO - ============================================================\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - ============================================================\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Output directory: atus_analysis/data/models/R16\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Groupby dimensions: employment, hh_size_band\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-30 21:48:24,281 - __main__ - INFO - Per-slot pooling - kappa_slot: 50.0\n",
      "2025-08-30 21:48:24,282 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 21:48:24,282 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 21:48:25,710 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 21:48:25,710 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 21:48:25,758 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 21:48:25,758 - __main__ - INFO - Grouping by: ['employment', 'hh_size_band']\n",
      "2025-08-30 21:48:25,758 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 21:48:25,758 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-30 22:09:26,110 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 16 groups\n",
      "2025-08-30 22:09:26,595 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 22:09:40,366 - __main__ - INFO - Data split: 29,123,424 train records, 7,280,928 test records\n",
      "2025-08-30 22:09:40,503 - __main__ - INFO - Train respondents: 202,246, Test respondents: 50,562\n",
      "2025-08-30 22:09:41,764 - __main__ - INFO - Fitting B1-H hierarchical model (with 144 per-slot matrices)...\n",
      "2025-08-30 22:10:05,531 - __main__ - INFO - ✓ Model fitting completed\n",
      "2025-08-30 22:10:05,615 - __main__ - INFO - Model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/b1h_model.json\n",
      "2025-08-30 22:10:05,615 - __main__ - INFO - Slot matrices sidecar saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/b1h_slot_mats.npz (size=1.6 MB)\n",
      "2025-08-30 22:10:05,615 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-30 22:12:50,975 - __main__ - INFO - B1-H test NLL (per weight): 0.4609\n",
      "2025-08-30 22:12:50,976 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/eval_b1h.json\n",
      "2025-08-30 22:12:50,976 - __main__ - INFO - ============================================================\n",
      "2025-08-30 22:12:50,976 - __main__ - INFO -  B1-H training completed in 1466.70 seconds\n",
      "2025-08-30 22:12:50,976 - __main__ - INFO -  Written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16\n",
      "2025-08-30 22:12:50,976 - __main__ - INFO - ============================================================\n",
      " B1-H run complete → /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16\n",
      "B1-H model completed successfully for R16\n",
      "\n",
      "---  Step 2: B2-H Model for R16 ---\n",
      "Running B2-H (hazard) model for R16...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline2_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R16 --groupby employment, hh_size_band --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --dwell_bins 1,2,3,4,6,9,14,20,30 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --b1h_path atus_analysis/data/models/R16/b1h_model.json --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 22:12:53,458 - __main__ - INFO - ============================================================\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - STARTING B2-H (HIERARCHICAL ROUTING + HAZARD) MODEL TRAINING\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - ============================================================\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Output directory: atus_analysis/data/models/R16\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Groupby dimensions: employment, hh_size_band\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Dwell bins: 1,2,3,4,6,9,14,20,30\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Existing B1-H model: atus_analysis/data/models/R16/b1h_model.json\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Shrinkage params - Route: tau_block=50.0, tau_group=20.0, add_k=1.0, kappa_slot=50.0\n",
      "2025-08-30 22:12:53,458 - __main__ - INFO - Shrinkage params - Hazard: tau_block=200.0, tau_group=50.0, k0_global=1.0\n",
      "2025-08-30 22:12:53,459 - __main__ - INFO - Parsing time blocks and dwell bins...\n",
      "2025-08-30 22:12:53,459 - __main__ - INFO - Time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 22:12:53,459 - __main__ - INFO - Dwell edges: [1, 2, 3, 4, 6, 9, 14, 20, 30]\n",
      "2025-08-30 22:12:53,459 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 22:12:54,881 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 22:12:54,881 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 22:12:54,934 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 22:12:54,934 - __main__ - INFO - Grouping by: ['employment', 'hh_size_band']\n",
      "2025-08-30 22:12:54,934 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 22:12:54,934 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-30 22:33:54,726 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 16 groups\n",
      "2025-08-30 22:33:54,727 - __main__ - INFO - Using split file: atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 22:33:55,261 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 22:34:08,716 - __main__ - INFO - Data split: 29,123,424 train records, 7,280,928 test records\n",
      "2025-08-30 22:34:08,853 - __main__ - INFO - Train respondents: 202,246, Test respondents: 50,562\n",
      "2025-08-30 22:34:10,015 - __main__ - INFO - Loading existing B1-H model from: atus_analysis/data/models/R16/b1h_model.json\n",
      "2025-08-30 22:34:10,096 - __main__ - INFO - B1-H (ensured with per-slot matrices & sidecar) saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/b1h_model.json\n",
      "2025-08-30 22:34:10,096 - __main__ - INFO - Slot matrices sidecar present: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/b1h_slot_mats.npz (size=1.6 MB)\n",
      "2025-08-30 22:34:10,096 - __main__ - INFO - Fitting B2-H hazard model...\n",
      "2025-08-30 22:35:07,556 - __main__ - INFO - B2-H hazard model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/b2h_model.json\n",
      "2025-08-30 22:35:07,557 - __main__ - INFO - Evaluating models on test set...\n",
      "2025-08-30 22:37:48,125 - __main__ - INFO - B1-H test NLL (per weight): 0.4609\n",
      "2025-08-30 22:41:10,788 - __main__ - INFO - B2-H test NLL (per weight): 0.4556\n",
      "2025-08-30 22:41:11,003 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/eval_b2h.json\n",
      "2025-08-30 22:41:11,004 - __main__ - INFO - ============================================================\n",
      "2025-08-30 22:41:11,004 - __main__ - INFO -  B2-H training completed in 1697.55 seconds\n",
      "2025-08-30 22:41:11,004 - __main__ - INFO -  B2-H written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16\n",
      "2025-08-30 22:41:11,004 - __main__ - INFO - ============================================================\n",
      " B2-H run complete → /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16\n",
      "B2-H model completed successfully for R16\n",
      "\n",
      "R16 COMPLETED SUCCESSFULLY in 3170.2 seconds (52.8 minutes)\n",
      "Memory cleanup completed\n",
      "\n",
      " R16 completed successfully! All experiments are now complete.\n",
      "\n",
      "============================================================\n",
      "STARTING RUNG R17\n",
      "============================================================\n",
      " Rung: R17\n",
      " Groupby: employment, quarter\n",
      " Output directory: atus_analysis/data/models/R17\n",
      " Include hazard: True\n",
      "\n",
      "Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.1% used, 746.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1947897.7GB free\n",
      "\n",
      "✓ System resources look good\n",
      "\n",
      "---  Step 1: B1-H Model for R17 ---\n",
      "Using subprocess execution...\n",
      "Running B1-H via subprocess for R17...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R17 --groupby employment, quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 22:41:14,472 - __main__ - INFO - ============================================================\n",
      "2025-08-30 22:41:14,472 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-30 22:41:14,472 - __main__ - INFO - ============================================================\n",
      "2025-08-30 22:41:14,472 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 22:41:14,472 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 22:41:14,472 - __main__ - INFO - Output directory: atus_analysis/data/models/R17\n",
      "2025-08-30 22:41:14,472 - __main__ - INFO - Groupby dimensions: employment, quarter\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Per-slot pooling - kappa_slot: 50.0\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 22:41:14,473 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 22:41:15,854 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 22:41:15,854 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 22:41:15,905 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 22:41:15,905 - __main__ - INFO - Grouping by: ['employment', 'quarter']\n",
      "2025-08-30 22:41:15,905 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 22:41:15,905 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-30 23:02:10,425 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 16 groups\n",
      "2025-08-30 23:02:10,894 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-30 23:02:24,221 - __main__ - INFO - Data split: 29,123,424 train records, 7,280,928 test records\n",
      "2025-08-30 23:02:24,356 - __main__ - INFO - Train respondents: 202,246, Test respondents: 50,562\n",
      "2025-08-30 23:02:25,532 - __main__ - INFO - Fitting B1-H hierarchical model (with 144 per-slot matrices)...\n",
      "2025-08-30 23:02:49,649 - __main__ - INFO - ✓ Model fitting completed\n",
      "2025-08-30 23:02:49,732 - __main__ - INFO - Model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17/b1h_model.json\n",
      "2025-08-30 23:02:49,732 - __main__ - INFO - Slot matrices sidecar saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17/b1h_slot_mats.npz (size=1.6 MB)\n",
      "2025-08-30 23:02:49,732 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-30 23:05:31,892 - __main__ - INFO - B1-H test NLL (per weight): 0.4636\n",
      "2025-08-30 23:05:31,894 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17/eval_b1h.json\n",
      "2025-08-30 23:05:31,894 - __main__ - INFO - ============================================================\n",
      "2025-08-30 23:05:31,894 - __main__ - INFO -  B1-H training completed in 1457.42 seconds\n",
      "2025-08-30 23:05:31,894 - __main__ - INFO -  Written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17\n",
      "2025-08-30 23:05:31,894 - __main__ - INFO - ============================================================\n",
      " B1-H run complete → /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17\n",
      "B1-H model completed successfully for R17\n",
      "\n",
      "---  Step 2: B2-H Model for R17 ---\n",
      "Running B2-H (hazard) model for R17...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline2_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R17 --groupby employment, quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --dwell_bins 1,2,3,4,6,9,14,20,30 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --b1h_path atus_analysis/data/models/R17/b1h_model.json --kappa_slot 50\n",
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)2025-08-30 23:05:34,358 - __main__ - INFO - ============================================================\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - STARTING B2-H (HIERARCHICAL ROUTING + HAZARD) MODEL TRAINING\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - ============================================================\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Output directory: atus_analysis/data/models/R17\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Groupby dimensions: employment, quarter\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Dwell bins: 1,2,3,4,6,9,14,20,30\n",
      "2025-08-30 23:05:34,358 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Existing B1-H model: atus_analysis/data/models/R17/b1h_model.json\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Shrinkage params - Route: tau_block=50.0, tau_group=20.0, add_k=1.0, kappa_slot=50.0\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Shrinkage params - Hazard: tau_block=200.0, tau_group=50.0, k0_global=1.0\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Parsing time blocks and dwell bins...\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Dwell edges: [1, 2, 3, 4, 6, 9, 14, 20, 30]\n",
      "2025-08-30 23:05:34,359 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-30 23:05:35,776 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-30 23:05:35,776 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-30 23:05:35,826 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-30 23:05:35,827 - __main__ - INFO - Grouping by: ['employment', 'quarter']\n",
      "2025-08-30 23:05:35,827 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-30 23:05:35,827 - __main__ - INFO - Preparing data with groups and time blocks...\n"
     ]
    }
   ],
   "source": [
    "exps = [\"R14\", \"R15\", \"R16\", \"R17\", \"R18\",\"R19\", \"R20\", \"R21\", \"R22\", \"R23\"]\n",
    "\n",
    "for i in exps:\n",
    "    success = run_single_rung(i, include_hazard=True)\n",
    "\n",
    "    if success:\n",
    "        print(f\"\\n {i} completed successfully! All experiments are now complete.\")\n",
    "    else:\n",
    "        print(f\"\\n {i} failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f982e5a-4bc2-4b77-8c56-8c72fa16308e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad127947-f5c8-4b47-81e0-3f1efd1a34c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eefbb-6758-4497-afe1-207e1409f562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e3458-f58f-4911-bfd0-ae161870f083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79d5ab40-c366-49e6-a12c-f22f99438d3a",
   "metadata": {},
   "source": [
    "## Cell 12: Final Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8817df5-d3d8-47c3-a431-4ff02a0c54bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "progress = load_progress()\n",
    "completed = set(progress.get('completed_rungs', []))\n",
    "failed = set(progress.get('failed_rungs', []))\n",
    "all_rungs = list(RUNG_SPECS.keys())\n",
    "\n",
    "print(f\"\\nTotal rungs: {len(all_rungs)}\")\n",
    "print(f\"Completed: {len(completed)} - {sorted(completed) if completed else 'None'}\")\n",
    "print(f\"Failed: {len(failed)} - {sorted(failed) if failed else 'None'}\")\n",
    "not_attempted = [r for r in all_rungs if r not in completed and r not in failed]\n",
    "print(f\"Not attempted: {not_attempted if not_attempted else 'None'}\")\n",
    "\n",
    "# Helper: extract nll_per_weight robustly from either CLI-style or notebook-style evals\n",
    "def _extract_nll(d: dict):\n",
    "    # Notebook/direct save: {\"nll_per_weight\": ..., ...}\n",
    "    if \"nll_per_weight\" in d:\n",
    "        return d[\"nll_per_weight\"]\n",
    "    # CLI B1 eval wrapper: {\"b1h\": {...}}\n",
    "    if \"b1h\" in d and isinstance(d[\"b1h\"], dict):\n",
    "        x = d[\"b1h\"]\n",
    "        if \"nll_per_weight\" in x:\n",
    "            return x[\"nll_per_weight\"]\n",
    "        if \"nll_weighted\" in x and \"weight_total\" in x and x[\"weight_total\"]:\n",
    "            return x[\"nll_weighted\"] / x[\"weight_total\"]\n",
    "    # Fallbacks\n",
    "    if \"nll_weighted\" in d and \"weight_total\" in d and d[\"weight_total\"]:\n",
    "        return d[\"nll_weighted\"] / d[\"weight_total\"]\n",
    "    return None\n",
    "\n",
    "def _read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "rows = []\n",
    "for rung in all_rungs:\n",
    "    rung_dir = OUTPUT_DIR / rung\n",
    "    status = \"completed\" if rung in completed else (\"failed\" if rung in failed else \"pending\")\n",
    "    dur_s = progress.get(f\"{rung}_duration_seconds\")\n",
    "    dur_min = (dur_s / 60.0) if isinstance(dur_s, (int, float)) else None\n",
    "\n",
    "    # Files\n",
    "    b1_json = rung_dir / \"b1h_model.json\"\n",
    "    b1_npz  = rung_dir / \"b1h_slot_mats.npz\"\n",
    "    b2_json = rung_dir / \"b2h_model.json\"\n",
    "    eval_b1 = rung_dir / \"eval_b1h.json\"\n",
    "    eval_b2 = rung_dir / \"eval_b2h.json\"\n",
    "\n",
    "    # Metrics\n",
    "    b1_nll = None\n",
    "    b2_nll = None\n",
    "    ej1 = _read_json(eval_b1)\n",
    "    ej2 = _read_json(eval_b2)\n",
    "    if ej1 is not None:\n",
    "        b1_nll = _extract_nll(ej1)\n",
    "        # If CLI-style eval_b1h.json, it wrapped under \"b1h\"\n",
    "        if b1_nll is None and \"b1h\" in ej1:\n",
    "            b1_nll = _extract_nll({\"b1h\": ej1[\"b1h\"]})\n",
    "    if ej2 is not None:\n",
    "        # CLI baseline2 writes {\"b1h\": {...}, \"b2h\": {...}, ...}\n",
    "        if \"b2h\" in ej2:\n",
    "            b2_nll = _extract_nll({\"b1h\": ej2[\"b2h\"]})\n",
    "        else:\n",
    "            # If someone saved just the dict (unlikely), still try\n",
    "            b2_nll = _extract_nll(ej2)\n",
    "\n",
    "    rows.append({\n",
    "        \"rung\": rung,\n",
    "        \"status\": status,\n",
    "        \"duration_min\": round(dur_min, 1) if dur_min is not None else None,\n",
    "        \"b1h_model\": b1_json.exists(),\n",
    "        \"b1h_slot_npz\": b1_npz.exists(),\n",
    "        \"b2h_model\": b2_json.exists(),\n",
    "        \"eval_b1h\": eval_b1.exists(),\n",
    "        \"eval_b2h\": eval_b2.exists(),\n",
    "        \"b1_nll_per_weight\": (None if b1_nll is None else float(b1_nll)),\n",
    "        \"b2_nll_per_weight\": (None if b2_nll is None else float(b2_nll)),\n",
    "        \"out_dir\": str(rung_dir),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).set_index(\"rung\")\n",
    "# Order by rung name naturally (R1..R7) if present\n",
    "df = df.loc[[r for r in all_rungs if r in df.index]]\n",
    "\n",
    "# Pretty print summary table\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", 120):\n",
    "    print(\"\\n=== Per-rung Summary ===\")\n",
    "    print(df)\n",
    "\n",
    "# Timing totals\n",
    "print(\"\\n=== Timing Information ===\")\n",
    "total_time = sum(progress.get(f\"{r}_duration_seconds\", 0.0) for r in completed)\n",
    "for r in completed:\n",
    "    dur = progress.get(f\"{r}_duration_seconds\")\n",
    "    if isinstance(dur, (int, float)):\n",
    "        print(f\"{r}: {dur:.0f} seconds ({dur/60:.1f} minutes)\")\n",
    "if total_time > 0:\n",
    "    print(f\"\\nTotal runtime: {total_time:.0f} seconds ({total_time/60:.1f} minutes, {total_time/3600:.1f} hours)\")\n",
    "\n",
    "# Output file counts (JSONs only)\n",
    "print(\"\\n=== Output Files (JSON count) ===\")\n",
    "for r in completed:\n",
    "    rd = OUTPUT_DIR / r\n",
    "    if rd.exists():\n",
    "        files = list(rd.glob(\"*.json\"))\n",
    "        print(f\"{r}: {len(files)} JSON files in {rd}\")\n",
    "\n",
    "# Success rate (use actual number of rungs configured)\n",
    "if len(all_rungs) > 0:\n",
    "    success_rate = len(completed) / len(all_rungs) * 100\n",
    "    print(f\"\\nOverall success rate: {success_rate:.1f}%\")\n",
    "\n",
    "# Note about R6 vs R7 when hazard=True for all\n",
    "if \"R6\" in RUNG_SPECS and \"R7\" in RUNG_SPECS:\n",
    "    if str(RUNG_SPECS[\"R6\"]).strip() == str(RUNG_SPECS[\"R7\"]).strip():\n",
    "        print(\"\\nNOTE:\")\n",
    "        print(\" R6 and R7 share the same grouping spec.\")\n",
    "        print(\" Since you ran hazard=True for all rungs, R6 and R7 are effectively identical runs.\")\n",
    "        print(\" You may keep only one of them for reporting.\")\n",
    "\n",
    "# Final line\n",
    "if len(completed) == len(all_rungs):\n",
    "    print(\"\\nALL EXPERIMENTS COMPLETED SUCCESSFULLY!\")\n",
    "elif len(failed) > 0:\n",
    "    print(\"\\nSome experiments failed. Re-run the failed ones to retry.\")\n",
    "else:\n",
    "    print(f\"\\n{len(all_rungs) - len(completed)} experiments remaining.\")\n",
    "\n",
    "print(f\"\\nProgress file saved as: {PROGRESS_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472f6a4-c49a-4752-babb-36e8150f4a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # === R0 (no covariates) — build B1-H, build B2-H, dump metrics for both ===\n",
    "# from pathlib import Path\n",
    "# import os, sys, time, subprocess\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Optional thread limiter for BLAS/OpenMP\n",
    "# try:\n",
    "#     from threadpoolctl import threadpool_limits\n",
    "# except Exception:\n",
    "#     threadpool_limits = None\n",
    "# from contextlib import nullcontext\n",
    "\n",
    "# # ---------- Paths from your existing globals ----------\n",
    "# REPO_ROOT  = Path(os.getcwd()).resolve()\n",
    "# MODELS_DIR = (REPO_ROOT / OUTPUT_DIR).resolve()\n",
    "# SEQS       = (REPO_ROOT / SEQUENCES_FILE).resolve()\n",
    "# SUBS       = (REPO_ROOT / SUBGROUPS_FILE).resolve()\n",
    "# SPLIT_PATH = (MODELS_DIR / \"fixed_split.parquet\").resolve()\n",
    "# R0_DIR     = (MODELS_DIR / \"R0\").resolve()\n",
    "# R0_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# t0 = time.time()\n",
    "# print(\"→ Building fresh R0 model in:\", R0_DIR)\n",
    "\n",
    "# # ---------- 1) Prepare pooled data (single group __ALL__) ----------\n",
    "# sequences = pd.read_parquet(SEQS)\n",
    "# subgroups = pd.read_parquet(SUBS).copy()\n",
    "# subgroups[\"__ALL__\"] = \"ALL\"           # pooled\n",
    "# groupby_cols = [\"__ALL__\"]\n",
    "\n",
    "# blocks = parse_time_blocks(TIME_BLOCKS)\n",
    "# # (threshold=0.0 is effectively a no-op but keeps consistent codepath)\n",
    "# subgroups = pool_rare_quarter(subgroups, groupby_cols, \"TUFNWGTP\", threshold=0.0)\n",
    "# long_df   = prepare_long_with_groups(sequences, subgroups, groupby_cols, \"TUFNWGTP\", blocks)\n",
    "\n",
    "# # ---------- 2) Shared split (use existing if present) ----------\n",
    "# if SPLIT_PATH.exists():\n",
    "#     split_df = pd.read_parquet(SPLIT_PATH)[[\"TUCASEID\",\"set\"]]\n",
    "#     print(\"✓ Using existing split:\", SPLIT_PATH)\n",
    "# else:\n",
    "#     rng  = np.random.RandomState(SEED)\n",
    "#     meta = long_df[[\"TUCASEID\"]].drop_duplicates().copy()\n",
    "#     meta[\"rand\"] = rng.rand(len(meta))\n",
    "#     test_n = int(round(TEST_SIZE * len(meta)))\n",
    "#     meta[\"set\"] = \"train\"\n",
    "#     if test_n > 0:\n",
    "#         take = meta.sort_values(\"rand\").head(test_n).index\n",
    "#         meta.loc[take, \"set\"] = \"test\"\n",
    "#     split_df = meta[[\"TUCASEID\",\"set\"]]\n",
    "#     SPLIT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "#     split_df.to_parquet(SPLIT_PATH, index=False)\n",
    "#     print(f\"✓ Created split: {SPLIT_PATH}  (test={test_n}, train={len(meta)-test_n})\")\n",
    "\n",
    "# df       = long_df.merge(split_df, on=\"TUCASEID\", how=\"left\")\n",
    "# train_df = df[df[\"set\"] == \"train\"].copy()\n",
    "# test_df  = df[df[\"set\"] == \"test\"].copy()\n",
    "# K        = int(df[\"state_id\"].max()) + 1\n",
    "\n",
    "# # ---------- 3) Train B1-H (routing) with 144 slot matrices ----------\n",
    "# num_threads = int(NUM_THREADS)\n",
    "# ctx = threadpool_limits(limits=num_threads) if threadpool_limits else nullcontext()\n",
    "# with ctx:\n",
    "#     b1 = fit_b1_hier(\n",
    "#         train_df, K, \"TUFNWGTP\",\n",
    "#         tau_block=50.0, tau_group=20.0, add_k=1.0,\n",
    "#         compute_slot_mats=True,\n",
    "#         kappa_slot=float(KAPPA_SLOT),\n",
    "#         time_blocks_spec=TIME_BLOCKS\n",
    "#     )\n",
    "\n",
    "# # Save JSON + slot sidecar with expected keys 'g0','g1',...\n",
    "# out_json_b1 = R0_DIR / \"b1h_model.json\"\n",
    "# sidecar     = R0_DIR / \"b1h_slot_mats.npz\"\n",
    "# to_save_b1  = dict(b1)\n",
    "\n",
    "# if \"slot_matrices\" in to_save_b1:\n",
    "#     slot_dict    = to_save_b1.pop(\"slot_matrices\")   # {group_key: [144,K,K]}\n",
    "#     groups_order = list(slot_dict.keys())            # for R0 → [\"ALL\"]\n",
    "#     arrays = {f\"g{gi}\": np.array(slot_dict[gk], dtype=np.float32)\n",
    "#               for gi, gk in enumerate(groups_order)}\n",
    "#     np.savez_compressed(sidecar, **arrays)\n",
    "#     meta = to_save_b1.setdefault(\"meta\", {})\n",
    "#     meta[\"groups_order\"]  = groups_order\n",
    "#     meta[\"slot_sidecar\"]  = str(sidecar)\n",
    "#     to_save_b1[\"slot_matrices_npz\"] = str(sidecar)\n",
    "\n",
    "# # Compatibility: some tools look for 'b1h_slot_matz.npz'\n",
    "# matz = R0_DIR / \"b1h_slot_matz.npz\"\n",
    "# try:\n",
    "#     if sidecar.exists() and not matz.exists():\n",
    "#         import shutil; shutil.copy2(sidecar, matz)\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# save_json(to_save_b1, out_json_b1)\n",
    "# print(\"✓ Saved:\", out_json_b1)\n",
    "# if sidecar.exists():\n",
    "#     print(f\"  Slot sidecar: {sidecar} ({sidecar.stat().st_size/1024/1024:.1f} MB)\")\n",
    "\n",
    "# # Quick eval summary for B1-H\n",
    "# eval_b1 = nll_b1(test_df, to_save_b1, K, \"TUFNWGTP\")\n",
    "# save_json(eval_b1, R0_DIR / \"eval_b1h.json\")\n",
    "# print(\"✓ Saved:\", R0_DIR / \"eval_b1h.json\")\n",
    "\n",
    "# # ---------- 4) Train B2-H (hazard) pooled over __ALL__ ----------\n",
    "# # baseline2_hier reads subgroups from disk; give it a temp parquet with __ALL__\n",
    "# SUBS_R0 = R0_DIR / \"_subgroups_all.parquet\"\n",
    "# subgroups.to_parquet(SUBS_R0, index=False)\n",
    "\n",
    "# env = os.environ.copy()\n",
    "# # normalize numexpr thread limits to avoid \"MAX_THREADS\" error\n",
    "# max_now = int(env.get(\"NUMEXPR_MAX_THREADS\", \"64\"))\n",
    "# if num_threads > max_now:\n",
    "#     env[\"NUMEXPR_MAX_THREADS\"] = str(num_threads)\n",
    "# env[\"NUMEXPR_NUM_THREADS\"]     = str(min(num_threads, int(env.get(\"NUMEXPR_MAX_THREADS\", str(num_threads)))))\n",
    "# env[\"OMP_NUM_THREADS\"]         = str(num_threads)\n",
    "# env[\"OPENBLAS_NUM_THREADS\"]    = str(num_threads)\n",
    "# env[\"MKL_NUM_THREADS\"]         = str(num_threads)\n",
    "# env[\"VECLIB_MAXIMUM_THREADS\"]  = str(num_threads)\n",
    "# env[\"OMP_WAIT_POLICY\"]         = \"PASSIVE\"\n",
    "\n",
    "# cmd_b2 = [\n",
    "#     sys.executable, \"-u\", \"-m\", \"atus_analysis.scripts.baseline2_hier\",\n",
    "#     \"--sequences\", str(SEQS),\n",
    "#     \"--subgroups\", str(SUBS_R0),\n",
    "#     \"--out_dir\",   str(R0_DIR),\n",
    "#     \"--groupby\",   \"__ALL__\",\n",
    "#     \"--time_blocks\", TIME_BLOCKS,\n",
    "#     \"--dwell_bins\",  DWELL_BINS,\n",
    "#     \"--seed\",        str(SEED),\n",
    "#     \"--test_size\",   str(TEST_SIZE),\n",
    "#     \"--split_path\",  str(SPLIT_PATH),\n",
    "#     \"--b1h_path\",    str(out_json_b1),\n",
    "#     \"--kappa_slot\",  str(KAPPA_SLOT),\n",
    "# ]\n",
    "# print(\"\\n>>>\", \" \".join(cmd_b2))\n",
    "# res_b2 = subprocess.run(cmd_b2, cwd=str(REPO_ROOT), text=True, capture_output=True, env=env)\n",
    "# print(res_b2.stdout)\n",
    "# if res_b2.returncode != 0:\n",
    "#     print(\"STDERR (B2-H):\\n\", res_b2.stderr)\n",
    "#     raise RuntimeError(\"B2-H training failed (see stderr above).\")\n",
    "# else:\n",
    "#     print(\"✓ R0 B2-H completed\")\n",
    "\n",
    "# # ---------- 5) Dump per-respondent metrics for BOTH variants ----------\n",
    "# def dump_metrics(model_type: str) -> None:\n",
    "#     cmd = [\n",
    "#         sys.executable, \"-u\", \"-m\", \"atus_analysis.scripts.dump_case_metrics\",\n",
    "#         \"--model_type\", model_type,\n",
    "#         \"--run_dir\", str(R0_DIR),\n",
    "#         \"--sequences\", str(SEQS),\n",
    "#         \"--subgroups\", str(SUBS),\n",
    "#         \"--time_blocks\", TIME_BLOCKS,\n",
    "#         \"--groupby\", \"\",              # no covariates at scoring time\n",
    "#         \"--dwell_bins\", DWELL_BINS,\n",
    "#     ]\n",
    "#     print(\"\\n>>>\", \" \".join(cmd))\n",
    "#     r = subprocess.run(cmd, cwd=str(REPO_ROOT), text=True, capture_output=True)\n",
    "#     print(r.stdout)\n",
    "#     if r.returncode != 0:\n",
    "#         print(\"STDERR:\\n\", r.stderr)\n",
    "#         raise RuntimeError(f\"dump_case_metrics failed for {model_type}\")\n",
    "\n",
    "# dump_metrics(\"b1h\")\n",
    "# dump_metrics(\"b2h\")\n",
    "\n",
    "# # ---------- 6) Sanity check outputs ----------\n",
    "# targets = [\n",
    "#     \"b1h_model.json\", \"b2h_model.json\", \"eval_b1h.json\", \"eval_b2h.json\",\n",
    "#     \"b1h_slot_mats.npz\", \"b1h_slot_matz.npz\",\n",
    "#     \"test_case_metrics_b1h.parquet\", \"test_case_metrics_b2h.parquet\", \"test_case_metrics.parquet\"\n",
    "# ]\n",
    "# print(\"\\nOutputs present:\")\n",
    "# for name in targets:\n",
    "#     p = R0_DIR / name\n",
    "#     print(f\"  {name:<30} exists={p.exists()}\")\n",
    "\n",
    "# # Quick peek at metrics pairwise deltas\n",
    "# try:\n",
    "#     b1p = R0_DIR / \"test_case_metrics_b1h.parquet\"\n",
    "#     b2p = R0_DIR / \"test_case_metrics_b2h.parquet\"\n",
    "#     if b1p.exists() and b2p.exists():\n",
    "#         A = pd.read_parquet(b1p)[[\"TUCASEID\",\"nll_weighted\",\"top1_correct_weight\",\"weight_total\"]].rename(\n",
    "#             columns=lambda c: c+\"_A\" if c!=\"TUCASEID\" else c)\n",
    "#         B = pd.read_parquet(b2p)[[\"TUCASEID\",\"nll_weighted\",\"top1_correct_weight\",\"weight_total\"]].rename(\n",
    "#             columns=lambda c: c+\"_B\" if c!=\"TUCASEID\" else c)\n",
    "#         M = A.merge(B, on=\"TUCASEID\", how=\"inner\")\n",
    "#         d_nll = (M[\"nll_weighted_A\"]/M[\"weight_total_A\"]) - (M[\"nll_weighted_B\"]/M[\"weight_total_B\"])\n",
    "#         d_top = (M[\"top1_correct_weight_A\"]/M[\"weight_total_A\"]) - (M[\"top1_correct_weight_B\"]/M[\"weight_total_B\"])\n",
    "#         print(\"\\nΔ(B1−B2) mean nll:\", float(d_nll.mean()), \" std:\", float(d_nll.std()))\n",
    "#         print(\"Δ(B1−B2) mean top1:\", float(d_top.mean()), \" std:\", float(d_top.std()))\n",
    "# except Exception as e:\n",
    "#     print(\"Note: could not preview metrics:\", e)\n",
    "\n",
    "# print(f\"\\nR0 build & dump complete in {time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35ddd2-480b-430e-ae8e-f598a4a07686",
   "metadata": {},
   "source": [
    "## Add R0 with no covariates for baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e8c4ebd-a80e-4134-9964-32a47ed327a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNG_SPECS8_14_23 = {\"R14\": \"employment,sex\", \n",
    "    \"R15\": \"employment,day_type\",\n",
    "    \"R16\": \"employment, hh_size_band\",\n",
    "    \"R17\": \"employment, quarter\",\n",
    "    \n",
    "    \"R18\": \"sex, day_type\",\n",
    "    \"R19\": \"sex, hh_size_band\",\n",
    "    \"R20\": \"sex, quarter\",\n",
    "    \n",
    "    \"R21\": \"day_type, hh_size_band\",\n",
    "    \"R22\": \"day_type, quarter\",\n",
    "    \"R23\": \"quarter, hh_size_band\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e62e7a-831f-40dc-91d5-49f0f19cb143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b1h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment,sex --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R14:b1h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/test_case_metrics_b1h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/test_case_metrics.parquet present\n",
      "[R14:b1h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b2h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment,sex --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R14:b2h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/test_case_metrics_b2h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R14/test_case_metrics.parquet present\n",
      "[R14:b2h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b1h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment,day_type --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R15:b1h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/test_case_metrics_b1h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/test_case_metrics.parquet present\n",
      "[R15:b1h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b2h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment,day_type --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R15:b2h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/test_case_metrics_b2h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R15/test_case_metrics.parquet present\n",
      "[R15:b2h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b1h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment, hh_size_band --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R16:b1h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/test_case_metrics_b1h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/test_case_metrics.parquet present\n",
      "[R16:b1h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b2h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment, hh_size_band --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R16:b2h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/test_case_metrics_b2h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R16/test_case_metrics.parquet present\n",
      "[R16:b2h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b1h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment, quarter --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R17:b1h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17/test_case_metrics_b1h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17/test_case_metrics.parquet present\n",
      "[R17:b1h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b2h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby employment, quarter --dwell_bins 1,2,3,4,6,9,14,20,30\n",
      "✓ [R17:b2h] wrote /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17/test_case_metrics_b2h.parquet\n",
      "  (compat) /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R17/test_case_metrics.parquet present\n",
      "[R17:b2h] return code: 0\n",
      "\n",
      ">>> /sw/eb/sw/Anaconda3/2023.09-0/bin/python -u -m atus_analysis.scripts.dump_case_metrics --model_type b1h --run_dir /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R18 --sequences /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/sequences/markov_sequences.parquet --subgroups /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/processed/subgroups.parquet --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --groupby sex, day_type --dwell_bins 1,2,3,4,6,9,14,20,30\n"
     ]
    }
   ],
   "source": [
    "# Run BOTH variants (b1h and b2h) for every rung, streaming output\n",
    "import subprocess\n",
    "\n",
    "\n",
    "REPO_ROOT = Path(os.getcwd()).resolve()                 # already chdir'ed to repo root\n",
    "MODELS_DIR = (REPO_ROOT / OUTPUT_DIR).resolve()\n",
    "SEQS       = (REPO_ROOT / SEQUENCES_FILE).resolve()\n",
    "SUBS       = (REPO_ROOT / SUBGROUPS_FILE).resolve()\n",
    "\n",
    "assert MODELS_DIR.exists(), f\"Models dir not found: {MODELS_DIR}\"\n",
    "assert SEQS.exists(),       f\"Sequences parquet not found: {SEQS}\"\n",
    "assert SUBS.exists(),       f\"Subgroups parquet not found: {SUBS}\"\n",
    "\n",
    "for rung, groupby in RUNG_SPECS8_14_23.items():\n",
    "    run_dir = MODELS_DIR / rung\n",
    "    if not run_dir.exists():\n",
    "        print(f\"[SKIP {rung}] missing directory: {run_dir}\")\n",
    "        continue\n",
    "\n",
    "    for model_type in (\"b1h\", \"b2h\"):\n",
    "        cmd = [\n",
    "            sys.executable, \"-u\", \"-m\", \"atus_analysis.scripts.dump_case_metrics\",\n",
    "            \"--model_type\", model_type,          #  explicitly run this variant\n",
    "            \"--run_dir\", str(run_dir),\n",
    "            \"--sequences\", str(SEQS),\n",
    "            \"--subgroups\", str(SUBS),\n",
    "            \"--time_blocks\", TIME_BLOCKS,\n",
    "            \"--groupby\", groupby,               # rung-specific groupby from RUNG_SPECS\n",
    "            \"--dwell_bins\", DWELL_BINS,\n",
    "        ]\n",
    "        print(\"\\n>>>\", \" \".join(cmd))\n",
    "        proc = subprocess.Popen(\n",
    "            cmd,\n",
    "            cwd=str(REPO_ROOT),                 # ensure package imports resolve\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        for line in proc.stdout:                # live progress\n",
    "            print(line, end=\"\")\n",
    "        rc = proc.wait()\n",
    "        print(f\"[{rung}:{model_type}] return code:\", rc)\n",
    "\n",
    "# List outputs we produced\n",
    "print(\"\\nDiscovered metrics files:\")\n",
    "for rung in RUNG_SPECS:\n",
    "    rdir = MODELS_DIR / rung\n",
    "    if rdir.exists():\n",
    "        hits = sorted(str(p) for p in rdir.glob(\"test_case_metrics*.parquet\"))\n",
    "        print(f\"  {rung}: {hits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2e8c4-246e-489b-81ae-7239c66209f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
