{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c39e102-9f11-4295-a1fc-85ed4a92d4d8",
   "metadata": {},
   "source": [
    "# ATUS Hierarchical Baseline Experiments - HPC Version\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook runs the ATUS (American Time Use Survey) hierarchical baseline experiments safely on HPC systems. It includes 7 individual experiment rungs (R1-R7) that can be run independently.\n",
    "\n",
    "## Experiment Structure\n",
    "\n",
    "- **R1**: Region only\n",
    "- **R2**: Region + Sex\n",
    "- **R3**: Region + Employment\n",
    "- **R4**: Region + Day Type\n",
    "- **R5**: Region + Household Size Band\n",
    "- **R6**: Full routing model (Employment + Day Type + HH Size + Sex + Region + Quarter)\n",
    "- **R7**: Full model with hazard (same grouping as R6 but includes hazard modeling)\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run Setup Cells**: Execute cells 1-3 to import libraries and set up the environment\n",
    "2. **Check System Resources**: Run cell 4 to verify your HPC node has sufficient resources\n",
    "3. **Run Individual Experiments**: Execute cells 5-11 one at a time for each rung (R1-R7)\n",
    "4. **Monitor Progress**: Each cell will show detailed progress and can be interrupted safely\n",
    "5. **Resume if Needed**: If interrupted, you can restart from any cell - completed experiments won't be re-run\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **R1-R4**: 30-60 minutes each\n",
    "- **R5-R6**: 60-120 minutes each  \n",
    "- **R7**: 120-180 minutes (includes hazard model)\n",
    "- **Total**: 6-12 hours for all experiments\n",
    "\n",
    "## Resource Requirements\n",
    "\n",
    "- **Memory**: At least 16GB RAM recommended\n",
    "- **Storage**: At least 20GB free disk space\n",
    "- **CPU**: Multi-core recommended for faster processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d0268-3ef4-4574-9ef4-0f6500a2acc7",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4261231a-9036-492e-a95e-d05b7ee91ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n",
      "Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\n",
      "Working directory: /ztank/scratch/user/u.rd143338/atus_analysis-main\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "# Set working directory\n",
    "os.chdir('/ztank/scratch/user/u.rd143338/atus_analysis-main')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3e35c-40cf-455e-9e58-ca42f7e25934",
   "metadata": {},
   "source": [
    "## Define Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e99f1e2-2f0f-4f6f-a6be-882dac63c843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration set\n",
      "Output directory: atus_analysis/data/models\n",
      "Number of rungs: 7\n"
     ]
    }
   ],
   "source": [
    "# Experiment configuration\n",
    "RUNG_SPECS = {\n",
    "    \"R1\": \"region\",\n",
    "    \"R2\": \"region,sex\", \n",
    "    \"R3\": \"region,employment\",\n",
    "    \"R4\": \"region,day_type\",\n",
    "    \"R5\": \"region,hh_size_band\",\n",
    "    \"R6\": \"employment,day_type,hh_size_band,sex,region,quarter\",\n",
    "    \"R7\": \"employment,day_type,hh_size_band,sex,region,quarter\"  # + hazard\n",
    "}\n",
    "\n",
    "# File paths (adjust if needed)\n",
    "BASE_DIR = Path(\".\")\n",
    "SEQUENCES_FILE = \"atus_analysis/data/sequences/markov_sequences.parquet\"\n",
    "SUBGROUPS_FILE = \"atus_analysis/data/processed/subgroups.parquet\"\n",
    "OUTPUT_DIR = Path(\"atus_analysis/data/models\")\n",
    "PROGRESS_FILE = \"experiment_progress_jupyter.json\"\n",
    "\n",
    "# Experiment settings\n",
    "SEED = 2025\n",
    "TEST_SIZE = 0.2\n",
    "TIME_BLOCKS = \"night:0-5,morning:6-11,afternoon:12-17,evening:18-23\"\n",
    "DWELL_BINS = \"1,2,3,4,6,9,14,20,30\"\n",
    "\n",
    "print(\"‚úì Configuration set\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Number of rungs: {len(RUNG_SPECS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6ffe7-45a3-42db-886c-393f0702e40a",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0829560a-7fc9-450d-a38d-848cded9b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined with improved direct execution support\n"
     ]
    }
   ],
   "source": [
    "def check_system_resources():\n",
    "    \"\"\"Check current system resources.\"\"\"\n",
    "    memory = psutil.virtual_memory()\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    \n",
    "    print(\"=== System Resources ===\")\n",
    "    print(f\"Memory: {memory.percent:.1f}% used, {memory.available / (1024**3):.1f}GB available\")\n",
    "    print(f\"CPU: {cpu_percent:.1f}% usage\")\n",
    "    \n",
    "    try:\n",
    "        disk = psutil.disk_usage('.')\n",
    "        print(f\"Disk: {disk.free / (1024**3):.1f}GB free\")\n",
    "    except:\n",
    "        print(\"Disk: Could not check disk usage\")\n",
    "    \n",
    "    # Check if resources are adequate\n",
    "    warnings = []\n",
    "    if memory.available < 4 * (1024**3):  # Less than 4GB\n",
    "        warnings.append(f\"Low memory: only {memory.available / (1024**3):.1f}GB available\")\n",
    "    if cpu_percent > 80:\n",
    "        warnings.append(f\"High CPU usage: {cpu_percent:.1f}%\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNINGS:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"   - {warning}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì System resources look good\")\n",
    "    \n",
    "    return len(warnings) == 0\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load experiment progress from file.\"\"\"\n",
    "    if Path(PROGRESS_FILE).exists():\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'completed_rungs': [], 'failed_rungs': [], 'session_start': datetime.now().isoformat()}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save experiment progress to file.\"\"\"\n",
    "    progress['last_updated'] = datetime.now().isoformat()\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "def is_rung_completed(rung, progress):\n",
    "    \"\"\"Check if a rung has been completed successfully.\"\"\"\n",
    "    return rung in progress.get('completed_rungs', [])\n",
    "\n",
    "def run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier directly in Jupyter (preferred method).\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from atus_analysis.scripts.common_hier import (\n",
    "            prepare_long_with_groups, pool_rare_quarter,\n",
    "            save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Loading data for {rung}...\")\n",
    "        \n",
    "        # Load sequences and subgroups\n",
    "        sequences = pd.read_parquet(SEQUENCES_FILE)\n",
    "        subgroups = pd.read_parquet(SUBGROUPS_FILE)\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(sequences)} sequences and {len(subgroups)} subgroups\")\n",
    "        \n",
    "        # Parse time blocks\n",
    "        time_blocks = parse_time_blocks(TIME_BLOCKS)\n",
    "        print(f\"‚úì Parsed time blocks: {time_blocks}\")\n",
    "        \n",
    "        # Create or load split\n",
    "        if split_path.exists():\n",
    "            print(f\"üìÇ Loading existing split from {split_path}\")\n",
    "            split_df = pd.read_parquet(split_path)\n",
    "        else:\n",
    "            print(f\"üé≤ Creating new split with seed {SEED}\")\n",
    "            # Create split logic here (simplified)\n",
    "            np.random.seed(SEED)\n",
    "            unique_ids = subgroups['TUCASEID'].unique()\n",
    "            test_size = int(len(unique_ids) * TEST_SIZE)\n",
    "            test_ids = np.random.choice(unique_ids, test_size, replace=False)\n",
    "            \n",
    "            split_df = pd.DataFrame({\n",
    "                'TUCASEID': subgroups['TUCASEID'].unique(),\n",
    "                'set': ['test' if id in test_ids else 'train' for id in subgroups['TUCASEID'].unique()]\n",
    "            })\n",
    "            split_df.to_parquet(split_path, index=False)\n",
    "            print(f\"‚úì Split saved to {split_path}\")\n",
    "        \n",
    "        # Prepare data with groups - fix the function call\n",
    "        print(f\"üîÑ Preparing data with groupby: {groupby}\")\n",
    "        groupby_cols = groupby.split(',')\n",
    "        \n",
    "        # Call with correct signature including blocks parameter\n",
    "        long_df = prepare_long_with_groups(sequences, subgroups, groupby_cols, time_blocks)\n",
    "        \n",
    "        # Pool rare quarters\n",
    "        print(f\"üîÑ Pooling rare quarter groups...\")\n",
    "        long_df = pool_rare_quarter(long_df)\n",
    "        \n",
    "        # Merge with split\n",
    "        print(f\"üîÑ Merging with train/test split...\")\n",
    "        long_df = long_df.merge(split_df, on='TUCASEID', how='left')\n",
    "        \n",
    "        print(f\"üìà Fitting B1-H model...\")\n",
    "        # Fit the model\n",
    "        result = fit_b1_hier(long_df)\n",
    "        \n",
    "        # Save results\n",
    "        output_file = output_dir / \"b1h_model.json\"\n",
    "        save_json(result, output_file)\n",
    "        \n",
    "        # Save evaluation\n",
    "        eval_file = output_dir / \"eval_b1h.json\"\n",
    "        test_data = long_df[long_df['set'] == 'test']\n",
    "        eval_result = {\n",
    "            'test_nll': nll_b1(result['params'], test_data),\n",
    "            'n_test_sequences': len(test_data['TUCASEID'].unique()),\n",
    "            'n_train_sequences': len(long_df[long_df['set'] == 'train']['TUCASEID'].unique())\n",
    "        }\n",
    "        save_json(eval_result, eval_file)\n",
    "        \n",
    "        print(f\"‚úÖ B1-H model completed successfully for {rung}\")\n",
    "        print(f\"üìÅ Saved to {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Direct execution failed: {e}\")\n",
    "        print(f\"üîç Error details: {type(e).__name__}\")\n",
    "        print(\"üîÑ Falling back to subprocess method...\")\n",
    "        return False\n",
    "\n",
    "def run_baseline1_hier_subprocess(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier via subprocess (fallback method).\"\"\"\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"atus_analysis.scripts.baseline1_hier\",\n",
    "        \"--sequences\", SEQUENCES_FILE,\n",
    "        \"--subgroups\", SUBGROUPS_FILE,\n",
    "        \"--out_dir\", str(output_dir),\n",
    "        \"--groupby\", groupby,\n",
    "        \"--time_blocks\", TIME_BLOCKS,\n",
    "        \"--seed\", str(SEED),\n",
    "        \"--test_size\", str(TEST_SIZE),\n",
    "        \"--split_path\", str(split_path)\n",
    "    ]\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  Running B1-H via subprocess for {rung}...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Use real-time output instead of capture_output\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                                 universal_newlines=True, bufsize=1)\n",
    "        \n",
    "        # Stream output in real-time\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        \n",
    "        process.wait()\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"‚úÖ B1-H model completed successfully for {rung}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå B1-H model failed for {rung} (return code: {process.returncode})\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Subprocess execution failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_baseline1_hier(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier with automatic fallback.\"\"\"\n",
    "    # Try direct execution first, fall back to subprocess if needed\n",
    "    if not globals().get('USE_SUBPROCESS', True):\n",
    "        print(\"üéØ Attempting direct execution...\")\n",
    "        if run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "            return True\n",
    "    \n",
    "    print(\"üñ•Ô∏è  Using subprocess execution...\")\n",
    "    return run_baseline1_hier_subprocess(rung, groupby, output_dir, split_path)\n",
    "\n",
    "def run_baseline2_hier(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline2_hier (hazard model) for a rung.\"\"\"\n",
    "    b1h_path = output_dir / \"b1h_model.json\"\n",
    "    \n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"atus_analysis.scripts.baseline2_hier\",\n",
    "        \"--sequences\", SEQUENCES_FILE,\n",
    "        \"--subgroups\", SUBGROUPS_FILE,\n",
    "        \"--out_dir\", str(output_dir),\n",
    "        \"--groupby\", groupby,\n",
    "        \"--time_blocks\", TIME_BLOCKS,\n",
    "        \"--dwell_bins\", DWELL_BINS,\n",
    "        \"--seed\", str(SEED),\n",
    "        \"--test_size\", str(TEST_SIZE),\n",
    "        \"--split_path\", str(split_path),\n",
    "        \"--b1h_path\", str(b1h_path)\n",
    "    ]\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  Running B2-H (hazard) model for {rung}...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Use real-time output\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                                 universal_newlines=True, bufsize=1)\n",
    "        \n",
    "        # Stream output in real-time\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        \n",
    "        process.wait()\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"‚úÖ B2-H model completed successfully for {rung}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå B2-H model failed for {rung} (return code: {process.returncode})\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå B2-H execution failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_single_rung(rung, include_hazard=False):\n",
    "    \"\"\"Run a complete experiment for a single rung.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ STARTING RUNG {rung}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load progress\n",
    "    progress = load_progress()\n",
    "    \n",
    "    # Check if already completed\n",
    "    if is_rung_completed(rung, progress):\n",
    "        print(f\"‚úÖ {rung} already completed - skipping\")\n",
    "        return True\n",
    "    \n",
    "    # Setup\n",
    "    groupby = RUNG_SPECS[rung]\n",
    "    output_dir = OUTPUT_DIR / rung\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    split_path = OUTPUT_DIR / \"fixed_split.parquet\"\n",
    "    \n",
    "    print(f\"üìã Rung: {rung}\")\n",
    "    print(f\"üìã Groupby: {groupby}\")\n",
    "    print(f\"üìÅ Output directory: {output_dir}\")\n",
    "    print(f\"‚ö° Include hazard: {include_hazard or rung == 'R7'}\")\n",
    "    \n",
    "    # Check resources before starting\n",
    "    print(f\"\\nüîç Checking system resources...\")\n",
    "    check_system_resources()\n",
    "    \n",
    "    try:\n",
    "        # Run B1-H (routing model)\n",
    "        print(f\"\\n--- üìä Step 1: B1-H Model for {rung} ---\")\n",
    "        if not run_baseline1_hier(rung, groupby, output_dir, split_path):\n",
    "            progress['failed_rungs'].append(rung)\n",
    "            save_progress(progress)\n",
    "            return False\n",
    "        \n",
    "        # Run B2-H (hazard model) if needed\n",
    "        if include_hazard or rung == \"R7\":\n",
    "            print(f\"\\n--- ‚ö° Step 2: B2-H Model for {rung} ---\")\n",
    "            if not run_baseline2_hier(rung, groupby, output_dir, split_path):\n",
    "                progress['failed_rungs'].append(rung)\n",
    "                save_progress(progress)\n",
    "                return False\n",
    "        \n",
    "        # Success!\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nüéâ {rung} COMPLETED SUCCESSFULLY in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "        \n",
    "        # Update progress\n",
    "        progress['completed_rungs'].append(rung)\n",
    "        progress[f'{rung}_completed_at'] = datetime.now().isoformat()\n",
    "        progress[f'{rung}_duration_seconds'] = elapsed\n",
    "        save_progress(progress)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        print(f\"üßπ Memory cleanup completed\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nüí• {rung} FAILED with exception: {e}\")\n",
    "        progress['failed_rungs'].append(rung)\n",
    "        progress[f'{rung}_error'] = str(e)\n",
    "        save_progress(progress)\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ Helper functions defined with improved direct execution support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63d4e59c-9891-4b1e-a7f5-53b2be7128cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_quarter_column(subgroups_df):\n",
    "    \"\"\"\n",
    "    Add quarter column to subgroups data if missing.\n",
    "    \n",
    "    This is needed for R6 and R7 experiments which use quarter in their groupby.\n",
    "    Derives quarter from month: Q1=Jan-Mar, Q2=Apr-Jun, Q3=Jul-Sep, Q4=Oct-Dec\n",
    "    \"\"\"\n",
    "    if 'quarter' in subgroups_df.columns:\n",
    "        print(\"‚úì Quarter column already exists\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    if 'month' not in subgroups_df.columns:\n",
    "        print(\"‚ùå Neither quarter nor month column found!\")\n",
    "        return subgroups_df\n",
    "    \n",
    "    print(\"üóìÔ∏è  Adding quarter column from month data...\")\n",
    "    df = subgroups_df.copy()\n",
    "    \n",
    "    # Convert month to numeric if it's string\n",
    "    month_numeric = pd.to_numeric(df['month'], errors='coerce')\n",
    "    \n",
    "    # Create quarter mapping: Q1=1-3, Q2=4-6, Q3=7-9, Q4=10-12\n",
    "    quarter_map = {\n",
    "        1: 'Q1', 2: 'Q1', 3: 'Q1',\n",
    "        4: 'Q2', 5: 'Q2', 6: 'Q2', \n",
    "        7: 'Q3', 8: 'Q3', 9: 'Q3',\n",
    "        10: 'Q4', 11: 'Q4', 12: 'Q4'\n",
    "    }\n",
    "    \n",
    "    df['quarter'] = month_numeric.map(quarter_map).fillna('Unknown')\n",
    "    \n",
    "    print(f\"‚úì Quarter column added. Distribution:\")\n",
    "    quarter_counts = df['quarter'].value_counts().sort_index()\n",
    "    for quarter, count in quarter_counts.items():\n",
    "        print(f\"   {quarter}: {count:,} respondents\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22aec82f-3e0b-46a7-9c58-8dd0616ae023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_baseline1_hier_direct(rung, groupby, output_dir, split_path):\n",
    "    \"\"\"Run baseline1_hier directly in Jupyter (preferred method).\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from atus_analysis.scripts.common_hier import (\n",
    "            prepare_long_with_groups, pool_rare_quarter,\n",
    "            save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Loading data for {rung}...\")\n",
    "        \n",
    "        # Load sequences and subgroups\n",
    "        sequences = pd.read_parquet(SEQUENCES_FILE)\n",
    "        subgroups = pd.read_parquet(SUBGROUPS_FILE)\n",
    "        \n",
    "        # Add quarter column if needed for R6/R7 experiments\n",
    "        subgroups = add_quarter_column(subgroups)\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(sequences)} sequences and {len(subgroups)} subgroups\")\n",
    "        \n",
    "        # Parse time blocks\n",
    "        time_blocks = parse_time_blocks(TIME_BLOCKS)\n",
    "        print(f\"‚úì Parsed time blocks: {time_blocks}\")\n",
    "        \n",
    "        # Create or load split\n",
    "        if split_path.exists():\n",
    "            print(f\"üìÇ Loading existing split from {split_path}\")\n",
    "            split_df = pd.read_parquet(split_path)\n",
    "        else:\n",
    "            print(f\"üé≤ Creating new split with seed {SEED}\")\n",
    "            # Create split logic here (simplified)\n",
    "            np.random.seed(SEED)\n",
    "            unique_ids = subgroups['TUCASEID'].unique()\n",
    "            test_size = int(len(unique_ids) * TEST_SIZE)\n",
    "            test_ids = np.random.choice(unique_ids, test_size, replace=False)\n",
    "            \n",
    "            split_df = pd.DataFrame({\n",
    "                'TUCASEID': subgroups['TUCASEID'].unique(),\n",
    "                'set': ['test' if id in test_ids else 'train' for id in subgroups['TUCASEID'].unique()]\n",
    "            })\n",
    "            split_df.to_parquet(split_path, index=False)\n",
    "            print(f\"‚úì Split saved to {split_path}\")\n",
    "        \n",
    "        # Prepare data with groups - fix the function call\n",
    "        print(f\"üîÑ Preparing data with groupby: {groupby}\")\n",
    "        groupby_cols = groupby.split(',')\n",
    "        \n",
    "        # Call with correct signature including blocks parameter\n",
    "        long_df = prepare_long_with_groups(sequences, subgroups, groupby_cols, time_blocks)\n",
    "        \n",
    "        # Pool rare quarters\n",
    "        print(f\"üîÑ Pooling rare quarter groups...\")\n",
    "        long_df = pool_rare_quarter(long_df)\n",
    "        \n",
    "        # Merge with split\n",
    "        print(f\"üîÑ Merging with train/test split...\")\n",
    "        long_df = long_df.merge(split_df, on='TUCASEID', how='left')\n",
    "        \n",
    "        print(f\"üìà Fitting B1-H model...\")\n",
    "        # Fit the model\n",
    "        result = fit_b1_hier(long_df)\n",
    "        \n",
    "        # Save results\n",
    "        output_file = output_dir / \"b1h_model.json\"\n",
    "        save_json(result, output_file)\n",
    "        \n",
    "        # Save evaluation\n",
    "        eval_file = output_dir / \"eval_b1h.json\"\n",
    "        test_data = long_df[long_df['set'] == 'test']\n",
    "        eval_result = {\n",
    "            'test_nll': nll_b1(result['params'], test_data),\n",
    "            'n_test_sequences': len(test_data['TUCASEID'].unique()),\n",
    "            'n_train_sequences': len(long_df[long_df['set'] == 'train']['TUCASEID'].unique())\n",
    "        }\n",
    "        save_json(eval_result, eval_file)\n",
    "        \n",
    "        print(f\"‚úÖ B1-H model completed successfully for {rung}\")\n",
    "        print(f\"üìÅ Saved to {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Direct execution failed: {e}\")\n",
    "        print(f\"üîß Falling back to subprocess method...\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc4ed-ab5a-4e42-86d1-84e4d2042ddd",
   "metadata": {},
   "source": [
    "## Cell 3b: Import Baseline Scripts Directly\n",
    "\n",
    "Instead of calling external processes, we'll import the baseline scripts directly for better integration with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab8619ea-d557-4291-aed5-364acdb7504a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully imported baseline common functions\n"
     ]
    }
   ],
   "source": [
    "# Import the baseline scripts directly instead of using subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path so we can import the modules\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    # Import the baseline functions directly\n",
    "    from atus_analysis.scripts.common_hier import (\n",
    "        prepare_long_with_groups, pool_rare_quarter, \n",
    "        save_json, nll_b1, fit_b1_hier, parse_time_blocks\n",
    "    )\n",
    "    print(\"‚úì Successfully imported baseline common functions\")\n",
    "    \n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not import baseline functions directly: {e}\")\n",
    "    print(\"Will fall back to subprocess calls\")\n",
    "    USE_SUBPROCESS = True\n",
    "else:\n",
    "    USE_SUBPROCESS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8672f3-8031-47bd-91c2-3728eea65c17",
   "metadata": {},
   "source": [
    "## Check System Status and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44daad3a-db50-4af4-8f20-a4cda6b1ce23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking system status and prerequisites...\n",
      "\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941552.2GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "=== File Prerequisites ===\n",
      "‚úì atus_analysis/data/sequences/markov_sequences.parquet\n",
      "‚úì atus_analysis/data/processed/subgroups.parquet\n",
      "‚úì atus_analysis/scripts/baseline1_hier.py\n",
      "‚úì atus_analysis/scripts/baseline2_hier.py\n",
      "‚úì Output directory: atus_analysis/data/models\n",
      "\n",
      "=== Current Progress ===\n",
      "Completed rungs: ['R1', 'R2', 'R3', 'R4', 'R5']\n",
      "Failed rungs: ['R1', 'R6', 'R6', 'R6']\n",
      "Remaining rungs: ['R6', 'R7']\n",
      "\n",
      "=== Overall Status ===\n",
      "‚úÖ READY TO START EXPERIMENTS\n",
      "\n",
      "You can now run the individual experiment cells below.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking system status and prerequisites...\\n\")\n",
    "\n",
    "# Check system resources\n",
    "resources_ok = check_system_resources()\n",
    "\n",
    "print(\"\\n=== File Prerequisites ===\")\n",
    "# Check required files\n",
    "required_files = [\n",
    "    SEQUENCES_FILE,\n",
    "    SUBGROUPS_FILE,\n",
    "    \"atus_analysis/scripts/baseline1_hier.py\",\n",
    "    \"atus_analysis/scripts/baseline2_hier.py\"\n",
    "]\n",
    "\n",
    "files_ok = True\n",
    "for file_path in required_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"‚úì {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚úó {file_path} - NOT FOUND\")\n",
    "        files_ok = False\n",
    "\n",
    "# Check output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úì Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load and display current progress\n",
    "print(\"\\n=== Current Progress ===\")\n",
    "progress = load_progress()\n",
    "completed = progress.get('completed_rungs', [])\n",
    "failed = progress.get('failed_rungs', [])\n",
    "\n",
    "print(f\"Completed rungs: {completed if completed else 'None'}\")\n",
    "print(f\"Failed rungs: {failed if failed else 'None'}\")\n",
    "print(f\"Remaining rungs: {[r for r in RUNG_SPECS.keys() if r not in completed]}\")\n",
    "\n",
    "# Overall status\n",
    "print(\"\\n=== Overall Status ===\")\n",
    "if resources_ok and files_ok:\n",
    "    print(\"‚úÖ READY TO START EXPERIMENTS\")\n",
    "    print(\"\\nYou can now run the individual experiment cells below.\")\n",
    "else:\n",
    "    print(\"‚ùå PREREQUISITES NOT MET\")\n",
    "    if not resources_ok:\n",
    "        print(\"   - System resources may be insufficient\")\n",
    "    if not files_ok:\n",
    "        print(\"   - Required files are missing\")\n",
    "    print(\"\\nPlease resolve issues before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad4c12-a087-47e3-8766-ed1032e94f34",
   "metadata": {},
   "source": [
    "# Individual Experiment Cells\n",
    "\n",
    "## Instructions for Running Experiments\n",
    "\n",
    "**Run these cells ONE AT A TIME** in order. Each cell represents one complete experiment rung.\n",
    "\n",
    "- ‚úÖ **Safe to interrupt**: You can stop any cell with the stop button - progress is automatically saved\n",
    "- üîÑ **Resume anytime**: If you restart the kernel, just re-run cells 1-4, then continue from where you left off\n",
    "- ‚è≠Ô∏è **Skip completed**: Cells will automatically skip rungs that have already completed successfully\n",
    "- üìä **Monitor progress**: Each cell shows detailed progress and resource usage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38d17c-292a-4281-ba26-3dcf7217b238",
   "metadata": {},
   "source": [
    "## Cell 5: Run Experiment R1 (Region Only)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Simplest model - groups by region only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91988ab-5442-4aa5-9ed6-cfb0b9d9f939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING RUNG R1\n",
      "============================================================\n",
      "üìã Rung: R1\n",
      "üìã Groupby: region\n",
      "üìÅ Output directory: atus_analysis/data/models/R1\n",
      "‚ö° Include hazard: False\n",
      "\n",
      "üîç Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.1% used, 746.5GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.8GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "--- üìä Step 1: B1-H Model for R1 ---\n",
      "üéØ Attempting direct execution...\n",
      "üìä Loading data for R1...\n",
      "‚úì Loaded 36404352 sequences and 252808 subgroups\n",
      "üìÇ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "üîÑ Preparing data with groupby: region\n",
      "‚ùå Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "üîÑ Falling back to subprocess method...\n",
      "üñ•Ô∏è  Using subprocess execution...\n",
      "üñ•Ô∏è  Running B1-H via subprocess for R1...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R1 --groupby region --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Output directory: atus_analysis/data/models/R1\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Groupby dimensions: region\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 00:21:17,382 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 00:21:20,645 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 00:21:20,646 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Grouping by: ['region']\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 00:21:20,701 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 00:35:23,966 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 4 groups\n",
      "2025-08-10 00:35:24,208 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 00:35:31,212 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 00:35:31,353 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 00:35:31,353 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 00:35:56,456 - __main__ - INFO - ‚úì Model fitting completed\n",
      "2025-08-10 00:35:56,461 - __main__ - INFO - Model saved to: atus_analysis/data/models/R1/b1h_model.json\n",
      "2025-08-10 00:35:56,461 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 00:39:26,261 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R1/eval_b1h.json\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - ‚úì B1-H training completed in 1088.88 seconds\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - ‚úì B1-H written to: atus_analysis/data/models/R1\n",
      "2025-08-10 00:39:26,262 - __main__ - INFO - ============================================================\n",
      "‚úì B1-H written to: atus_analysis/data/models/R1\n",
      "‚úÖ B1-H model completed successfully for R1\n",
      "\n",
      "üéâ R1 COMPLETED SUCCESSFULLY in 1094.3 seconds (18.2 minutes)\n",
      "üßπ Memory cleanup completed\n",
      "\n",
      " R1 completed successfully! You can now run R2.\n"
     ]
    }
   ],
   "source": [
    "# Run R1 experiment\n",
    "success = run_single_rung(\"R1\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n R1 completed successfully! You can now run R2.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R1 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8749bab-46d0-46b8-9925-3cb8c73c34a9",
   "metadata": {},
   "source": [
    "## Cell 6: Run Experiment R2 (Region + Sex)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Low-Medium**  \n",
    "**Description: Groups by region and sex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a23b4b-84a6-4e0f-8c30-3eaad4489b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING RUNG R2\n",
      "============================================================\n",
      "üìã Rung: R2\n",
      "üìã Groupby: region,sex\n",
      "üìÅ Output directory: atus_analysis/data/models/R2\n",
      "‚ö° Include hazard: False\n",
      "\n",
      "üîç Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 746.0GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.8GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "--- üìä Step 1: B1-H Model for R2 ---\n",
      "üéØ Attempting direct execution...\n",
      "üìä Loading data for R2...\n",
      "‚úì Loaded 36404352 sequences and 252808 subgroups\n",
      "üìÇ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "üîÑ Preparing data with groupby: region,sex\n",
      "‚ùå Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "üîÑ Falling back to subprocess method...\n",
      "üñ•Ô∏è  Using subprocess execution...\n",
      "üñ•Ô∏è  Running B1-H via subprocess for R2...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R2 --groupby region,sex --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - ============================================================\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Output directory: atus_analysis/data/models/R2\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Groupby dimensions: region,sex\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 00:39:58,073 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 00:40:01,322 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 00:40:01,322 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Grouping by: ['region', 'sex']\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 00:40:01,376 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 01:01:16,057 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-10 01:01:16,341 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:01:25,006 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 01:01:25,146 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 01:01:25,146 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 01:01:55,760 - __main__ - INFO - ‚úì Model fitting completed\n",
      "2025-08-10 01:01:55,768 - __main__ - INFO - Model saved to: atus_analysis/data/models/R2/b1h_model.json\n",
      "2025-08-10 01:01:55,768 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 01:05:28,334 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R2/eval_b1h.json\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - ‚úì B1-H training completed in 1530.26 seconds\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - ‚úì B1-H written to: atus_analysis/data/models/R2\n",
      "2025-08-10 01:05:28,336 - __main__ - INFO - ============================================================\n",
      "‚úì B1-H written to: atus_analysis/data/models/R2\n",
      "‚úÖ B1-H model completed successfully for R2\n",
      "\n",
      "üéâ R2 COMPLETED SUCCESSFULLY in 1536.8 seconds (25.6 minutes)\n",
      "üßπ Memory cleanup completed\n",
      "\n",
      "üéâ R2 completed successfully! You can now run R3.\n"
     ]
    }
   ],
   "source": [
    "# Run R2 experiment\n",
    "success = run_single_rung(\"R2\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R2 completed successfully! You can now run R3.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R2 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482aee4-3677-4527-977c-fc8a3a23f86b",
   "metadata": {},
   "source": [
    "## Cell 7: Run Experiment R3 (Region + Employment)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and employment status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d642dcc7-5173-4cac-9fc2-7715f69c02be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING RUNG R3\n",
      "============================================================\n",
      "üìã Rung: R3\n",
      "üìã Groupby: region,employment\n",
      "üìÅ Output directory: atus_analysis/data/models/R3\n",
      "‚ö° Include hazard: False\n",
      "\n",
      "üîç Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.8GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "--- üìä Step 1: B1-H Model for R3 ---\n",
      "üéØ Attempting direct execution...\n",
      "üìä Loading data for R3...\n",
      "‚úì Loaded 36404352 sequences and 252808 subgroups\n",
      "üìÇ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "üîÑ Preparing data with groupby: region,employment\n",
      "‚ùå Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "üîÑ Falling back to subprocess method...\n",
      "üñ•Ô∏è  Using subprocess execution...\n",
      "üñ•Ô∏è  Running B1-H via subprocess for R3...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R3 --groupby region,employment --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Output directory: atus_analysis/data/models/R3\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Groupby dimensions: region,employment\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 01:05:34,934 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 01:05:34,935 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 01:05:38,202 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 01:05:38,202 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Grouping by: ['region', 'employment']\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 01:05:38,256 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 01:26:37,684 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 16 groups\n",
      "2025-08-10 01:26:37,999 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:26:48,419 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 01:26:48,557 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 01:26:48,558 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 01:27:19,331 - __main__ - INFO - ‚úì Model fitting completed\n",
      "2025-08-10 01:27:19,345 - __main__ - INFO - Model saved to: atus_analysis/data/models/R3/b1h_model.json\n",
      "2025-08-10 01:27:19,345 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 01:30:52,926 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R3/eval_b1h.json\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - ‚úì B1-H training completed in 1517.99 seconds\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - ‚úì B1-H written to: atus_analysis/data/models/R3\n",
      "2025-08-10 01:30:52,927 - __main__ - INFO - ============================================================\n",
      "‚úì B1-H written to: atus_analysis/data/models/R3\n",
      "‚úÖ B1-H model completed successfully for R3\n",
      "\n",
      "üéâ R3 COMPLETED SUCCESSFULLY in 1524.7 seconds (25.4 minutes)\n",
      "üßπ Memory cleanup completed\n",
      "\n",
      "üéâ R3 completed successfully! You can now run R4.\n"
     ]
    }
   ],
   "source": [
    "# Run R3 experiment\n",
    "success = run_single_rung(\"R3\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R3 completed successfully! You can now run R4.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R3 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003e061-ca9c-45cd-b0ae-9e2d1dac7223",
   "metadata": {},
   "source": [
    "## Cell 8: Run Experiment R4 (Region + Day Type)\n",
    "\n",
    "**Expected runtime: 30-60 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and day type (weekday/weekend)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c43458-3883-46bc-b531-3849405cbd07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING RUNG R4\n",
      "============================================================\n",
      "üìã Rung: R4\n",
      "üìã Groupby: region,day_type\n",
      "üìÅ Output directory: atus_analysis/data/models/R4\n",
      "‚ö° Include hazard: False\n",
      "\n",
      "üîç Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.7GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "--- üìä Step 1: B1-H Model for R4 ---\n",
      "üéØ Attempting direct execution...\n",
      "üìä Loading data for R4...\n",
      "‚úì Loaded 36404352 sequences and 252808 subgroups\n",
      "üìÇ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "üîÑ Preparing data with groupby: region,day_type\n",
      "‚ùå Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "üîÑ Falling back to subprocess method...\n",
      "üñ•Ô∏è  Using subprocess execution...\n",
      "üñ•Ô∏è  Running B1-H via subprocess for R4...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R4 --groupby region,day_type --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Output directory: atus_analysis/data/models/R4\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Groupby dimensions: region,day_type\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 01:30:59,697 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 01:30:59,698 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 01:30:59,698 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 01:30:59,698 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 01:31:02,958 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 01:31:02,958 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Grouping by: ['region', 'day_type']\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 01:31:03,012 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 01:52:08,766 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 8 groups\n",
      "2025-08-10 01:52:09,112 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:52:17,733 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 01:52:17,872 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 01:52:17,872 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 01:52:48,749 - __main__ - INFO - ‚úì Model fitting completed\n",
      "2025-08-10 01:52:48,756 - __main__ - INFO - Model saved to: atus_analysis/data/models/R4/b1h_model.json\n",
      "2025-08-10 01:52:48,756 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 01:56:20,560 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R4/eval_b1h.json\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - ‚úì B1-H training completed in 1520.87 seconds\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - ‚úì B1-H written to: atus_analysis/data/models/R4\n",
      "2025-08-10 01:56:20,562 - __main__ - INFO - ============================================================\n",
      "‚úì B1-H written to: atus_analysis/data/models/R4\n",
      "‚úÖ B1-H model completed successfully for R4\n",
      "\n",
      "üéâ R4 COMPLETED SUCCESSFULLY in 1527.5 seconds (25.5 minutes)\n",
      "üßπ Memory cleanup completed\n",
      "\n",
      "üéâ R4 completed successfully! You can now run R5.\n"
     ]
    }
   ],
   "source": [
    "# Run R4 experiment\n",
    "success = run_single_rung(\"R4\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R4 completed successfully! You can now run R5.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R4 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72b726-3099-4764-8e9f-67bdc9c343ee",
   "metadata": {},
   "source": [
    "## Cell 9: Run Experiment R5 (Region + Household Size)\n",
    "\n",
    "**Expected runtime: 60-90 minutes**  \n",
    "**Memory usage: Medium**  \n",
    "**Description: Groups by region and household size band**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1acf2ea-7a52-4f60-9820-78152c899e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING RUNG R5\n",
      "============================================================\n",
      "üìã Rung: R5\n",
      "üìã Groupby: region,hh_size_band\n",
      "üìÅ Output directory: atus_analysis/data/models/R5\n",
      "‚ö° Include hazard: False\n",
      "\n",
      "üîç Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.9GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941548.6GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "--- üìä Step 1: B1-H Model for R5 ---\n",
      "üéØ Attempting direct execution...\n",
      "üìä Loading data for R5...\n",
      "‚úì Loaded 36404352 sequences and 252808 subgroups\n",
      "üìÇ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "üîÑ Preparing data with groupby: region,hh_size_band\n",
      "‚ùå Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "üîÑ Falling back to subprocess method...\n",
      "üñ•Ô∏è  Using subprocess execution...\n",
      "üñ•Ô∏è  Running B1-H via subprocess for R5...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R5 --groupby region,hh_size_band --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - ============================================================\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Output directory: atus_analysis/data/models/R5\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Groupby dimensions: region,hh_size_band\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 01:56:27,314 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 01:56:30,577 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 01:56:30,577 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Grouping by: ['region', 'hh_size_band']\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 01:56:30,631 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 02:17:32,440 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 16 groups\n",
      "2025-08-10 02:17:32,960 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 02:17:41,574 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 02:17:41,711 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 02:17:41,711 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 02:18:11,911 - __main__ - INFO - ‚úì Model fitting completed\n",
      "2025-08-10 02:18:11,925 - __main__ - INFO - Model saved to: atus_analysis/data/models/R5/b1h_model.json\n",
      "2025-08-10 02:18:11,925 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 02:21:45,817 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R5/eval_b1h.json\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - ============================================================\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - ‚úì B1-H training completed in 1518.51 seconds\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - ‚úì B1-H written to: atus_analysis/data/models/R5\n",
      "2025-08-10 02:21:45,818 - __main__ - INFO - ============================================================\n",
      "‚úì B1-H written to: atus_analysis/data/models/R5\n",
      "‚úÖ B1-H model completed successfully for R5\n",
      "\n",
      "üéâ R5 COMPLETED SUCCESSFULLY in 1525.2 seconds (25.4 minutes)\n",
      "üßπ Memory cleanup completed\n",
      "\n",
      "üéâ R5 completed successfully! You can now run R6.\n"
     ]
    }
   ],
   "source": [
    "# Run R5 experiment\n",
    "success = run_single_rung(\"R5\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R5 completed successfully! You can now run R6.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R5 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b4517-cebb-49fd-8d65-32e42d2ca040",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 9b: Add Quarter Column to Subgroups File (Run Before R6)\n",
    "\n",
    "**Important**: Run this cell before attempting R6 or R7 experiments.  \n",
    "This will permanently add the quarter column to the subgroups.parquet file.  \n",
    "**Runtime**: 1-2 minutes  \n",
    "**Purpose**: Ensures R6 and R7 experiments have the required quarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dc51406-b02a-4332-887f-ca37b9880418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóìÔ∏è  Adding quarter column to subgroups.parquet file...\n",
      "üìÇ Loading subgroups from: atus_analysis/data/processed/subgroups.parquet\n",
      "‚úÖ Loaded 252808 subgroup records\n",
      "Current columns: ['TUCASEID', 'sex', 'hh_size_band', 'month', 'region', 'employment', 'day_type', 'TUFNWGTP']\n",
      "üìù Quarter column not found - adding it now...\n",
      "üóìÔ∏è  Adding quarter column from month data...\n",
      "‚úì Quarter column added. Distribution:\n",
      "   Q1: 68,017 respondents\n",
      "   Q2: 62,107 respondents\n",
      "   Q3: 61,880 respondents\n",
      "   Q4: 60,804 respondents\n",
      "üíæ Creating backup at: atus_analysis/data/processed/subgroups.parquet.backup\n",
      "üíæ Saving updated subgroups with quarter column...\n",
      "‚úÖ Quarter column successfully added to subgroups.parquet!\n",
      "Final quarter distribution:\n",
      "   Q1: 68,017 respondents\n",
      "   Q2: 62,107 respondents\n",
      "   Q3: 61,880 respondents\n",
      "   Q4: 60,804 respondents\n",
      "\n",
      "üìã Summary:\n",
      "   - Original file backed up to: atus_analysis/data/processed/subgroups.parquet.backup\n",
      "   - Updated file saved to: atus_analysis/data/processed/subgroups.parquet\n",
      "   - Quarter column added with 252808 records\n",
      "   - R6 and R7 experiments are now ready to run!\n",
      "\n",
      "============================================================\n",
      "üéØ READY FOR R6 AND R7 EXPERIMENTS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Add quarter column permanently to subgroups.parquet file\n",
    "print(\"üóìÔ∏è  Adding quarter column to subgroups.parquet file...\")\n",
    "\n",
    "try:\n",
    "    # Load the current subgroups file\n",
    "    subgroups_path = Path(SUBGROUPS_FILE)\n",
    "    print(f\"üìÇ Loading subgroups from: {subgroups_path}\")\n",
    "    \n",
    "    if not subgroups_path.exists():\n",
    "        print(f\"‚ùå Subgroups file not found at {subgroups_path}\")\n",
    "        print(\"Please ensure the file exists before running this cell.\")\n",
    "    else:\n",
    "        # Load the data\n",
    "        subgroups_df = pd.read_parquet(subgroups_path)\n",
    "        print(f\"‚úÖ Loaded {len(subgroups_df)} subgroup records\")\n",
    "        print(f\"Current columns: {list(subgroups_df.columns)}\")\n",
    "        \n",
    "        # Check if quarter column already exists\n",
    "        if 'quarter' in subgroups_df.columns:\n",
    "            print(\"‚úÖ Quarter column already exists in the file!\")\n",
    "            quarter_counts = subgroups_df['quarter'].value_counts().sort_index()\n",
    "            print(\"Current quarter distribution:\")\n",
    "            for quarter, count in quarter_counts.items():\n",
    "                print(f\"   {quarter}: {count:,} respondents\")\n",
    "        else:\n",
    "            print(\"üìù Quarter column not found - adding it now...\")\n",
    "            \n",
    "            # Add quarter column using our function\n",
    "            subgroups_with_quarter = add_quarter_column(subgroups_df)\n",
    "            \n",
    "            # Create backup of original file\n",
    "            backup_path = subgroups_path.with_suffix('.parquet.backup')\n",
    "            print(f\"üíæ Creating backup at: {backup_path}\")\n",
    "            subgroups_df.to_parquet(backup_path, index=False)\n",
    "            \n",
    "            # Save the updated file\n",
    "            print(f\"üíæ Saving updated subgroups with quarter column...\")\n",
    "            subgroups_with_quarter.to_parquet(subgroups_path, index=False)\n",
    "            \n",
    "            # Verify the save\n",
    "            verification_df = pd.read_parquet(subgroups_path)\n",
    "            if 'quarter' in verification_df.columns:\n",
    "                print(\"‚úÖ Quarter column successfully added to subgroups.parquet!\")\n",
    "                quarter_counts = verification_df['quarter'].value_counts().sort_index()\n",
    "                print(\"Final quarter distribution:\")\n",
    "                for quarter, count in quarter_counts.items():\n",
    "                    print(f\"   {quarter}: {count:,} respondents\")\n",
    "                    \n",
    "                print(f\"\\nüìã Summary:\")\n",
    "                print(f\"   - Original file backed up to: {backup_path}\")\n",
    "                print(f\"   - Updated file saved to: {subgroups_path}\")\n",
    "                print(f\"   - Quarter column added with {len(verification_df)} records\")\n",
    "                print(f\"   - R6 and R7 experiments are now ready to run!\")\n",
    "            else:\n",
    "                print(\"‚ùå Failed to verify quarter column in saved file\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error adding quarter column: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(\"\\nYou can still run R6/R7 - the quarter column will be added dynamically.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ READY FOR R6 AND R7 EXPERIMENTS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea1bfd-a23f-4b49-a5c3-9d7685ec9dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 10: Run Experiment R6 (Full Routing Model)\n",
    "\n",
    "**Expected runtime: 90-120 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full complexity routing model with all demographic variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d2c9b3f-c90d-4354-8e22-36f1a2e8d7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING RUNG R6\n",
      "============================================================\n",
      "üìã Rung: R6\n",
      "üìã Groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "üìÅ Output directory: atus_analysis/data/models/R6\n",
      "‚ö° Include hazard: False\n",
      "\n",
      "üîç Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941552.2GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "--- üìä Step 1: B1-H Model for R6 ---\n",
      "üéØ Attempting direct execution...\n",
      "üìä Loading data for R6...\n",
      "‚úì Quarter column already exists\n",
      "‚úì Loaded 36404352 sequences and 252808 subgroups\n",
      "‚úì Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "üìÇ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "üîÑ Preparing data with groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "‚ùå Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "üîß Falling back to subprocess method...\n",
      "üñ•Ô∏è  Using subprocess execution...\n",
      "üñ•Ô∏è  Running B1-H via subprocess for R6...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R6 --groupby employment,day_type,hh_size_band,sex,region,quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - ============================================================\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - ============================================================\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Output directory: atus_analysis/data/models/R6\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Groupby dimensions: employment,day_type,hh_size_band,sex,region,quarter\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 02:59:24,954 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 02:59:28,225 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 02:59:28,225 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Grouping by: ['employment', 'day_type', 'hh_size_band', 'sex', 'region', 'quarter']\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 02:59:28,286 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 03:31:14,406 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 1024 groups\n",
      "2025-08-10 03:31:14,811 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 03:31:28,435 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 03:31:28,572 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 03:31:28,572 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 03:32:09,464 - __main__ - INFO - ‚úì Model fitting completed\n",
      "2025-08-10 03:32:10,256 - __main__ - INFO - Model saved to: atus_analysis/data/models/R6/b1h_model.json\n",
      "2025-08-10 03:32:10,256 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 03:35:42,986 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R6/eval_b1h.json\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - ============================================================\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - ‚úì B1-H training completed in 2178.03 seconds\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - ‚úì B1-H written to: atus_analysis/data/models/R6\n",
      "2025-08-10 03:35:42,987 - __main__ - INFO - ============================================================\n",
      "‚úì B1-H written to: atus_analysis/data/models/R6\n",
      "‚úÖ B1-H model completed successfully for R6\n",
      "\n",
      "üéâ R6 COMPLETED SUCCESSFULLY in 2185.2 seconds (36.4 minutes)\n",
      "üßπ Memory cleanup completed\n",
      "\n",
      "üéâ R6 completed successfully! You can now run R7.\n"
     ]
    }
   ],
   "source": [
    "# Run R6 experiment\n",
    "success = run_single_rung(\"R6\", include_hazard=False)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R6 completed successfully! You can now run R7.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R6 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94e9ac-59fc-4537-8091-79a5d9edc5a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 11: Run Experiment R7 (Full Model + Hazard)\n",
    "\n",
    "**Expected runtime: 120-180 minutes**  \n",
    "**Memory usage: High**  \n",
    "**Description: Full model including hazard modeling - most computationally intensive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "701b9d75-acdc-47f5-812b-dbcec5a24d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING RUNG R7\n",
      "============================================================\n",
      "üìã Rung: R7\n",
      "üìã Groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "üìÅ Output directory: atus_analysis/data/models/R7\n",
      "‚ö° Include hazard: True\n",
      "\n",
      "üîç Checking system resources...\n",
      "=== System Resources ===\n",
      "Memory: 1.2% used, 745.8GB available\n",
      "CPU: 0.0% usage\n",
      "Disk: 1941553.0GB free\n",
      "\n",
      "‚úì System resources look good\n",
      "\n",
      "--- üìä Step 1: B1-H Model for R7 ---\n",
      "üéØ Attempting direct execution...\n",
      "üìä Loading data for R7...\n",
      "‚úì Quarter column already exists\n",
      "‚úì Loaded 36404352 sequences and 252808 subgroups\n",
      "‚úì Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "üìÇ Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "üîÑ Preparing data with groupby: employment,day_type,hh_size_band,sex,region,quarter\n",
      "‚ùå Direct execution failed: prepare_long_with_groups() missing 1 required positional argument: 'blocks'\n",
      "üîß Falling back to subprocess method...\n",
      "üñ•Ô∏è  Using subprocess execution...\n",
      "üñ•Ô∏è  Running B1-H via subprocess for R7...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline1_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R7 --groupby employment,day_type,hh_size_band,sex,region,quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - ============================================================\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - STARTING B1-H (HIERARCHICAL ROUTING) MODEL TRAINING\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - ============================================================\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Output directory: atus_analysis/data/models/R7\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Groupby dimensions: employment,day_type,hh_size_band,sex,region,quarter\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Shrinkage - tau_block: 50.0, tau_group: 20.0, add_k: 1.0\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Parsed time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 04:54:51,692 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 04:54:54,913 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 04:54:54,913 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Grouping by: ['employment', 'day_type', 'hh_size_band', 'sex', 'region', 'quarter']\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 04:54:54,973 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 05:26:49,127 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 1024 groups\n",
      "2025-08-10 05:26:49,655 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 05:27:02,852 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 05:27:02,993 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 05:27:02,993 - __main__ - INFO - Fitting B1-H hierarchical model...\n",
      "2025-08-10 05:27:44,079 - __main__ - INFO - ‚úì Model fitting completed\n",
      "2025-08-10 05:27:44,890 - __main__ - INFO - Model saved to: atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 05:27:44,891 - __main__ - INFO - Evaluating model on test set...\n",
      "2025-08-10 05:31:17,560 - __main__ - INFO - Test NLL: N/A\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - Evaluation results saved to: atus_analysis/data/models/R7/eval_b1h.json\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - ============================================================\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - ‚úì B1-H training completed in 2185.87 seconds\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - ‚úì B1-H written to: atus_analysis/data/models/R7\n",
      "2025-08-10 05:31:17,561 - __main__ - INFO - ============================================================\n",
      "‚úì B1-H written to: atus_analysis/data/models/R7\n",
      "‚úÖ B1-H model completed successfully for R7\n",
      "\n",
      "--- ‚ö° Step 2: B2-H Model for R7 ---\n",
      "üñ•Ô∏è  Running B2-H (hazard) model for R7...\n",
      "Command: /sw/eb/sw/Anaconda3/2023.09-0/bin/python -m atus_analysis.scripts.baseline2_hier --sequences atus_analysis/data/sequences/markov_sequences.parquet --subgroups atus_analysis/data/processed/subgroups.parquet --out_dir atus_analysis/data/models/R7 --groupby employment,day_type,hh_size_band,sex,region,quarter --time_blocks night:0-5,morning:6-11,afternoon:12-17,evening:18-23 --dwell_bins 1,2,3,4,6,9,14,20,30 --seed 2025 --test_size 0.2 --split_path atus_analysis/data/models/fixed_split.parquet --b1h_path atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - ============================================================\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - STARTING B2-H (HIERARCHICAL ROUTING + HAZARD) MODEL TRAINING\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - ============================================================\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Sequences file: atus_analysis/data/sequences/markov_sequences.parquet\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Subgroups file: atus_analysis/data/processed/subgroups.parquet\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Output directory: atus_analysis/data/models/R7\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Groupby dimensions: employment,day_type,hh_size_band,sex,region,quarter\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Weight column: TUFNWGTP\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Time blocks: night:0-5,morning:6-11,afternoon:12-17,evening:18-23\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Dwell bins: 1,2,3,4,6,9,14,20,30\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Test size: 0.2\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Random seed: 2025\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Existing B1-H model: atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Shrinkage params - Route: tau_block=50.0, tau_group=20.0, add_k=1.0\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Shrinkage params - Hazard: tau_block=200.0, tau_group=50.0, k0_global=1.0\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Parsing time blocks and dwell bins...\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Time blocks: [('night', 0, 5), ('morning', 6, 11), ('afternoon', 12, 17), ('evening', 18, 23)]\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Dwell edges: [1, 2, 3, 4, 6, 9, 14, 20, 30]\n",
      "2025-08-10 05:31:19,957 - __main__ - INFO - Loading sequences data...\n",
      "2025-08-10 05:31:23,215 - __main__ - INFO - Loaded 36,404,352 sequence records\n",
      "2025-08-10 05:31:23,215 - __main__ - INFO - Loading subgroups data...\n",
      "2025-08-10 05:31:23,275 - __main__ - INFO - Loaded 252,808 respondent records\n",
      "2025-08-10 05:31:23,276 - __main__ - INFO - Grouping by: ['employment', 'day_type', 'hh_size_band', 'sex', 'region', 'quarter']\n",
      "2025-08-10 05:31:23,276 - __main__ - INFO - Pooling rare quarter groups...\n",
      "2025-08-10 05:31:23,276 - __main__ - INFO - Preparing data with groups and time blocks...\n",
      "2025-08-10 06:03:12,288 - __main__ - INFO - Data prepared: 36,404,352 records, 15 states, 1024 groups\n",
      "2025-08-10 06:03:12,289 - __main__ - INFO - Using split file: atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 06:03:12,845 - __main__ - INFO - Loading existing split from atus_analysis/data/models/fixed_split.parquet\n",
      "2025-08-10 06:03:32,147 - __main__ - INFO - Data split: 29,123,568 train records, 7,280,784 test records\n",
      "2025-08-10 06:03:32,285 - __main__ - INFO - Train respondents: 202,247, Test respondents: 50,561\n",
      "2025-08-10 06:03:32,285 - __main__ - INFO - Loading existing B1-H model from: atus_analysis/data/models/R7/b1h_model.json\n",
      "2025-08-10 06:03:32,513 - __main__ - INFO - ‚úì B1-H model loaded successfully\n",
      "2025-08-10 06:03:32,513 - __main__ - INFO - Fitting B2-H hazard model...\n",
      "2025-08-10 06:05:42,513 - __main__ - INFO - ‚úì B2-H hazard model fitted successfully\n",
      "2025-08-10 06:05:43,788 - __main__ - INFO - B2-H model saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7/b2h_model.json\n",
      "2025-08-10 06:05:43,789 - __main__ - INFO - Evaluating models on test set...\n",
      "2025-08-10 06:09:17,552 - __main__ - INFO - B1-H test NLL: N/A\n",
      "2025-08-10 06:13:39,094 - __main__ - INFO - B2-H test NLL: N/A\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - Evaluation results saved to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7/eval_b2h.json\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - ============================================================\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - ‚úì B2-H training completed in 2539.14 seconds\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - ‚úì B2-H written to: /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7\n",
      "2025-08-10 06:13:39,096 - __main__ - INFO - ============================================================\n",
      "‚úì B2-H run complete ‚Üí /ztank/scratch/user/u.rd143338/atus_analysis-main/atus_analysis/data/models/R7\n",
      "‚úÖ B2-H model completed successfully for R7\n",
      "\n",
      "üéâ R7 COMPLETED SUCCESSFULLY in 4735.3 seconds (78.9 minutes)\n",
      "üßπ Memory cleanup completed\n",
      "\n",
      "üéâ R7 completed successfully! All experiments are now complete.\n"
     ]
    }
   ],
   "source": [
    "# Run R7 experiment (automatically includes hazard model)\n",
    "success = run_single_rung(\"R7\", include_hazard=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ R7 completed successfully! All experiments are now complete.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå R7 failed. Check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5ab40-c366-49e6-a12c-f22f99438d3a",
   "metadata": {},
   "source": [
    "## Cell 12: Final Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8817df5-d3d8-47c3-a431-4ff02a0c54bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EXPERIMENT SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total rungs: 7\n",
      "Completed: 7 - ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7']\n",
      "Failed: 6 - ['R1', 'R6', 'R6', 'R6', 'R6', 'R7']\n",
      "Not attempted: []\n",
      "\n",
      "=== Timing Information ===\n",
      "R1: 1094 seconds (18.2 minutes)\n",
      "R2: 1537 seconds (25.6 minutes)\n",
      "R3: 1525 seconds (25.4 minutes)\n",
      "R4: 1528 seconds (25.5 minutes)\n",
      "R5: 1525 seconds (25.4 minutes)\n",
      "R6: 2185 seconds (36.4 minutes)\n",
      "R7: 4735 seconds (78.9 minutes)\n",
      "\n",
      "Total runtime: 14129 seconds (235.5 minutes, 3.9 hours)\n",
      "\n",
      "=== Output Files ===\n",
      "R1: 2 files in atus_analysis/data/models/R1\n",
      "R2: 2 files in atus_analysis/data/models/R2\n",
      "R3: 2 files in atus_analysis/data/models/R3\n",
      "R4: 2 files in atus_analysis/data/models/R4\n",
      "R5: 2 files in atus_analysis/data/models/R5\n",
      "R6: 2 files in atus_analysis/data/models/R6\n",
      "R7: 4 files in atus_analysis/data/models/R7\n",
      "\n",
      "Overall success rate: 100.0%\n",
      "\n",
      "üéâ ALL EXPERIMENTS COMPLETED SUCCESSFULLY! üéâ\n",
      "\n",
      "Progress file saved as: experiment_progress_jupyter.json\n",
      "Output directory: atus_analysis/data/models\n"
     ]
    }
   ],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "progress = load_progress()\n",
    "completed = progress.get('completed_rungs', [])\n",
    "failed = progress.get('failed_rungs', [])\n",
    "all_rungs = list(RUNG_SPECS.keys())\n",
    "\n",
    "print(f\"\\nTotal rungs: {len(all_rungs)}\")\n",
    "print(f\"Completed: {len(completed)} - {completed}\")\n",
    "print(f\"Failed: {len(failed)} - {failed}\")\n",
    "print(f\"Not attempted: {[r for r in all_rungs if r not in completed and r not in failed]}\")\n",
    "\n",
    "# Show timing information\n",
    "print(\"\\n=== Timing Information ===\")\n",
    "total_time = 0\n",
    "for rung in completed:\n",
    "    duration_key = f'{rung}_duration_seconds'\n",
    "    if duration_key in progress:\n",
    "        duration = progress[duration_key]\n",
    "        total_time += duration\n",
    "        print(f\"{rung}: {duration:.0f} seconds ({duration/60:.1f} minutes)\")\n",
    "\n",
    "if total_time > 0:\n",
    "    print(f\"\\nTotal runtime: {total_time:.0f} seconds ({total_time/60:.1f} minutes, {total_time/3600:.1f} hours)\")\n",
    "\n",
    "# Show output locations\n",
    "print(\"\\n=== Output Files ===\")\n",
    "for rung in completed:\n",
    "    rung_dir = OUTPUT_DIR / rung\n",
    "    if rung_dir.exists():\n",
    "        files = list(rung_dir.glob(\"*.json\"))\n",
    "        print(f\"{rung}: {len(files)} files in {rung_dir}\")\n",
    "\n",
    "# Success rate\n",
    "if len(completed) + len(failed) > 0:\n",
    "    success_rate = len(completed) / 7 * 100\n",
    "    print(f\"\\nOverall success rate: {success_rate:.1f}%\")\n",
    "\n",
    "if len(completed) == len(all_rungs):\n",
    "    print(\"\\nüéâ ALL EXPERIMENTS COMPLETED SUCCESSFULLY! üéâ\")\n",
    "elif len(failed) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Some experiments failed. You can re-run the failed cells to retry.\")\n",
    "else:\n",
    "    print(f\"\\nüìù {len(all_rungs) - len(completed)} experiments remaining.\")\n",
    "\n",
    "print(f\"\\nProgress file saved as: {PROGRESS_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c4ebd-a80e-4134-9964-32a47ed327a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4a610-b6d2-489d-8cb4-6a5b0bfa6f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6d417-21e3-47e0-805e-bd9636cd9295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
